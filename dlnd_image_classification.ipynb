{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 6:\n",
      "Image - Min Value: 7 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e536a0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 6\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x.astype(float)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            for k in range(x.shape[2]):\n",
    "                for l in range(x.shape[3]):\n",
    "                    x[i][j][k][l] = x[i][j][k][l] / 255.\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(range(10))\n",
    "\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    a = np.array(x)\n",
    "    ret = lb.transform(a)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, image_shape[0],image_shape[1],image_shape[2]], name='x')\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    y = tf.placeholder(tf.float32, [None, n_classes], name = 'y')\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    z = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([*conv_ksize, x_tensor.get_shape().as_list()[-1], conv_num_outputs]))\n",
    "    biases = tf.Variable(tf.truncated_normal([conv_num_outputs]))\n",
    "    \n",
    "    ret = tf.nn.conv2d(x_tensor, weights, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    ret = tf.nn.bias_add(ret, biases)\n",
    "    ret = tf.nn.relu(ret)\n",
    "    \n",
    "    #maxpooling w/ same padding\n",
    "    ret = tf.nn.max_pool(\n",
    "        ret,\n",
    "        ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "        strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "        padding='SAME')\n",
    "    \n",
    "    return ret \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs, use_bias=True, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs, use_bias=True, activation=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv1 = conv2d_maxpool(x, 48, (2,2), (2,2), (2,2), (2,2))\n",
    "    conv2 = conv2d_maxpool(conv1, 24, (2,2), (2,2), (2,2), (2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    flat = flatten(conv2)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    fc1 = fully_conn(flat, num_outputs = 50)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, num_outputs = 25)\n",
    "    fc2 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fc2, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = sess.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    \n",
    "    print('Loss: {} Validation Accuracy: {}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2911360263824463 Validation Accuracy: 0.09860000014305115\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.2789130210876465 Validation Accuracy: 0.1080000028014183\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.2607693672180176 Validation Accuracy: 0.12380000203847885\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.247020959854126 Validation Accuracy: 0.13019999861717224\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.23050856590271 Validation Accuracy: 0.1451999992132187\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.2110304832458496 Validation Accuracy: 0.15960000455379486\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.20483660697937 Validation Accuracy: 0.15860000252723694\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.182863235473633 Validation Accuracy: 0.1746000051498413\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.185802698135376 Validation Accuracy: 0.1687999963760376\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.1962742805480957 Validation Accuracy: 0.17260000109672546\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 2.1764626502990723 Validation Accuracy: 0.20640000700950623\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 2.1753933429718018 Validation Accuracy: 0.2150000035762787\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 2.165843963623047 Validation Accuracy: 0.21080000698566437\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 2.1779472827911377 Validation Accuracy: 0.21580000221729279\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 2.176279067993164 Validation Accuracy: 0.23319999873638153\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 2.1859560012817383 Validation Accuracy: 0.2290000021457672\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 2.192230463027954 Validation Accuracy: 0.23340000212192535\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 2.2031238079071045 Validation Accuracy: 0.21240000426769257\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 2.185223340988159 Validation Accuracy: 0.2223999947309494\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 2.215890884399414 Validation Accuracy: 0.22859999537467957\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 2.2094035148620605 Validation Accuracy: 0.22220000624656677\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 2.2159993648529053 Validation Accuracy: 0.2386000007390976\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 2.2106971740722656 Validation Accuracy: 0.2370000034570694\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 2.214064121246338 Validation Accuracy: 0.24940000474452972\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 2.213850736618042 Validation Accuracy: 0.2513999938964844\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 2.2266125679016113 Validation Accuracy: 0.26739999651908875\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 2.192250967025757 Validation Accuracy: 0.262800008058548\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 2.175201416015625 Validation Accuracy: 0.2651999890804291\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 2.155578374862671 Validation Accuracy: 0.2741999924182892\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 2.120976448059082 Validation Accuracy: 0.2797999978065491\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 2.1136887073516846 Validation Accuracy: 0.28679999709129333\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 2.052570343017578 Validation Accuracy: 0.2818000018596649\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 2.065157413482666 Validation Accuracy: 0.2815999984741211\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.9924981594085693 Validation Accuracy: 0.2924000024795532\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 2.005230188369751 Validation Accuracy: 0.28780001401901245\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 2.0047550201416016 Validation Accuracy: 0.28519999980926514\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.947077751159668 Validation Accuracy: 0.2953999936580658\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.9533993005752563 Validation Accuracy: 0.3025999963283539\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.9158451557159424 Validation Accuracy: 0.3107999861240387\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.9392063617706299 Validation Accuracy: 0.2935999929904938\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.9009215831756592 Validation Accuracy: 0.303600013256073\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.8674039840698242 Validation Accuracy: 0.319599986076355\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.8795093297958374 Validation Accuracy: 0.32019999623298645\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.8507702350616455 Validation Accuracy: 0.3224000036716461\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.8888391256332397 Validation Accuracy: 0.31779998540878296\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.839068055152893 Validation Accuracy: 0.326200008392334\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.8138363361358643 Validation Accuracy: 0.3240000009536743\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.846014380455017 Validation Accuracy: 0.31779998540878296\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.8421634435653687 Validation Accuracy: 0.3160000145435333\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.8258545398712158 Validation Accuracy: 0.32019999623298645\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.8616310358047485 Validation Accuracy: 0.3199999928474426\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.8248687982559204 Validation Accuracy: 0.33219999074935913\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.8317705392837524 Validation Accuracy: 0.33180001378059387\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.8396503925323486 Validation Accuracy: 0.3310000002384186\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.8045127391815186 Validation Accuracy: 0.3325999975204468\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.8081252574920654 Validation Accuracy: 0.33399999141693115\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.8023765087127686 Validation Accuracy: 0.34139999747276306\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.7528870105743408 Validation Accuracy: 0.3465999960899353\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.7747608423233032 Validation Accuracy: 0.3447999954223633\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.7616283893585205 Validation Accuracy: 0.34380000829696655\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.7870490550994873 Validation Accuracy: 0.34220001101493835\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.7674709558486938 Validation Accuracy: 0.3407999873161316\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.734267234802246 Validation Accuracy: 0.3513999879360199\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.745684266090393 Validation Accuracy: 0.35600000619888306\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.7025954723358154 Validation Accuracy: 0.35420000553131104\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.7398483753204346 Validation Accuracy: 0.3531999886035919\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.7114410400390625 Validation Accuracy: 0.3458000123500824\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.6972932815551758 Validation Accuracy: 0.36039999127388\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.6636369228363037 Validation Accuracy: 0.35359999537467957\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.6720962524414062 Validation Accuracy: 0.3594000041484833\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.6487525701522827 Validation Accuracy: 0.3601999878883362\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.6318271160125732 Validation Accuracy: 0.36880001425743103\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.6579301357269287 Validation Accuracy: 0.36899998784065247\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.6614032983779907 Validation Accuracy: 0.3776000142097473\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.6443872451782227 Validation Accuracy: 0.3677999973297119\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.6395270824432373 Validation Accuracy: 0.3765999972820282\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.610171914100647 Validation Accuracy: 0.36980000138282776\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.6060791015625 Validation Accuracy: 0.37700000405311584\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.639183759689331 Validation Accuracy: 0.3815999925136566\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.601858139038086 Validation Accuracy: 0.37139999866485596\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.6271812915802002 Validation Accuracy: 0.38499999046325684\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.6024491786956787 Validation Accuracy: 0.3797999918460846\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.581171989440918 Validation Accuracy: 0.3862000107765198\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.581222653388977 Validation Accuracy: 0.38760000467300415\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.5584673881530762 Validation Accuracy: 0.3880000114440918\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.5775150060653687 Validation Accuracy: 0.38999998569488525\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.5533020496368408 Validation Accuracy: 0.38019999861717224\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.5379327535629272 Validation Accuracy: 0.38960000872612\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.5040721893310547 Validation Accuracy: 0.3921999931335449\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.4987303018569946 Validation Accuracy: 0.3946000039577484\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.5179965496063232 Validation Accuracy: 0.38940000534057617\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.5011926889419556 Validation Accuracy: 0.38999998569488525\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.4804946184158325 Validation Accuracy: 0.3930000066757202\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.4669935703277588 Validation Accuracy: 0.3928000032901764\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 1.4645429849624634 Validation Accuracy: 0.3970000147819519\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 1.478100061416626 Validation Accuracy: 0.4002000093460083\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 1.4683502912521362 Validation Accuracy: 0.39259999990463257\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 1.4444925785064697 Validation Accuracy: 0.39719998836517334\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.4192967414855957 Validation Accuracy: 0.3970000147819519\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.4152336120605469 Validation Accuracy: 0.3977999985218048\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 1.3888804912567139 Validation Accuracy: 0.40560001134872437\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 1.3955177068710327 Validation Accuracy: 0.39879998564720154\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 1.3569419384002686 Validation Accuracy: 0.3977999985218048\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 1.3533775806427002 Validation Accuracy: 0.39480000734329224\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 1.3444260358810425 Validation Accuracy: 0.4016000032424927\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 1.3507678508758545 Validation Accuracy: 0.40639999508857727\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 1.3272240161895752 Validation Accuracy: 0.40619999170303345\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 1.3323737382888794 Validation Accuracy: 0.40540000796318054\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 1.3447797298431396 Validation Accuracy: 0.40799999237060547\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 1.328643798828125 Validation Accuracy: 0.4065999984741211\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 1.3615602254867554 Validation Accuracy: 0.40560001134872437\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 1.3275954723358154 Validation Accuracy: 0.4081999957561493\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 1.307878017425537 Validation Accuracy: 0.41200000047683716\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 1.3353667259216309 Validation Accuracy: 0.4142000079154968\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 1.3254567384719849 Validation Accuracy: 0.41019999980926514\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 1.3108742237091064 Validation Accuracy: 0.4169999957084656\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 1.2773808240890503 Validation Accuracy: 0.41839998960494995\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 1.2964534759521484 Validation Accuracy: 0.4180000126361847\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 1.295661211013794 Validation Accuracy: 0.41100001335144043\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 1.2782680988311768 Validation Accuracy: 0.4115999937057495\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 1.2948334217071533 Validation Accuracy: 0.41940000653266907\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 1.2663053274154663 Validation Accuracy: 0.4189999997615814\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 1.286132574081421 Validation Accuracy: 0.42719998955726624\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 1.290855050086975 Validation Accuracy: 0.4203999936580658\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 1.2829828262329102 Validation Accuracy: 0.423799991607666\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 1.2705345153808594 Validation Accuracy: 0.41760000586509705\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 1.2568414211273193 Validation Accuracy: 0.42080000042915344\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 1.2714641094207764 Validation Accuracy: 0.42340001463890076\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 1.276865005493164 Validation Accuracy: 0.41920000314712524\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 1.2585875988006592 Validation Accuracy: 0.42239999771118164\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 1.2757803201675415 Validation Accuracy: 0.42579999566078186\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 1.265497088432312 Validation Accuracy: 0.4253999888896942\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 1.267707109451294 Validation Accuracy: 0.42320001125335693\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 1.2518784999847412 Validation Accuracy: 0.4327999949455261\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 1.2723278999328613 Validation Accuracy: 0.430400013923645\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 1.2530834674835205 Validation Accuracy: 0.42980000376701355\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 1.2683414220809937 Validation Accuracy: 0.4291999936103821\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 1.2576881647109985 Validation Accuracy: 0.4262000024318695\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 1.2242848873138428 Validation Accuracy: 0.42980000376701355\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 1.2217533588409424 Validation Accuracy: 0.4309999942779541\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 1.2576738595962524 Validation Accuracy: 0.43160000443458557\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 1.2468525171279907 Validation Accuracy: 0.43380001187324524\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 1.2287710905075073 Validation Accuracy: 0.43299999833106995\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 1.2302873134613037 Validation Accuracy: 0.43560001254081726\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 1.2183411121368408 Validation Accuracy: 0.4277999997138977\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 1.2429524660110474 Validation Accuracy: 0.43619999289512634\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 1.2328993082046509 Validation Accuracy: 0.42800000309944153\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 1.2395551204681396 Validation Accuracy: 0.4381999969482422\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 1.2242157459259033 Validation Accuracy: 0.42980000376701355\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 1.2762521505355835 Validation Accuracy: 0.4357999861240387\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 1.25724458694458 Validation Accuracy: 0.4327999949455261\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 1.2058823108673096 Validation Accuracy: 0.436599999666214\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 1.2295608520507812 Validation Accuracy: 0.4359999895095825\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 1.1895155906677246 Validation Accuracy: 0.4390000104904175\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 1.2128033638000488 Validation Accuracy: 0.4352000057697296\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 1.2171474695205688 Validation Accuracy: 0.43639999628067017\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 1.2127711772918701 Validation Accuracy: 0.4359999895095825\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 1.1792834997177124 Validation Accuracy: 0.4309999942779541\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 1.1810848712921143 Validation Accuracy: 0.43700000643730164\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 1.22493314743042 Validation Accuracy: 0.4311999976634979\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 1.1933777332305908 Validation Accuracy: 0.43700000643730164\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 1.1922132968902588 Validation Accuracy: 0.4318000078201294\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 1.1816283464431763 Validation Accuracy: 0.4350000023841858\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 1.1798665523529053 Validation Accuracy: 0.43560001254081726\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 1.1958271265029907 Validation Accuracy: 0.4334000051021576\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 1.1991981267929077 Validation Accuracy: 0.43479999899864197\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 1.1927268505096436 Validation Accuracy: 0.4381999969482422\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 1.1969084739685059 Validation Accuracy: 0.4375999867916107\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 1.1830480098724365 Validation Accuracy: 0.43619999289512634\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 1.1530190706253052 Validation Accuracy: 0.4374000132083893\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 1.1747175455093384 Validation Accuracy: 0.43639999628067017\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 1.1600710153579712 Validation Accuracy: 0.4431999921798706\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 1.17350172996521 Validation Accuracy: 0.43380001187324524\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 1.1599376201629639 Validation Accuracy: 0.43799999356269836\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 1.1846226453781128 Validation Accuracy: 0.43639999628067017\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 1.1440516710281372 Validation Accuracy: 0.4399999976158142\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 1.1489137411117554 Validation Accuracy: 0.43860000371932983\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 1.1633981466293335 Validation Accuracy: 0.4390000104904175\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 1.1584136486053467 Validation Accuracy: 0.438400000333786\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 1.1695306301116943 Validation Accuracy: 0.43860000371932983\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 1.1578251123428345 Validation Accuracy: 0.44679999351501465\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 1.1446964740753174 Validation Accuracy: 0.43959999084472656\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 1.1406971216201782 Validation Accuracy: 0.4406000077724457\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 1.1531765460968018 Validation Accuracy: 0.43799999356269836\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 1.14801025390625 Validation Accuracy: 0.4440000057220459\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 1.1320692300796509 Validation Accuracy: 0.44339999556541443\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 1.1706701517105103 Validation Accuracy: 0.44020000100135803\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 1.1408591270446777 Validation Accuracy: 0.44600000977516174\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 1.1323190927505493 Validation Accuracy: 0.4429999887943268\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 1.1464543342590332 Validation Accuracy: 0.4429999887943268\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 1.1451361179351807 Validation Accuracy: 0.4438000023365021\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 1.1392756700515747 Validation Accuracy: 0.43619999289512634\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 1.1534268856048584 Validation Accuracy: 0.43939998745918274\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 1.141088843345642 Validation Accuracy: 0.4392000138759613\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 1.1410903930664062 Validation Accuracy: 0.43560001254081726\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 1.1369129419326782 Validation Accuracy: 0.44040000438690186\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 1.155051827430725 Validation Accuracy: 0.4424000084400177\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 1.1244659423828125 Validation Accuracy: 0.4480000138282776\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 1.1353613138198853 Validation Accuracy: 0.44359999895095825\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 1.1361157894134521 Validation Accuracy: 0.4415999948978424\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2836341857910156 Validation Accuracy: 0.11479999870061874\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.2902212142944336 Validation Accuracy: 0.12620000541210175\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.2909154891967773 Validation Accuracy: 0.15639999508857727\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.2713124752044678 Validation Accuracy: 0.16619999706745148\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.2943248748779297 Validation Accuracy: 0.18979999423027039\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.1745104789733887 Validation Accuracy: 0.20819999277591705\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 2.15468168258667 Validation Accuracy: 0.21379999816417694\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 2.073115110397339 Validation Accuracy: 0.2272000014781952\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 2.132960081100464 Validation Accuracy: 0.23420000076293945\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 2.1105780601501465 Validation Accuracy: 0.25540000200271606\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.091686964035034 Validation Accuracy: 0.251800000667572\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 2.0795841217041016 Validation Accuracy: 0.2669999897480011\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 2.0060667991638184 Validation Accuracy: 0.2775999903678894\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 2.0137362480163574 Validation Accuracy: 0.2815999984741211\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 2.0348305702209473 Validation Accuracy: 0.2874000072479248\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.0970840454101562 Validation Accuracy: 0.2847999930381775\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 2.034900426864624 Validation Accuracy: 0.29580000042915344\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.9499728679656982 Validation Accuracy: 0.30720001459121704\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.925390601158142 Validation Accuracy: 0.2955999970436096\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.9138195514678955 Validation Accuracy: 0.30320000648498535\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.077371120452881 Validation Accuracy: 0.32100000977516174\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.9872491359710693 Validation Accuracy: 0.3197999894618988\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.8674942255020142 Validation Accuracy: 0.32580000162124634\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.896868109703064 Validation Accuracy: 0.33480000495910645\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.9174124002456665 Validation Accuracy: 0.3325999975204468\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.068747043609619 Validation Accuracy: 0.3285999894142151\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.93498957157135 Validation Accuracy: 0.34940001368522644\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.8097469806671143 Validation Accuracy: 0.3490000069141388\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.8689298629760742 Validation Accuracy: 0.35100001096725464\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.8829352855682373 Validation Accuracy: 0.3582000136375427\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.030327796936035 Validation Accuracy: 0.34700000286102295\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.9041454792022705 Validation Accuracy: 0.3553999960422516\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.761488676071167 Validation Accuracy: 0.367000013589859\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.792464256286621 Validation Accuracy: 0.3637999892234802\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.8096227645874023 Validation Accuracy: 0.3635999858379364\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.01586651802063 Validation Accuracy: 0.3580000102519989\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.8731586933135986 Validation Accuracy: 0.36480000615119934\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.7268638610839844 Validation Accuracy: 0.3643999993801117\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.7568035125732422 Validation Accuracy: 0.37439998984336853\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.828904390335083 Validation Accuracy: 0.3675999939441681\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.027606964111328 Validation Accuracy: 0.36820000410079956\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.825018286705017 Validation Accuracy: 0.37040001153945923\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.654762625694275 Validation Accuracy: 0.37119999527931213\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.7558590173721313 Validation Accuracy: 0.37560001015663147\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.7953040599822998 Validation Accuracy: 0.3716000020503998\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.014232873916626 Validation Accuracy: 0.37220001220703125\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.8004757165908813 Validation Accuracy: 0.37700000405311584\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.6322591304779053 Validation Accuracy: 0.3783999979496002\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.697161078453064 Validation Accuracy: 0.39419999718666077\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.7612266540527344 Validation Accuracy: 0.38659998774528503\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.9727470874786377 Validation Accuracy: 0.37940001487731934\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.7617849111557007 Validation Accuracy: 0.39160001277923584\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.6014976501464844 Validation Accuracy: 0.3815999925136566\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.6964600086212158 Validation Accuracy: 0.38519999384880066\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.7435424327850342 Validation Accuracy: 0.3887999951839447\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.9731693267822266 Validation Accuracy: 0.3822000026702881\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.7406527996063232 Validation Accuracy: 0.399399995803833\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.5854427814483643 Validation Accuracy: 0.39820000529289246\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.6845893859863281 Validation Accuracy: 0.39660000801086426\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.7205960750579834 Validation Accuracy: 0.39739999175071716\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.9785820245742798 Validation Accuracy: 0.4009999930858612\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.698339819908142 Validation Accuracy: 0.4041999876499176\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.5118849277496338 Validation Accuracy: 0.3946000039577484\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.6336357593536377 Validation Accuracy: 0.40560001134872437\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.7117812633514404 Validation Accuracy: 0.4032000005245209\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.9907277822494507 Validation Accuracy: 0.40380001068115234\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.7235263586044312 Validation Accuracy: 0.41040000319480896\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 1.5293855667114258 Validation Accuracy: 0.39660000801086426\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.6225864887237549 Validation Accuracy: 0.40799999237060547\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.7157214879989624 Validation Accuracy: 0.40299999713897705\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.9842323064804077 Validation Accuracy: 0.4142000079154968\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.7176491022109985 Validation Accuracy: 0.4163999855518341\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.5296270847320557 Validation Accuracy: 0.41019999980926514\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.5944597721099854 Validation Accuracy: 0.41359999775886536\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.6797425746917725 Validation Accuracy: 0.41179999709129333\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.9417861700057983 Validation Accuracy: 0.41940000653266907\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.6578449010849 Validation Accuracy: 0.4131999909877777\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.4793739318847656 Validation Accuracy: 0.41620001196861267\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.5930495262145996 Validation Accuracy: 0.42320001125335693\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.6879593133926392 Validation Accuracy: 0.4185999929904938\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.9237310886383057 Validation Accuracy: 0.4284000098705292\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.6856132745742798 Validation Accuracy: 0.42739999294281006\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.5023313760757446 Validation Accuracy: 0.4163999855518341\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.544016718864441 Validation Accuracy: 0.4235999882221222\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.6740115880966187 Validation Accuracy: 0.4251999855041504\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.9770110845565796 Validation Accuracy: 0.4277999997138977\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.6755969524383545 Validation Accuracy: 0.4318000078201294\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.4059171676635742 Validation Accuracy: 0.42980000376701355\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.5147414207458496 Validation Accuracy: 0.43700000643730164\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 1.678070068359375 Validation Accuracy: 0.43160000443458557\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.9571259021759033 Validation Accuracy: 0.430400013923645\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.6606823205947876 Validation Accuracy: 0.43560001254081726\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.4379558563232422 Validation Accuracy: 0.43160000443458557\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.5099424123764038 Validation Accuracy: 0.43959999084472656\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 1.6822744607925415 Validation Accuracy: 0.4368000030517578\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.90655517578125 Validation Accuracy: 0.43860000371932983\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.6449216604232788 Validation Accuracy: 0.4410000145435333\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.3911405801773071 Validation Accuracy: 0.43959999084472656\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.4907915592193604 Validation Accuracy: 0.44699999690055847\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.6792871952056885 Validation Accuracy: 0.4339999854564667\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.8618513345718384 Validation Accuracy: 0.4465999901294708\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.6877257823944092 Validation Accuracy: 0.4480000138282776\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.3931567668914795 Validation Accuracy: 0.4458000063896179\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.4826951026916504 Validation Accuracy: 0.4449999928474426\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.7064456939697266 Validation Accuracy: 0.44440001249313354\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.8733339309692383 Validation Accuracy: 0.4505999982357025\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.626245141029358 Validation Accuracy: 0.44760000705718994\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.4271438121795654 Validation Accuracy: 0.4415999948978424\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 1.4922964572906494 Validation Accuracy: 0.44859999418258667\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 1.670480728149414 Validation Accuracy: 0.45100000500679016\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.8218605518341064 Validation Accuracy: 0.45500001311302185\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.6145166158676147 Validation Accuracy: 0.4521999955177307\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.4212357997894287 Validation Accuracy: 0.4537999927997589\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 1.4465210437774658 Validation Accuracy: 0.44760000705718994\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 1.6651502847671509 Validation Accuracy: 0.4498000144958496\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.778556227684021 Validation Accuracy: 0.4578000009059906\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.5639147758483887 Validation Accuracy: 0.45680001378059387\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 1.4239680767059326 Validation Accuracy: 0.4562000036239624\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 1.4419008493423462 Validation Accuracy: 0.45419999957084656\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 1.6666700839996338 Validation Accuracy: 0.45660001039505005\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.7932929992675781 Validation Accuracy: 0.46000000834465027\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 1.6011451482772827 Validation Accuracy: 0.4607999920845032\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 1.3935292959213257 Validation Accuracy: 0.45419999957084656\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 1.4531673192977905 Validation Accuracy: 0.453000009059906\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 1.6422046422958374 Validation Accuracy: 0.4564000070095062\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.7781517505645752 Validation Accuracy: 0.45840001106262207\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 1.5453965663909912 Validation Accuracy: 0.46219998598098755\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 1.373677372932434 Validation Accuracy: 0.4593999981880188\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 1.4394549131393433 Validation Accuracy: 0.4596000015735626\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 1.657973051071167 Validation Accuracy: 0.4586000144481659\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.7905069589614868 Validation Accuracy: 0.4618000090122223\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 1.5764648914337158 Validation Accuracy: 0.4618000090122223\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 1.3777137994766235 Validation Accuracy: 0.462799996137619\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 1.4587149620056152 Validation Accuracy: 0.4625999927520752\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 1.5999901294708252 Validation Accuracy: 0.46320000290870667\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.7690719366073608 Validation Accuracy: 0.46880000829696655\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 1.5672476291656494 Validation Accuracy: 0.4674000144004822\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 1.3545074462890625 Validation Accuracy: 0.4607999920845032\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 1.4136037826538086 Validation Accuracy: 0.46299999952316284\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 1.5684314966201782 Validation Accuracy: 0.4641999900341034\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.7038933038711548 Validation Accuracy: 0.4690000116825104\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 1.5758891105651855 Validation Accuracy: 0.47040000557899475\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 1.3486229181289673 Validation Accuracy: 0.47040000557899475\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 1.4097011089324951 Validation Accuracy: 0.4706000089645386\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 1.6010468006134033 Validation Accuracy: 0.46239998936653137\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.6979801654815674 Validation Accuracy: 0.46799999475479126\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 1.5222305059432983 Validation Accuracy: 0.47119998931884766\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 1.3371117115020752 Validation Accuracy: 0.46860000491142273\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 1.4077759981155396 Validation Accuracy: 0.4742000102996826\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 1.5437424182891846 Validation Accuracy: 0.4691999852657318\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.6547256708145142 Validation Accuracy: 0.4742000102996826\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 1.5542247295379639 Validation Accuracy: 0.4702000021934509\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 1.327711582183838 Validation Accuracy: 0.4684000015258789\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 1.3959028720855713 Validation Accuracy: 0.4772000014781952\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 1.5758670568466187 Validation Accuracy: 0.46860000491142273\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.653280258178711 Validation Accuracy: 0.4740000069141388\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 1.5000627040863037 Validation Accuracy: 0.48019999265670776\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 1.3353488445281982 Validation Accuracy: 0.4812000095844269\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 1.3818273544311523 Validation Accuracy: 0.4758000075817108\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 1.5360805988311768 Validation Accuracy: 0.4749999940395355\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.6016089916229248 Validation Accuracy: 0.47380000352859497\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 1.4518201351165771 Validation Accuracy: 0.47440001368522644\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 1.3205363750457764 Validation Accuracy: 0.477400004863739\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 1.3777215480804443 Validation Accuracy: 0.4779999852180481\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 1.528814673423767 Validation Accuracy: 0.47200000286102295\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.6546630859375 Validation Accuracy: 0.4697999954223633\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 1.4713852405548096 Validation Accuracy: 0.47859999537467957\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 1.3510488271713257 Validation Accuracy: 0.4837999939918518\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 1.364931583404541 Validation Accuracy: 0.4797999858856201\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 1.5012115240097046 Validation Accuracy: 0.48399999737739563\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.6097393035888672 Validation Accuracy: 0.47999998927116394\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 1.4381747245788574 Validation Accuracy: 0.48820000886917114\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 1.3150780200958252 Validation Accuracy: 0.48240000009536743\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 1.354835867881775 Validation Accuracy: 0.48159998655319214\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 1.5147196054458618 Validation Accuracy: 0.47859999537467957\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.5723875761032104 Validation Accuracy: 0.48080000281333923\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 1.4507545232772827 Validation Accuracy: 0.4885999858379364\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 1.318388819694519 Validation Accuracy: 0.4790000021457672\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 1.3410961627960205 Validation Accuracy: 0.48660001158714294\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 1.5118181705474854 Validation Accuracy: 0.4862000048160553\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.527655005455017 Validation Accuracy: 0.47859999537467957\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 1.4456623792648315 Validation Accuracy: 0.49140000343322754\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 1.3213772773742676 Validation Accuracy: 0.4893999993801117\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 1.361721396446228 Validation Accuracy: 0.4812000095844269\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 1.4660032987594604 Validation Accuracy: 0.4936000108718872\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.5629546642303467 Validation Accuracy: 0.48420000076293945\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 1.452984094619751 Validation Accuracy: 0.48399999737739563\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 1.3412954807281494 Validation Accuracy: 0.4867999851703644\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 1.3266748189926147 Validation Accuracy: 0.4878000020980835\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 1.4829118251800537 Validation Accuracy: 0.48899999260902405\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.4983558654785156 Validation Accuracy: 0.49059998989105225\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 1.369439959526062 Validation Accuracy: 0.4918000102043152\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 1.32195246219635 Validation Accuracy: 0.4934000074863434\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 1.3056700229644775 Validation Accuracy: 0.48660001158714294\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 1.4479573965072632 Validation Accuracy: 0.4918000102043152\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.4958041906356812 Validation Accuracy: 0.4918000102043152\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 1.4328529834747314 Validation Accuracy: 0.4934000074863434\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 1.3258394002914429 Validation Accuracy: 0.4936000108718872\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 1.3241711854934692 Validation Accuracy: 0.4918000102043152\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 1.4550905227661133 Validation Accuracy: 0.49639999866485596\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.4800631999969482 Validation Accuracy: 0.4878000020980835\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 1.3813000917434692 Validation Accuracy: 0.501800000667572\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 1.304869294166565 Validation Accuracy: 0.4927999973297119\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 1.2981866598129272 Validation Accuracy: 0.4878000020980835\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 1.4456942081451416 Validation Accuracy: 0.4950000047683716\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.491727352142334 Validation Accuracy: 0.4918000102043152\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 1.3316179513931274 Validation Accuracy: 0.4943999946117401\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 1.3043906688690186 Validation Accuracy: 0.49880000948905945\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 1.2802419662475586 Validation Accuracy: 0.49720001220703125\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 1.4282808303833008 Validation Accuracy: 0.4952000081539154\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.4935815334320068 Validation Accuracy: 0.49239999055862427\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 1.381253957748413 Validation Accuracy: 0.49559998512268066\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 1.3046574592590332 Validation Accuracy: 0.4975999891757965\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 1.2823474407196045 Validation Accuracy: 0.5012000203132629\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 1.4231688976287842 Validation Accuracy: 0.4986000061035156\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.4309298992156982 Validation Accuracy: 0.4934000074863434\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 1.3581054210662842 Validation Accuracy: 0.5008000135421753\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 1.323530673980713 Validation Accuracy: 0.4975999891757965\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 1.2892990112304688 Validation Accuracy: 0.4986000061035156\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 1.4424175024032593 Validation Accuracy: 0.49720001220703125\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.434524655342102 Validation Accuracy: 0.5027999877929688\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 1.3739268779754639 Validation Accuracy: 0.5\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 1.3006216287612915 Validation Accuracy: 0.5045999884605408\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 1.3048862218856812 Validation Accuracy: 0.5001999735832214\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 1.4150350093841553 Validation Accuracy: 0.5004000067710876\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.4531958103179932 Validation Accuracy: 0.4973999857902527\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 1.3742015361785889 Validation Accuracy: 0.5034000277519226\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 1.268673062324524 Validation Accuracy: 0.504800021648407\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 1.271010160446167 Validation Accuracy: 0.5037999749183655\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 1.43980073928833 Validation Accuracy: 0.49639999866485596\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.4446847438812256 Validation Accuracy: 0.501800000667572\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 1.3436018228530884 Validation Accuracy: 0.504800021648407\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 1.2595829963684082 Validation Accuracy: 0.49480000138282776\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 1.2782875299453735 Validation Accuracy: 0.5044000148773193\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 1.4009859561920166 Validation Accuracy: 0.5004000067710876\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.4464941024780273 Validation Accuracy: 0.4984000027179718\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 1.336916208267212 Validation Accuracy: 0.5059999823570251\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 1.242989182472229 Validation Accuracy: 0.5040000081062317\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 1.25680410861969 Validation Accuracy: 0.5099999904632568\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 1.422296166419983 Validation Accuracy: 0.503600001335144\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.431958794593811 Validation Accuracy: 0.5058000087738037\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 1.3485215902328491 Validation Accuracy: 0.5076000094413757\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 1.2624279260635376 Validation Accuracy: 0.5063999891281128\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 1.261194109916687 Validation Accuracy: 0.5094000101089478\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 1.3867897987365723 Validation Accuracy: 0.5022000074386597\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.387697458267212 Validation Accuracy: 0.504800021648407\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 1.3748910427093506 Validation Accuracy: 0.5099999904632568\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 1.2591521739959717 Validation Accuracy: 0.5019999742507935\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 1.242012858390808 Validation Accuracy: 0.5009999871253967\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 1.4031827449798584 Validation Accuracy: 0.5080000162124634\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.412046194076538 Validation Accuracy: 0.5063999891281128\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 1.3523309230804443 Validation Accuracy: 0.5088000297546387\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 1.232574701309204 Validation Accuracy: 0.5103999972343445\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 1.2148473262786865 Validation Accuracy: 0.5063999891281128\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 1.4010589122772217 Validation Accuracy: 0.5073999762535095\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.3670684099197388 Validation Accuracy: 0.5063999891281128\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 1.2496588230133057 Validation Accuracy: 0.5054000020027161\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 1.2404091358184814 Validation Accuracy: 0.5121999979019165\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 1.2219998836517334 Validation Accuracy: 0.5099999904632568\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 1.4018046855926514 Validation Accuracy: 0.5045999884605408\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.3866393566131592 Validation Accuracy: 0.5055999755859375\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 1.330039381980896 Validation Accuracy: 0.508400022983551\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 1.2491137981414795 Validation Accuracy: 0.5062000155448914\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 1.2157602310180664 Validation Accuracy: 0.5109999775886536\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 1.4105263948440552 Validation Accuracy: 0.5052000284194946\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.3669177293777466 Validation Accuracy: 0.5090000033378601\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 1.3265174627304077 Validation Accuracy: 0.5116000175476074\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 1.2314287424087524 Validation Accuracy: 0.5134000182151794\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 1.2270500659942627 Validation Accuracy: 0.5088000297546387\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 1.3887431621551514 Validation Accuracy: 0.5091999769210815\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.3384201526641846 Validation Accuracy: 0.5135999917984009\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 1.2927820682525635 Validation Accuracy: 0.5144000053405762\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 1.2364985942840576 Validation Accuracy: 0.5081999897956848\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 1.2180851697921753 Validation Accuracy: 0.5108000040054321\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 1.4150618314743042 Validation Accuracy: 0.5085999965667725\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.3354145288467407 Validation Accuracy: 0.5080000162124634\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 1.2677834033966064 Validation Accuracy: 0.5134000182151794\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 1.21868896484375 Validation Accuracy: 0.5094000101089478\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 1.2255536317825317 Validation Accuracy: 0.5098000168800354\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 1.3700710535049438 Validation Accuracy: 0.5135999917984009\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.3434864282608032 Validation Accuracy: 0.5184000134468079\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 1.242759346961975 Validation Accuracy: 0.5109999775886536\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 1.2421327829360962 Validation Accuracy: 0.5153999924659729\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 1.2110183238983154 Validation Accuracy: 0.5130000114440918\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 1.3735804557800293 Validation Accuracy: 0.5157999992370605\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.3476754426956177 Validation Accuracy: 0.5113999843597412\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 1.2740989923477173 Validation Accuracy: 0.5123999714851379\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 1.214551568031311 Validation Accuracy: 0.5112000107765198\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 1.201282262802124 Validation Accuracy: 0.517799973487854\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 1.3465107679367065 Validation Accuracy: 0.515999972820282\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.3263828754425049 Validation Accuracy: 0.5175999999046326\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 1.243003249168396 Validation Accuracy: 0.5171999931335449\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 1.2265037298202515 Validation Accuracy: 0.5139999985694885\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 1.1654855012893677 Validation Accuracy: 0.5157999992370605\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 1.3558824062347412 Validation Accuracy: 0.5130000114440918\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.3530144691467285 Validation Accuracy: 0.5103999972343445\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 1.2553820610046387 Validation Accuracy: 0.5171999931335449\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 1.2532355785369873 Validation Accuracy: 0.5116000175476074\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 1.2128312587738037 Validation Accuracy: 0.5162000060081482\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 1.338529348373413 Validation Accuracy: 0.5162000060081482\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.3207401037216187 Validation Accuracy: 0.5188000202178955\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 1.2394440174102783 Validation Accuracy: 0.5185999870300293\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 1.2031291723251343 Validation Accuracy: 0.5134000182151794\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 1.1689178943634033 Validation Accuracy: 0.5181999802589417\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 1.345953106880188 Validation Accuracy: 0.5171999931335449\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.3238545656204224 Validation Accuracy: 0.5134000182151794\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 1.2266429662704468 Validation Accuracy: 0.5239999890327454\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 1.2040990591049194 Validation Accuracy: 0.5148000121116638\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 1.149573564529419 Validation Accuracy: 0.515999972820282\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 1.330012559890747 Validation Accuracy: 0.5170000195503235\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.3526405096054077 Validation Accuracy: 0.5170000195503235\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 1.2459040880203247 Validation Accuracy: 0.5202000141143799\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 1.1930979490280151 Validation Accuracy: 0.5181999802589417\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 1.1619741916656494 Validation Accuracy: 0.5210000276565552\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 1.3134448528289795 Validation Accuracy: 0.5153999924659729\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.3272182941436768 Validation Accuracy: 0.5175999999046326\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 1.2575315237045288 Validation Accuracy: 0.5189999938011169\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 1.1990342140197754 Validation Accuracy: 0.5181999802589417\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 1.1264314651489258 Validation Accuracy: 0.5189999938011169\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 1.2999658584594727 Validation Accuracy: 0.5112000107765198\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.2874232530593872 Validation Accuracy: 0.520799994468689\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 1.2141426801681519 Validation Accuracy: 0.5221999883651733\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 1.1962385177612305 Validation Accuracy: 0.5153999924659729\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 1.1421053409576416 Validation Accuracy: 0.5198000073432922\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 1.3064801692962646 Validation Accuracy: 0.510200023651123\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.2928088903427124 Validation Accuracy: 0.5199999809265137\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 1.1921361684799194 Validation Accuracy: 0.5248000025749207\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 1.1927731037139893 Validation Accuracy: 0.5157999992370605\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 1.1397353410720825 Validation Accuracy: 0.5267999768257141\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 1.3125582933425903 Validation Accuracy: 0.5185999870300293\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.275840163230896 Validation Accuracy: 0.5189999938011169\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 1.1796610355377197 Validation Accuracy: 0.5221999883651733\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 1.191847562789917 Validation Accuracy: 0.5217999815940857\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 1.128964900970459 Validation Accuracy: 0.5296000242233276\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 1.3077106475830078 Validation Accuracy: 0.5203999876976013\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.2801381349563599 Validation Accuracy: 0.5230000019073486\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 1.2413146495819092 Validation Accuracy: 0.5249999761581421\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 1.1860034465789795 Validation Accuracy: 0.5285999774932861\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 1.1436424255371094 Validation Accuracy: 0.52920001745224\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 1.2863433361053467 Validation Accuracy: 0.525600016117096\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.260759711265564 Validation Accuracy: 0.5275999903678894\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 1.212197184562683 Validation Accuracy: 0.5264000296592712\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 1.1936572790145874 Validation Accuracy: 0.5198000073432922\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 1.1194117069244385 Validation Accuracy: 0.5198000073432922\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 1.3121743202209473 Validation Accuracy: 0.524399995803833\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.2848163843154907 Validation Accuracy: 0.5156000256538391\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 1.1910200119018555 Validation Accuracy: 0.5212000012397766\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 1.186399221420288 Validation Accuracy: 0.520799994468689\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 1.1209338903427124 Validation Accuracy: 0.5270000100135803\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 1.2938005924224854 Validation Accuracy: 0.5216000080108643\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.257838487625122 Validation Accuracy: 0.5217999815940857\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 1.2413833141326904 Validation Accuracy: 0.5238000154495239\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 1.1824992895126343 Validation Accuracy: 0.5199999809265137\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 1.1153157949447632 Validation Accuracy: 0.5270000100135803\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 1.282604455947876 Validation Accuracy: 0.5216000080108643\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.2457882165908813 Validation Accuracy: 0.5284000039100647\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 1.1718196868896484 Validation Accuracy: 0.5228000283241272\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 1.1724390983581543 Validation Accuracy: 0.5230000019073486\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 1.133304238319397 Validation Accuracy: 0.5296000242233276\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 1.2754329442977905 Validation Accuracy: 0.5252000093460083\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.2128998041152954 Validation Accuracy: 0.5230000019073486\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 1.207979679107666 Validation Accuracy: 0.5249999761581421\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 1.1669256687164307 Validation Accuracy: 0.5275999903678894\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 1.1289374828338623 Validation Accuracy: 0.5199999809265137\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 1.2591091394424438 Validation Accuracy: 0.522599995136261\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.246976613998413 Validation Accuracy: 0.5249999761581421\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 1.184435248374939 Validation Accuracy: 0.5285999774932861\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 1.170503854751587 Validation Accuracy: 0.5246000289916992\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 1.1208255290985107 Validation Accuracy: 0.5206000208854675\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 1.2634233236312866 Validation Accuracy: 0.5257999897003174\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.249006986618042 Validation Accuracy: 0.5242000222206116\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 1.1750171184539795 Validation Accuracy: 0.5224000215530396\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 1.1356072425842285 Validation Accuracy: 0.5228000283241272\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 1.102846384048462 Validation Accuracy: 0.524399995803833\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 1.2534120082855225 Validation Accuracy: 0.5242000222206116\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.282646894454956 Validation Accuracy: 0.5188000202178955\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 1.138435959815979 Validation Accuracy: 0.5260000228881836\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 1.1507306098937988 Validation Accuracy: 0.5212000012397766\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 1.1018553972244263 Validation Accuracy: 0.5275999903678894\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 1.2466682195663452 Validation Accuracy: 0.5252000093460083\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.2097487449645996 Validation Accuracy: 0.5299999713897705\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 1.1981693506240845 Validation Accuracy: 0.5252000093460083\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 1.1224018335342407 Validation Accuracy: 0.5257999897003174\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 1.0943454504013062 Validation Accuracy: 0.5260000228881836\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 1.2596193552017212 Validation Accuracy: 0.5221999883651733\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.2213205099105835 Validation Accuracy: 0.5271999835968018\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 1.1918246746063232 Validation Accuracy: 0.5264000296592712\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 1.1091032028198242 Validation Accuracy: 0.5266000032424927\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 1.1000667810440063 Validation Accuracy: 0.5289999842643738\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 1.2630259990692139 Validation Accuracy: 0.5234000086784363\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.249932050704956 Validation Accuracy: 0.5246000289916992\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 1.203385353088379 Validation Accuracy: 0.5293999910354614\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 1.1402723789215088 Validation Accuracy: 0.5248000025749207\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 1.0769689083099365 Validation Accuracy: 0.527400016784668\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 1.2366209030151367 Validation Accuracy: 0.5264000296592712\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.2029633522033691 Validation Accuracy: 0.527400016784668\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 1.116344690322876 Validation Accuracy: 0.5260000228881836\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 1.1332072019577026 Validation Accuracy: 0.5275999903678894\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 1.087985873222351 Validation Accuracy: 0.5234000086784363\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 1.2898836135864258 Validation Accuracy: 0.5234000086784363\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.1903561353683472 Validation Accuracy: 0.5289999842643738\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 1.144242525100708 Validation Accuracy: 0.5271999835968018\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 1.1223095655441284 Validation Accuracy: 0.5288000106811523\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 1.0950281620025635 Validation Accuracy: 0.5324000120162964\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 1.284899115562439 Validation Accuracy: 0.5332000255584717\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.1948511600494385 Validation Accuracy: 0.5285999774932861\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 1.1382709741592407 Validation Accuracy: 0.5257999897003174\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 1.1105300188064575 Validation Accuracy: 0.5314000248908997\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 1.1163158416748047 Validation Accuracy: 0.5332000255584717\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 1.2480359077453613 Validation Accuracy: 0.5289999842643738\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.191730260848999 Validation Accuracy: 0.5315999984741211\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 1.1606965065002441 Validation Accuracy: 0.5297999978065491\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 1.119896650314331 Validation Accuracy: 0.5242000222206116\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 1.0827436447143555 Validation Accuracy: 0.5307999849319458\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 1.254101276397705 Validation Accuracy: 0.5297999978065491\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.1867619752883911 Validation Accuracy: 0.531000018119812\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 1.1185804605484009 Validation Accuracy: 0.5260000228881836\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 1.1113166809082031 Validation Accuracy: 0.525600016117096\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 1.0902674198150635 Validation Accuracy: 0.527400016784668\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 1.2589435577392578 Validation Accuracy: 0.5315999984741211\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.1703218221664429 Validation Accuracy: 0.5260000228881836\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 1.126592993736267 Validation Accuracy: 0.5281999707221985\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 1.1218702793121338 Validation Accuracy: 0.5270000100135803\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 1.0928276777267456 Validation Accuracy: 0.5307999849319458\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 1.2116817235946655 Validation Accuracy: 0.52920001745224\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.1666464805603027 Validation Accuracy: 0.5293999910354614\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 1.1569409370422363 Validation Accuracy: 0.527999997138977\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 1.081658124923706 Validation Accuracy: 0.5296000242233276\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 1.069559097290039 Validation Accuracy: 0.5342000126838684\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 1.235272765159607 Validation Accuracy: 0.5325999855995178\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.188206672668457 Validation Accuracy: 0.5321999788284302\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 1.0977728366851807 Validation Accuracy: 0.531000018119812\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 1.092956304550171 Validation Accuracy: 0.532800018787384\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 1.0842357873916626 Validation Accuracy: 0.5357999801635742\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 1.2225916385650635 Validation Accuracy: 0.5388000011444092\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.1995689868927002 Validation Accuracy: 0.5267999768257141\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 1.129056692123413 Validation Accuracy: 0.5281999707221985\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 1.0838816165924072 Validation Accuracy: 0.5317999720573425\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 1.0714300870895386 Validation Accuracy: 0.5333999991416931\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 1.2118898630142212 Validation Accuracy: 0.5296000242233276\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.1769956350326538 Validation Accuracy: 0.52920001745224\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 1.133874535560608 Validation Accuracy: 0.5342000126838684\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 1.0774414539337158 Validation Accuracy: 0.5299999713897705\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 1.1002715826034546 Validation Accuracy: 0.5357999801635742\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 1.2336026430130005 Validation Accuracy: 0.5370000004768372\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.1685199737548828 Validation Accuracy: 0.5302000045776367\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 1.136268973350525 Validation Accuracy: 0.5382000207901001\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 1.086807131767273 Validation Accuracy: 0.5289999842643738\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 1.0670255422592163 Validation Accuracy: 0.5353999733924866\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 1.2308136224746704 Validation Accuracy: 0.5306000113487244\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.1912695169448853 Validation Accuracy: 0.5364000201225281\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 1.1232478618621826 Validation Accuracy: 0.527400016784668\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 1.0657320022583008 Validation Accuracy: 0.5321999788284302\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 1.069812536239624 Validation Accuracy: 0.5325999855995178\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 1.268122911453247 Validation Accuracy: 0.5338000059127808\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.1749433279037476 Validation Accuracy: 0.5374000072479248\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 1.1066490411758423 Validation Accuracy: 0.5296000242233276\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 1.0808292627334595 Validation Accuracy: 0.5386000275611877\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 1.0623284578323364 Validation Accuracy: 0.5442000031471252\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 1.209692120552063 Validation Accuracy: 0.5360000133514404\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.1690895557403564 Validation Accuracy: 0.5311999917030334\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 1.1298023462295532 Validation Accuracy: 0.531000018119812\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 1.0930894613265991 Validation Accuracy: 0.5343999862670898\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 1.0481380224227905 Validation Accuracy: 0.5375999808311462\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 1.2256402969360352 Validation Accuracy: 0.534600019454956\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.1668403148651123 Validation Accuracy: 0.534600019454956\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 1.113491415977478 Validation Accuracy: 0.5315999984741211\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 1.0822961330413818 Validation Accuracy: 0.5374000072479248\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 1.0693901777267456 Validation Accuracy: 0.5389999747276306\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 1.2263327836990356 Validation Accuracy: 0.5267999768257141\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 1.1585363149642944 Validation Accuracy: 0.5393999814987183\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 1.1246678829193115 Validation Accuracy: 0.5360000133514404\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 1.0755558013916016 Validation Accuracy: 0.5375999808311462\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 1.03581702709198 Validation Accuracy: 0.5386000275611877\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 1.2103166580200195 Validation Accuracy: 0.5311999917030334\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 1.1740312576293945 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 1.1068257093429565 Validation Accuracy: 0.5325999855995178\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 1.0654237270355225 Validation Accuracy: 0.5351999998092651\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 1.0555918216705322 Validation Accuracy: 0.5332000255584717\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 1.2183825969696045 Validation Accuracy: 0.5321999788284302\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 1.1587287187576294 Validation Accuracy: 0.5314000248908997\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 1.0591152906417847 Validation Accuracy: 0.532800018787384\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 1.068810224533081 Validation Accuracy: 0.5347999930381775\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 1.0401725769042969 Validation Accuracy: 0.5361999869346619\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 1.222885012626648 Validation Accuracy: 0.5401999950408936\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 1.16056227684021 Validation Accuracy: 0.5343999862670898\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 1.1127761602401733 Validation Accuracy: 0.5365999937057495\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 1.0614174604415894 Validation Accuracy: 0.5396000146865845\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 1.0275096893310547 Validation Accuracy: 0.5392000079154968\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 1.1970784664154053 Validation Accuracy: 0.5361999869346619\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.175289273262024 Validation Accuracy: 0.5386000275611877\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 1.0964657068252563 Validation Accuracy: 0.5360000133514404\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 1.0758352279663086 Validation Accuracy: 0.5429999828338623\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 1.0799567699432373 Validation Accuracy: 0.5396000146865845\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 1.19419264793396 Validation Accuracy: 0.532800018787384\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.169834017753601 Validation Accuracy: 0.5382000207901001\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 1.1151599884033203 Validation Accuracy: 0.5356000065803528\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 1.0596439838409424 Validation Accuracy: 0.5307999849319458\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 1.0672752857208252 Validation Accuracy: 0.5418000221252441\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 1.2039939165115356 Validation Accuracy: 0.5371999740600586\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 1.1504665613174438 Validation Accuracy: 0.5371999740600586\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 1.0949405431747437 Validation Accuracy: 0.5350000262260437\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 1.0839836597442627 Validation Accuracy: 0.5339999794960022\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 1.0513572692871094 Validation Accuracy: 0.5422000288963318\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 1.173405408859253 Validation Accuracy: 0.5343999862670898\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 1.1399486064910889 Validation Accuracy: 0.5374000072479248\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 1.09170663356781 Validation Accuracy: 0.5360000133514404\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 1.0633395910263062 Validation Accuracy: 0.5422000288963318\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 1.0348317623138428 Validation Accuracy: 0.5382000207901001\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 1.1999024152755737 Validation Accuracy: 0.5389999747276306\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 1.1602061986923218 Validation Accuracy: 0.5418000221252441\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 1.1242514848709106 Validation Accuracy: 0.5378000140190125\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 1.0676767826080322 Validation Accuracy: 0.5357999801635742\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 1.0590078830718994 Validation Accuracy: 0.5418000221252441\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 1.1935935020446777 Validation Accuracy: 0.5374000072479248\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 1.1398066282272339 Validation Accuracy: 0.5375999808311462\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 1.1161969900131226 Validation Accuracy: 0.5393999814987183\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 1.0536861419677734 Validation Accuracy: 0.5335999727249146\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 1.0586836338043213 Validation Accuracy: 0.5446000099182129\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 1.228150486946106 Validation Accuracy: 0.5370000004768372\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 1.136775255203247 Validation Accuracy: 0.5386000275611877\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 1.084977388381958 Validation Accuracy: 0.5379999876022339\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 1.077483892440796 Validation Accuracy: 0.5375999808311462\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 1.0425126552581787 Validation Accuracy: 0.5446000099182129\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 1.1777215003967285 Validation Accuracy: 0.54339998960495\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 1.1392772197723389 Validation Accuracy: 0.5404000282287598\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 1.0620378255844116 Validation Accuracy: 0.5382000207901001\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 1.0649268627166748 Validation Accuracy: 0.5419999957084656\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 1.0608679056167603 Validation Accuracy: 0.5460000038146973\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 1.1854082345962524 Validation Accuracy: 0.5437999963760376\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 1.1442618370056152 Validation Accuracy: 0.5410000085830688\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 1.1059825420379639 Validation Accuracy: 0.5397999882698059\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 1.0570199489593506 Validation Accuracy: 0.5442000031471252\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 1.0427032709121704 Validation Accuracy: 0.5443999767303467\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 1.2172046899795532 Validation Accuracy: 0.5443999767303467\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 1.1635544300079346 Validation Accuracy: 0.5397999882698059\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 1.096909999847412 Validation Accuracy: 0.5389999747276306\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 1.0495761632919312 Validation Accuracy: 0.5401999950408936\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 1.0524705648422241 Validation Accuracy: 0.5400000214576721\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 1.1651742458343506 Validation Accuracy: 0.5353999733924866\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 1.1519016027450562 Validation Accuracy: 0.5371999740600586\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 1.0773804187774658 Validation Accuracy: 0.5425999760627747\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 1.067883849143982 Validation Accuracy: 0.5404000282287598\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 1.026293396949768 Validation Accuracy: 0.54339998960495\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 1.1932541131973267 Validation Accuracy: 0.5382000207901001\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 1.15023672580719 Validation Accuracy: 0.5406000018119812\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 1.1209425926208496 Validation Accuracy: 0.5442000031471252\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 1.0266530513763428 Validation Accuracy: 0.5375999808311462\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 1.057424545288086 Validation Accuracy: 0.5396000146865845\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 1.1832454204559326 Validation Accuracy: 0.5415999889373779\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 1.1413419246673584 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 1.064445972442627 Validation Accuracy: 0.5393999814987183\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 1.0301767587661743 Validation Accuracy: 0.5482000112533569\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 1.0313968658447266 Validation Accuracy: 0.5424000024795532\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 1.196162462234497 Validation Accuracy: 0.545799970626831\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 1.1419241428375244 Validation Accuracy: 0.5361999869346619\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 1.1032640933990479 Validation Accuracy: 0.5389999747276306\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 1.0332363843917847 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 1.041404366493225 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 1.1769931316375732 Validation Accuracy: 0.5419999957084656\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 1.1648499965667725 Validation Accuracy: 0.5424000024795532\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 1.0613645315170288 Validation Accuracy: 0.5440000295639038\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 1.0336374044418335 Validation Accuracy: 0.5397999882698059\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 1.000478982925415 Validation Accuracy: 0.5410000085830688\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 1.2031981945037842 Validation Accuracy: 0.5415999889373779\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 1.1085751056671143 Validation Accuracy: 0.5442000031471252\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 1.053323745727539 Validation Accuracy: 0.5418000221252441\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 1.0279067754745483 Validation Accuracy: 0.5415999889373779\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 1.017486572265625 Validation Accuracy: 0.5424000024795532\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 1.1749427318572998 Validation Accuracy: 0.5397999882698059\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 1.1348521709442139 Validation Accuracy: 0.5383999943733215\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 1.073241949081421 Validation Accuracy: 0.5454000234603882\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 1.0364625453948975 Validation Accuracy: 0.5401999950408936\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 1.0087333917617798 Validation Accuracy: 0.5432000160217285\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 1.2102233171463013 Validation Accuracy: 0.5436000227928162\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 1.1457408666610718 Validation Accuracy: 0.5418000221252441\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 1.0604768991470337 Validation Accuracy: 0.5356000065803528\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 1.024911642074585 Validation Accuracy: 0.5422000288963318\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 1.0119295120239258 Validation Accuracy: 0.5446000099182129\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 1.2098596096038818 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 1.108635663986206 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 1.0708274841308594 Validation Accuracy: 0.5415999889373779\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 1.0343854427337646 Validation Accuracy: 0.5370000004768372\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 1.0216810703277588 Validation Accuracy: 0.548799991607666\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 1.223431944847107 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 1.137055516242981 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 1.047075867652893 Validation Accuracy: 0.5383999943733215\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 1.0397323369979858 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 1.0023667812347412 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 1.1935794353485107 Validation Accuracy: 0.5425999760627747\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 1.139556884765625 Validation Accuracy: 0.5468000173568726\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 1.0290635824203491 Validation Accuracy: 0.5440000295639038\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 1.0279881954193115 Validation Accuracy: 0.5393999814987183\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 1.0131146907806396 Validation Accuracy: 0.5486000180244446\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 1.1788311004638672 Validation Accuracy: 0.545199990272522\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 1.1239579916000366 Validation Accuracy: 0.5454000234603882\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 1.0670722723007202 Validation Accuracy: 0.5504000186920166\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 1.0279741287231445 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 0.999716579914093 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 1.2059036493301392 Validation Accuracy: 0.546999990940094\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 1.1178005933761597 Validation Accuracy: 0.5411999821662903\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 1.1186493635177612 Validation Accuracy: 0.5432000160217285\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 1.021977186203003 Validation Accuracy: 0.5404000282287598\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 1.0074436664581299 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 1.2067378759384155 Validation Accuracy: 0.5442000031471252\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 1.115351676940918 Validation Accuracy: 0.5461999773979187\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 1.060486912727356 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 1.0219309329986572 Validation Accuracy: 0.546999990940094\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 0.985765278339386 Validation Accuracy: 0.5446000099182129\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 1.239530086517334 Validation Accuracy: 0.5478000044822693\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 1.118651032447815 Validation Accuracy: 0.550000011920929\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 1.1044687032699585 Validation Accuracy: 0.545199990272522\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 1.0292208194732666 Validation Accuracy: 0.546999990940094\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.9904006719589233 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 1.1759777069091797 Validation Accuracy: 0.5407999753952026\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 1.1157402992248535 Validation Accuracy: 0.5461999773979187\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 1.0422279834747314 Validation Accuracy: 0.5419999957084656\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 1.0374959707260132 Validation Accuracy: 0.5401999950408936\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 1.0114715099334717 Validation Accuracy: 0.5443999767303467\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 1.1980739831924438 Validation Accuracy: 0.5496000051498413\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 1.1025320291519165 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 1.045272707939148 Validation Accuracy: 0.545199990272522\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 1.032750129699707 Validation Accuracy: 0.5461999773979187\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.9919277429580688 Validation Accuracy: 0.5454000234603882\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 1.2283058166503906 Validation Accuracy: 0.5454000234603882\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 1.1151483058929443 Validation Accuracy: 0.5483999848365784\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 1.028996229171753 Validation Accuracy: 0.5404000282287598\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 1.005037784576416 Validation Accuracy: 0.5501999855041504\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.9839898943901062 Validation Accuracy: 0.5490000247955322\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 1.2398593425750732 Validation Accuracy: 0.5493999719619751\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 1.130275011062622 Validation Accuracy: 0.5460000038146973\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 1.066438913345337 Validation Accuracy: 0.5418000221252441\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 1.0040233135223389 Validation Accuracy: 0.546999990940094\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.9757243990898132 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 1.2015281915664673 Validation Accuracy: 0.5478000044822693\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 1.0926414728164673 Validation Accuracy: 0.5472000241279602\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 1.0380299091339111 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 1.023837685585022 Validation Accuracy: 0.5401999950408936\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.9829870462417603 Validation Accuracy: 0.5483999848365784\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 1.1772115230560303 Validation Accuracy: 0.546999990940094\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 1.1004480123519897 Validation Accuracy: 0.551800012588501\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 1.0392379760742188 Validation Accuracy: 0.5486000180244446\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.9819971919059753 Validation Accuracy: 0.5472000241279602\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 0.9823265075683594 Validation Accuracy: 0.5483999848365784\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 1.200231909751892 Validation Accuracy: 0.5522000193595886\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 1.0865566730499268 Validation Accuracy: 0.5504000186920166\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 1.056563138961792 Validation Accuracy: 0.5522000193595886\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.9996010065078735 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.9853675961494446 Validation Accuracy: 0.550000011920929\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 1.2278575897216797 Validation Accuracy: 0.5529999732971191\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 1.0816619396209717 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 1.0264091491699219 Validation Accuracy: 0.5450000166893005\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 1.007178544998169 Validation Accuracy: 0.5468000173568726\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.9684039950370789 Validation Accuracy: 0.5568000078201294\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 1.223714828491211 Validation Accuracy: 0.553600013256073\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 1.096702218055725 Validation Accuracy: 0.5493999719619751\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 1.0178619623184204 Validation Accuracy: 0.550599992275238\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.9730215072631836 Validation Accuracy: 0.545199990272522\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.988201916217804 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 1.2155910730361938 Validation Accuracy: 0.5478000044822693\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 1.1055951118469238 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 1.0227619409561157 Validation Accuracy: 0.5515999794006348\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 1.0006330013275146 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.9686515927314758 Validation Accuracy: 0.5496000051498413\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 1.2065502405166626 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 1.1002295017242432 Validation Accuracy: 0.5540000200271606\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 1.0250051021575928 Validation Accuracy: 0.5486000180244446\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.9986175298690796 Validation Accuracy: 0.5468000173568726\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.9623361825942993 Validation Accuracy: 0.5526000261306763\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 1.1977614164352417 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 1.0964069366455078 Validation Accuracy: 0.550599992275238\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 1.0538533926010132 Validation Accuracy: 0.5541999936103821\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 1.0052906274795532 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.968767523765564 Validation Accuracy: 0.550599992275238\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 1.1946046352386475 Validation Accuracy: 0.5540000200271606\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 1.0940446853637695 Validation Accuracy: 0.548799991607666\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 1.0106345415115356 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 1.0074816942214966 Validation Accuracy: 0.550599992275238\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.9735744595527649 Validation Accuracy: 0.551800012588501\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 1.1704375743865967 Validation Accuracy: 0.5482000112533569\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 1.1246751546859741 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 1.0009796619415283 Validation Accuracy: 0.5437999963760376\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 1.0048280954360962 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.9560295343399048 Validation Accuracy: 0.5529999732971191\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 1.201454758644104 Validation Accuracy: 0.548799991607666\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 1.1061391830444336 Validation Accuracy: 0.5472000241279602\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.985916256904602 Validation Accuracy: 0.551800012588501\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 1.0018424987792969 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.9641935229301453 Validation Accuracy: 0.5551999807357788\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 1.1861999034881592 Validation Accuracy: 0.5526000261306763\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 1.140956163406372 Validation Accuracy: 0.5460000038146973\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 1.028430700302124 Validation Accuracy: 0.5541999936103821\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.9899724721908569 Validation Accuracy: 0.5486000180244446\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.9638005495071411 Validation Accuracy: 0.5577999949455261\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 1.1754705905914307 Validation Accuracy: 0.557200014591217\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 1.100396752357483 Validation Accuracy: 0.5473999977111816\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 1.0256248712539673 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 1.0214735269546509 Validation Accuracy: 0.5437999963760376\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.9604242444038391 Validation Accuracy: 0.5508000254631042\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 1.2114742994308472 Validation Accuracy: 0.5411999821662903\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 1.1081697940826416 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 1.0101454257965088 Validation Accuracy: 0.5544000267982483\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.9920956492424011 Validation Accuracy: 0.5422000288963318\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.9607038497924805 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 1.1622178554534912 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 1.118938684463501 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 1.0077813863754272 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 1.0010771751403809 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.9665800929069519 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 1.2155272960662842 Validation Accuracy: 0.5501999855041504\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 1.0749714374542236 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 1.0519263744354248 Validation Accuracy: 0.5486000180244446\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.9887294769287109 Validation Accuracy: 0.54339998960495\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.9639949798583984 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 1.1906124353408813 Validation Accuracy: 0.5479999780654907\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 1.0832881927490234 Validation Accuracy: 0.5522000193595886\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.9934777021408081 Validation Accuracy: 0.5475999712944031\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.9985181093215942 Validation Accuracy: 0.5472000241279602\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.9462698101997375 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 1.1839921474456787 Validation Accuracy: 0.550599992275238\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 1.0884983539581299 Validation Accuracy: 0.5515999794006348\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 1.0084563493728638 Validation Accuracy: 0.54339998960495\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.9962638020515442 Validation Accuracy: 0.5429999828338623\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.9583936929702759 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 1.1855957508087158 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 1.0786235332489014 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 1.0271258354187012 Validation Accuracy: 0.5515999794006348\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.9756467938423157 Validation Accuracy: 0.5559999942779541\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.9706832766532898 Validation Accuracy: 0.5559999942779541\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 1.197931170463562 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 1.0874974727630615 Validation Accuracy: 0.5493999719619751\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 1.0089267492294312 Validation Accuracy: 0.5558000206947327\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.9661976099014282 Validation Accuracy: 0.5464000105857849\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.9721916317939758 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 1.1965951919555664 Validation Accuracy: 0.5504000186920166\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 1.0580416917800903 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.9792813062667847 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.9816734194755554 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.9486042857170105 Validation Accuracy: 0.553600013256073\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 1.1575157642364502 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 1.076796293258667 Validation Accuracy: 0.5473999977111816\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 1.0055429935455322 Validation Accuracy: 0.5551999807357788\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 1.0018771886825562 Validation Accuracy: 0.5529999732971191\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.9686876535415649 Validation Accuracy: 0.5580000281333923\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 1.2001104354858398 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 1.089813470840454 Validation Accuracy: 0.5587999820709229\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 1.0033727884292603 Validation Accuracy: 0.550599992275238\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.9527254104614258 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.9414642453193665 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 1.192809820175171 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 1.0888488292694092 Validation Accuracy: 0.5483999848365784\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 1.0073132514953613 Validation Accuracy: 0.5501999855041504\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 1.0005650520324707 Validation Accuracy: 0.550599992275238\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 0.9554556608200073 Validation Accuracy: 0.5609999895095825\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 1.1905672550201416 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 1.0701308250427246 Validation Accuracy: 0.5508000254631042\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 1.0259594917297363 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 0.9766885638237 Validation Accuracy: 0.5473999977111816\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 0.9587135314941406 Validation Accuracy: 0.5558000206947327\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 1.1708186864852905 Validation Accuracy: 0.5568000078201294\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 1.077326774597168 Validation Accuracy: 0.5562000274658203\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 1.0305849313735962 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 0.9802623987197876 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 0.9386723637580872 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 1.1787630319595337 Validation Accuracy: 0.5514000058174133\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 1.0813367366790771 Validation Accuracy: 0.5478000044822693\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 1.0169315338134766 Validation Accuracy: 0.5537999868392944\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 0.9714391827583313 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 0.9389774203300476 Validation Accuracy: 0.5583999752998352\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 1.1542510986328125 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 1.0715198516845703 Validation Accuracy: 0.550599992275238\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 1.0114576816558838 Validation Accuracy: 0.5580000281333923\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 0.9739082455635071 Validation Accuracy: 0.5523999929428101\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 0.9430273175239563 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 1.1358692646026611 Validation Accuracy: 0.5483999848365784\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 1.0940425395965576 Validation Accuracy: 0.555400013923645\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 1.0302438735961914 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 1.0008258819580078 Validation Accuracy: 0.5482000112533569\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 0.977888286113739 Validation Accuracy: 0.5514000058174133\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 1.1686910390853882 Validation Accuracy: 0.551800012588501\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 1.0436128377914429 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 1.0225379467010498 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 0.9739874005317688 Validation Accuracy: 0.5544000267982483\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 0.9186192750930786 Validation Accuracy: 0.5601999759674072\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 1.1660425662994385 Validation Accuracy: 0.5555999875068665\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 1.0742223262786865 Validation Accuracy: 0.5550000071525574\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 1.0064256191253662 Validation Accuracy: 0.5526000261306763\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 0.9965931177139282 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 0.9684633016586304 Validation Accuracy: 0.553600013256073\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 1.1727972030639648 Validation Accuracy: 0.5511999726295471\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 1.0843027830123901 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 1.015560269355774 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 0.9778373837471008 Validation Accuracy: 0.5464000105857849\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 0.9333545565605164 Validation Accuracy: 0.5598000288009644\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 1.1750179529190063 Validation Accuracy: 0.5515999794006348\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 1.060287594795227 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 1.019237995147705 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 1.0202550888061523 Validation Accuracy: 0.555400013923645\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 0.9486300349235535 Validation Accuracy: 0.5583999752998352\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 1.1564282178878784 Validation Accuracy: 0.5508000254631042\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 1.0634649991989136 Validation Accuracy: 0.5478000044822693\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 1.0011584758758545 Validation Accuracy: 0.5559999942779541\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 0.9856964349746704 Validation Accuracy: 0.546999990940094\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 0.9432865381240845 Validation Accuracy: 0.5582000017166138\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 1.1644985675811768 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 1.0641796588897705 Validation Accuracy: 0.5501999855041504\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 1.0076818466186523 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 1.001796841621399 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 0.9592882394790649 Validation Accuracy: 0.5594000220298767\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 1.1819044351577759 Validation Accuracy: 0.550599992275238\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 1.0768415927886963 Validation Accuracy: 0.5454000234603882\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 1.034305214881897 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 0.9987934231758118 Validation Accuracy: 0.5454000234603882\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 0.9596625566482544 Validation Accuracy: 0.5577999949455261\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 1.182150959968567 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 1.0434229373931885 Validation Accuracy: 0.5583999752998352\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 1.0064436197280884 Validation Accuracy: 0.5514000058174133\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 0.9714158773422241 Validation Accuracy: 0.5464000105857849\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 0.9588491320610046 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 1.1593798398971558 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 1.0730657577514648 Validation Accuracy: 0.557200014591217\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 0.9984155893325806 Validation Accuracy: 0.5604000091552734\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 0.9954292178153992 Validation Accuracy: 0.5551999807357788\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 0.9372860193252563 Validation Accuracy: 0.5591999888420105\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 1.1744279861450195 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 1.0433845520019531 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 1.0110069513320923 Validation Accuracy: 0.5558000206947327\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 0.9712364077568054 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 0.9338468313217163 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 1.1739072799682617 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 1.0585899353027344 Validation Accuracy: 0.5590000152587891\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 0.9816223382949829 Validation Accuracy: 0.5541999936103821\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 0.9989072680473328 Validation Accuracy: 0.5576000213623047\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 0.9463974237442017 Validation Accuracy: 0.5568000078201294\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 1.1865930557250977 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 1.0368119478225708 Validation Accuracy: 0.550000011920929\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 0.9998151659965515 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 0.9577598571777344 Validation Accuracy: 0.5523999929428101\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 0.9421226382255554 Validation Accuracy: 0.5583999752998352\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 1.1651884317398071 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 1.0532653331756592 Validation Accuracy: 0.551800012588501\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 1.0190047025680542 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 0.97711181640625 Validation Accuracy: 0.551800012588501\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 0.9351808428764343 Validation Accuracy: 0.5618000030517578\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 1.1968778371810913 Validation Accuracy: 0.5582000017166138\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 1.027756929397583 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 0.9759799838066101 Validation Accuracy: 0.5591999888420105\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 0.9680143594741821 Validation Accuracy: 0.550599992275238\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 0.9171218872070312 Validation Accuracy: 0.5587999820709229\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 1.1624958515167236 Validation Accuracy: 0.5540000200271606\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 1.0426037311553955 Validation Accuracy: 0.5496000051498413\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 0.9792718887329102 Validation Accuracy: 0.5562000274658203\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 0.9585797190666199 Validation Accuracy: 0.5509999990463257\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 0.9268377423286438 Validation Accuracy: 0.5601999759674072\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 1.1814944744110107 Validation Accuracy: 0.5576000213623047\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 1.0394591093063354 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 0.9964213371276855 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 0.9822542071342468 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 0.9262756109237671 Validation Accuracy: 0.5605999827384949\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 1.1800119876861572 Validation Accuracy: 0.5559999942779541\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 1.042859435081482 Validation Accuracy: 0.5514000058174133\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 0.9795253872871399 Validation Accuracy: 0.5526000261306763\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 0.9665991067886353 Validation Accuracy: 0.5478000044822693\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 0.9226169586181641 Validation Accuracy: 0.555400013923645\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 1.1496775150299072 Validation Accuracy: 0.5529999732971191\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 1.0298807621002197 Validation Accuracy: 0.5522000193595886\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 1.0049594640731812 Validation Accuracy: 0.5541999936103821\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 0.9640892744064331 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 0.9087880849838257 Validation Accuracy: 0.5612000226974487\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 1.1606647968292236 Validation Accuracy: 0.5541999936103821\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 1.0529758930206299 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 0.9791361689567566 Validation Accuracy: 0.5562000274658203\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 0.9588857889175415 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 0.9235421419143677 Validation Accuracy: 0.5659999847412109\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 1.1988970041275024 Validation Accuracy: 0.5598000288009644\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 1.022322416305542 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 0.9849979281425476 Validation Accuracy: 0.555400013923645\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 0.9794836044311523 Validation Accuracy: 0.5523999929428101\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 0.9216233491897583 Validation Accuracy: 0.5613999962806702\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 1.1503198146820068 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 1.0118129253387451 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 0.973785400390625 Validation Accuracy: 0.5555999875068665\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 0.9661425352096558 Validation Accuracy: 0.5465999841690063\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 0.9377349019050598 Validation Accuracy: 0.5576000213623047\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 1.1837341785430908 Validation Accuracy: 0.5551999807357788\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 1.051018476486206 Validation Accuracy: 0.5504000186920166\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 0.9495218396186829 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 0.9621980786323547 Validation Accuracy: 0.5442000031471252\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 0.9475582838058472 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 1.1108663082122803 Validation Accuracy: 0.5537999868392944\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 1.0275768041610718 Validation Accuracy: 0.5523999929428101\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 0.9819145202636719 Validation Accuracy: 0.5522000193595886\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 0.9624330401420593 Validation Accuracy: 0.5590000152587891\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 0.922356128692627 Validation Accuracy: 0.5605999827384949\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 1.1726109981536865 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 1.024222731590271 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 0.9754665493965149 Validation Accuracy: 0.5529999732971191\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 0.9429397583007812 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 0.9262563586235046 Validation Accuracy: 0.5601999759674072\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 1.1910898685455322 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 1.0323503017425537 Validation Accuracy: 0.5504000186920166\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 0.9944912195205688 Validation Accuracy: 0.5587999820709229\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 0.9618740081787109 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 0.9249418377876282 Validation Accuracy: 0.5626000165939331\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 1.16805100440979 Validation Accuracy: 0.5544000267982483\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 1.0360569953918457 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 0.9946451187133789 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 0.9560192227363586 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 0.9384128451347351 Validation Accuracy: 0.553600013256073\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 1.1990361213684082 Validation Accuracy: 0.5601999759674072\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 1.0261040925979614 Validation Accuracy: 0.5555999875068665\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 1.017104983329773 Validation Accuracy: 0.5627999901771545\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 0.9445000886917114 Validation Accuracy: 0.550000011920929\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 0.9271878004074097 Validation Accuracy: 0.5612000226974487\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 1.1900588274002075 Validation Accuracy: 0.5586000084877014\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 1.027786135673523 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 0.9885556101799011 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 0.9515134692192078 Validation Accuracy: 0.553600013256073\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 0.9042608141899109 Validation Accuracy: 0.5587999820709229\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 1.1733019351959229 Validation Accuracy: 0.5562000274658203\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 1.0520941019058228 Validation Accuracy: 0.5419999957084656\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 0.9906997680664062 Validation Accuracy: 0.5600000023841858\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 0.9547806978225708 Validation Accuracy: 0.5550000071525574\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 0.9153561592102051 Validation Accuracy: 0.5623999834060669\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 1.1721746921539307 Validation Accuracy: 0.5576000213623047\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 1.0141355991363525 Validation Accuracy: 0.5559999942779541\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 0.9973052144050598 Validation Accuracy: 0.5590000152587891\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 0.9604591131210327 Validation Accuracy: 0.555400013923645\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 0.9419387578964233 Validation Accuracy: 0.5582000017166138\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 1.2029750347137451 Validation Accuracy: 0.5573999881744385\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 1.0331943035125732 Validation Accuracy: 0.5483999848365784\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 1.0012664794921875 Validation Accuracy: 0.5605999827384949\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 0.9354894757270813 Validation Accuracy: 0.5508000254631042\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 0.9189287424087524 Validation Accuracy: 0.5626000165939331\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 1.1720987558364868 Validation Accuracy: 0.5540000200271606\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 1.0326178073883057 Validation Accuracy: 0.5497999787330627\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 1.0071525573730469 Validation Accuracy: 0.5605999827384949\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 0.9617018699645996 Validation Accuracy: 0.5591999888420105\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 0.9561230540275574 Validation Accuracy: 0.5576000213623047\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 1.1610147953033447 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 1.0295253992080688 Validation Accuracy: 0.5491999983787537\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 1.0067678689956665 Validation Accuracy: 0.5631999969482422\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 0.9969244003295898 Validation Accuracy: 0.5547999739646912\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 0.9299122095108032 Validation Accuracy: 0.5623999834060669\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 1.197450876235962 Validation Accuracy: 0.5622000098228455\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 1.0109226703643799 Validation Accuracy: 0.5464000105857849\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 0.9842168688774109 Validation Accuracy: 0.5636000037193298\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 0.9410821199417114 Validation Accuracy: 0.5550000071525574\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 0.9108972549438477 Validation Accuracy: 0.5601999759674072\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 1.2030105590820312 Validation Accuracy: 0.5587999820709229\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 1.0080121755599976 Validation Accuracy: 0.5562000274658203\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 0.9859777688980103 Validation Accuracy: 0.5591999888420105\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 0.9497274160385132 Validation Accuracy: 0.5508000254631042\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 0.88678377866745 Validation Accuracy: 0.5631999969482422\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 1.1773655414581299 Validation Accuracy: 0.5604000091552734\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 1.0170186758041382 Validation Accuracy: 0.5580000281333923\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 0.9877563714981079 Validation Accuracy: 0.5616000294685364\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 0.9345670938491821 Validation Accuracy: 0.5583999752998352\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 0.9082315564155579 Validation Accuracy: 0.5626000165939331\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 1.2058587074279785 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 1.0210343599319458 Validation Accuracy: 0.5573999881744385\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 0.9778153300285339 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 0.9105045199394226 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 0.9134634137153625 Validation Accuracy: 0.5641999840736389\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 1.205500841140747 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 1.0288116931915283 Validation Accuracy: 0.5523999929428101\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 1.0056918859481812 Validation Accuracy: 0.5630000233650208\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 0.9464582204818726 Validation Accuracy: 0.5568000078201294\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 0.8909433484077454 Validation Accuracy: 0.5627999901771545\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 1.1569550037384033 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 1.0156258344650269 Validation Accuracy: 0.5555999875068665\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 0.9702666997909546 Validation Accuracy: 0.555400013923645\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 0.9305785894393921 Validation Accuracy: 0.5532000064849854\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 0.8968477249145508 Validation Accuracy: 0.5648000240325928\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 1.1874310970306396 Validation Accuracy: 0.5546000003814697\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 1.010931372642517 Validation Accuracy: 0.5565999746322632\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 0.9758020639419556 Validation Accuracy: 0.5598000288009644\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 0.9289222955703735 Validation Accuracy: 0.5540000200271606\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 0.8874243497848511 Validation Accuracy: 0.5631999969482422\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 1.143388032913208 Validation Accuracy: 0.5594000220298767\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 1.0107660293579102 Validation Accuracy: 0.555400013923645\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 0.9628822207450867 Validation Accuracy: 0.5608000159263611\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 0.9255250096321106 Validation Accuracy: 0.5583999752998352\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 0.9045864939689636 Validation Accuracy: 0.5641999840736389\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 1.1325290203094482 Validation Accuracy: 0.5569999814033508\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 1.0224519968032837 Validation Accuracy: 0.5504000186920166\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 0.9578506350517273 Validation Accuracy: 0.5619999766349792\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 0.9338356852531433 Validation Accuracy: 0.5586000084877014\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 0.9116472005844116 Validation Accuracy: 0.5618000030517578\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 1.1869392395019531 Validation Accuracy: 0.5598000288009644\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 1.0144679546356201 Validation Accuracy: 0.5540000200271606\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 0.9797971844673157 Validation Accuracy: 0.5519999861717224\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 0.9114410281181335 Validation Accuracy: 0.5577999949455261\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 0.9178083539009094 Validation Accuracy: 0.5651999711990356\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 1.1816816329956055 Validation Accuracy: 0.5608000159263611\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 1.0116729736328125 Validation Accuracy: 0.5533999800682068\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 0.9769718050956726 Validation Accuracy: 0.5564000010490417\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 0.9356660842895508 Validation Accuracy: 0.5527999997138977\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 0.9028253555297852 Validation Accuracy: 0.5582000017166138\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 1.178325891494751 Validation Accuracy: 0.5613999962806702\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5524129746835443\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3Xec3Ed9//HX56pOd5JOXbJkWe4dG4xtMMU2hhAwxYQW\nCIQSCCWYGkIPdgglhIBDCcQB4h/V9JDQwdjGGIzBBhs3XOUiybLqSdfb5/fHZ3a/X321t7en66f3\n8/HYx97OzHe+s/VmZz8zY+6OiIiIiIhA3XQ3QERERERkplDnWEREREQkUedYRERERCRR51hERERE\nJFHnWEREREQkUedYRERERCRR51hEREREJFHnWEREREQkUedYRERERCRR51hEREREJFHnWEREREQk\nUedYRERERCRR51hEREREJFHnWEREREQkUed4mpnZIWb2F2b2GjN7h5m93czON7Pnmtkjzaxtuts4\nEjOrM7NnmtmlZnanme02M89d/me62ygy05jZ+sL75IKJKDtTmdlZhfvw0uluk4hINQ3T3YADkZkt\nAV4DvBI4ZJTiw2Z2C3AV8H3gMnfvneQmjirdh28CZ093W2TqmdklwEtGKTYI7AK2AdcTr+GvunvH\n5LZORERk/2nkeIqZ2dOAW4B/ZvSOMcRzdALRmf4e8JzJa92YfIExdIw1enRAagCWAccALwQ+DWw0\nswvMTF/MZ5HCe/eS6W6PiMhk0j+oKWRmzwO+yr5fSnYDfwQeBPqAxcA64NgKZaedmT0KODeXdC9w\nIfA7YE8uvXsq2yWzQivwXuDxZvYUd++b7gaJiIjkqXM8RczscGK0Nd/ZvQl4F/ADdx+scEwbcCbw\nXOBZwMIpaGot/qJw+5nufsO0tERmircSYTZ5DcBK4LHAa4kvfCVnEyPJL5+S1omIiNRIneOp836g\nOXf7Z8Az3L1npAPcvZOIM/6+mZ0PvIIYXZ5up+T+3qCOsQDb3H1DhfQ7gavN7BPAl4gveSUvNbOP\nu/sfpqKBs1F6TG262zEe7n4Fs/w+iMiBZcb9ZD8XmVkL8Ixc0gDwkmod4yJ33+PuH3P3n014A8du\nRe7vTdPWCpk13L0b+Cvg9lyyAa+enhaJiIhUps7x1HgE0JK7/St3n82dyvzycgPT1gqZVdKXwY8V\nks+ZjraIiIiMRGEVU2NV4fbGqTy5mS0EHgesAZYSk+a2AL9x9/v2p8oJbN6EMLPDiHCPtUATsAG4\n3N0fGuW4tURM7MHE/dqcjntgHG1ZAxwPHAa0p+QdwH3Arw/wpcwuK9w+3Mzq3X1oLJWY2QnAccBq\nYpLfBnf/Sg3HNQGPBtYTv4AMAw8BN05EeJCZHQmcBhwE9AIPANe6+5S+5yu06yjgZGA58ZrsJl7r\nNwG3uPvwNDZvVGZ2MPAoIoZ9AfF+2gRc5e67JvhchxEDGgcD9cRn5dXufvc46jyaePxXEYMLg0An\ncD9wB3Cbu/s4my4iE8XddZnkC/CXgOcuP5yi8z4S+CHQXzh//nIjscyWVannrCrHj3S5Ih27YX+P\nLbThknyZXPqZwOVEJ6dYTz/wH0BbhfqOA34wwnHDwLeANTU+znWpHZ8G7hrlvg0BPwXOrrHu/1c4\n/uIxPP8fLBz7f9We5zG+ti4p1P3SGo9rqfCYrKhQLv+6uSKX/jKiQ1esY9co5z0a+ArxxXCk5+YB\n4M1A0348Ho8BfjNCvYPE3IFTUtn1hfwLqtRbc9kKx7YD7yO+lFV7TW4FPg+cOspzXNOlhs+Pml4r\n6djnAX+ocr6B9H561BjqvCJ3/IZc+unEl7dKnwkOXAM8egznaQTeQsTdj/a47SI+c540Ee9PXXTR\nZXyXaW/AgXABnlD4INwDtE/i+Qz4cJUP+UqXK4DFI9RX/OdWU33p2A37e2yhDXv9o05pr6/xPv6W\nXAeZWG2ju4bjNgAH1/B4v3w/7qMD/wbUj1J3K3Bb4bjn19CmPys8Ng8ASyfwNXZJoU0vrfG4/eoc\nE5NZv17lsazYOSbeC/9EdKJqfV5uquV5z53jnTW+DvuJuOv1hfQLqtRdc9nCcc8Cdo7x9fiHUZ7j\nmi41fH6M+lohVub52RjPfRFQV0PdV+SO2ZDSzqf6IEL+OXxeDedYTmx8M9bH738m6j2qiy667P9F\nYRVT4zpixLA+3W4DvmBmL/RYkWKi/RfwN4W0fmLkYxMxovRIYoOGkjOBX5jZ49195yS0aUKlNaP/\nPd10YnTpLqIzdDJweK74I4FPAC8zs7OBr5GFFN2WLv3EutIn5o47hNo2OynG7vcANxM/W+8mOoTr\ngIcRIR8lbyY6bW8fqWJ370r39TfAvJR8sZn9zt3vqnSMma0CvkgW/jIEvNDdt49yP6bCmsJtB2pp\n10XEkoalY35P1oE+DDi0eICZGTHy/uJCVg/RcSnF/R9BvGZKj9fxwK/M7FR3r7o6jJm9kViJJm+I\neL7uJ0IAHk6EfzQSHc7ie3NCpTZ9lH3Dnx4kfinaBswnQpBOZO9VdKadmS0AriSek7ydwLXpejUR\nZpFv+xuIz7QXjfF8LwI+nku6iRjt7SM+R04heywbgUvM7PfufscI9RnwbeJ5z9tCrGe/jfgytSjV\nfwQKcRSZWaa7d36gXIjd7YqjBJuIDRFOZOJ+7n5J4RzDRMeivVCugfgn3VEo/9UKdc4jRrBKlwdy\n5a8p5JUuq9Kxa9PtYmjJ349wXPnYQhsuKRxfGhX7HnB4hfLPIzpB+cfh0ekxd+BXwMkVjjuL6Kzl\nz/XUUR7z0hJ7H0znqDgaTHwpeRvQVWjX6TU8r68utOl3VPj5n+ioF0fc3jMJr+fi8/HSGo/728Jx\nd45QbkOuTD4U4ovA2grl11dIe3vhXDvS4zivQtlDge8Wyv+Y6uFGJ7LvaONXiq/f9Jw8j4htLrUj\nf8wFVc6xvtayqfyTic55/pgrgTMq3Reic/l04if96wp5y8jek/n6vsnI791Kz8NZY3mtAP9dKL8b\neBXQWCi3iPj1pThq/6pR6r8iV7aT7HPiO8ARFcofC9xQOMfXqtR/bqHsHcTE04qvJeLXoWcClwLf\nmOj3qi666DL2y7Q34EC5EKMgvYUPzfxlOxGX+B7gSUDrfpyjjYhdy9f7plGOOZ29O2vOKHFvjBAP\nOsoxY/oHWeH4Syo8Zl+mys+oxJbblTrUPwOaqxz3tFr/Eabyq6rVV6H8owuvhar1544rhhX8e4Uy\n7yqUuazaYzSO13Px+Rj1+SS+ZN1aOK5iDDWVw3E+OIb2Hc/eoRT3U6HjVjjGiNjb/DnPrVL+8kLZ\nT9bQpmLHeMI6x8Ro8JZim2p9/oGVVfLydV4yxtdKze99YuJwvmw38JhR6n9d4ZhORggRS+WvqPAc\nfJLqX4RWsneYSu9I5yDmHpTKDQCHjuGx2ueLmy666DL1Fy3lNkU8Njp4MfGhWskS4KlEfORPgJ1m\ndpWZvSqtNlGLlxCjKSU/cvfi0lnFdv0G+MdC8htqPN902kSMEFWbZf85YmS8pDRL/8VeZdtid/8e\n8Kdc0lnVGuLuD1arr0L5XwOfyiWdZ2a1/LT9CiA/Y/71ZvbM0g0zeyyxjXfJVuBFozxGU8LM5hGj\nvscUsv6zxir+ALx7DKf8B7Kfqh14rlfepKTM3Z3YyS+/UknF94KZHc/er4vbiTCZavXfnNo1WV7J\n3muQXw6cX+vz7+5bJqVVY/P6wu0L3f3qage4+yeJX5BKWhlb6MpNxCCCVznHFqLTW9JMhHVUkt8J\n8g/ufk+tDXH3kf4/iMgUUud4Crn7N4ifN39ZQ/FGYomxzwB3m9lrUyxbNX9VuP3eGpv2caIjVfJU\nM1tS47HT5WIfJV7b3fuB4j/WS919cw31/zz394oUxzuRvpv7u4l94yv34e67gecTP+WX/LeZrTOz\npcBXyeLaHfjrGu/rRFhmZusLlyPM7Awz+wfgFuA5hWO+7O7X1Vj/RV7jcm9m1g68IJf0fXe/ppZj\nU+fk4lzS2WY2v0LR4nvtw+n1NprPM3lLOb6ycLtqh2+mMbNW4Lxc0k4iJKwWxS9OY4k7/pi717Je\n+w8Kt0+q4ZjlY2iHiMwQ6hxPMXf/vbs/Dng8MbJZdR3eZCkx0nhpWqd1H2nkMb+t893ufm2NbRoA\nvpGvjpFHRWaKn9RYrjhp7ac1Hndn4faY/8lZWGBmBxU7juw7Wao4olqRu/+OiFsuWUx0ii8h4rtL\n/tXdfzTWNo/DvwL3FC53EF9O/oV9J8xdzb6duWr+bwxlH0N8uSz55hiOBbgq93cDEXpU9Ojc36Wl\n/0aVRnG/MWrBMTKz5UTYRslvffZt634qe09M+06tv8ik+3pLLunENLGvFrW+T24r3B7pMyH/q9Mh\nZvZ3NdYvIjOEZshOE3e/ivRP2MyOI0aUTyH+QZxMNgKY9zxipnOlD9sT2HslhN+MsUnXED8pl5zC\nviMlM0nxH9VIdhdu/6liqdGPGzW0xczqgScSqyqcSnR4K36ZqWBxjeVw94vSqhulLcnPKBS5hog9\nnol6iFVG/rHG0TqA+9x9xxjO8ZjC7e3pC0mtiu+9Ssc+Ivf3HT62jSh+O4aytSp24K+qWGpmO6Vw\ne38+w45Lf9cRn6OjPQ67vfbdSoub94z0mXAp8Kbc7U+a2XnERMMf+ixYDUjkQKfO8Qzg7rcQox6f\nBTCzRcQ6pW9k35/uXmtmn3P36wvpxVGMissMVVHsNM70nwNr3WVucIKOa6xYKjGzRxPxsydWK1dF\nrXHlJS8jljNbV0jfBbzA3Yvtnw5DxOO9nWjrVcBXxtjRhb1DfmqxtnB7LKPOlewVYpTip/PPV8Ul\n9aoo/ioxEYphP7dOwjkm23R8htW8W6W7DxQi2yp+Jrj7tWb2H+w92PDEdBk2sz8Sv5z8ghp28RSR\nqaewihnI3Tvc/RJincwLKxQpTlqBbJvikuLI52iK/yRqHsmcDuOYZDbhk9PM7M+JyU/72zGGMb4X\nUwfzAxWy3jLaxLNJ8jJ3t8Klwd2XuvtR7v58d//kfnSMIVYfGIuJjpdvK9ye6PfaRFhauD2hWypP\nken4DJusyaqvI3696S6k1xEDHq8lRpg3m9nlZvacGuaUiMgUUed4BvNwAbFpRd4Tp6E5UkGauPgl\n9t6MYAOxbe9TiG2L24klmsodRypsWjHG8y4llv0repGZHejv66qj/PthNnZaZs1EvLkofXZ/gNig\n5m3Ar9n31yiI/8FnEXHoV5rZ6ilrpIiMSGEVs8MniFUKStaYWYu79+TSiiNFY/2ZflHhtuLiavNa\n9h61uxR4SQ0rF9Q6WWgfuZ3firvNQezm925iScADVXF0+jh3n8gwg4l+r02E4n0ujsLOBnPuMywt\nAfdh4MNm1gacRqzlfDYRG5//H/w44EdmdtpYloYUkYl3oI8wzRaVZp0XfzIsxmUeMcZzHDVKfVLZ\nubm/O4BX1Lik13iWhntT4bzXsveqJ/9oZo8bR/2zXTGGc1nFUvspLfeW/8n/8JHKjmCs781aFLe5\nPnYSzjHZ5vRnmLt3uvvP3f1Cdz+L2AL73cQk1ZKHAS+fjvaJSEad49mhUlxcMR7vJvZe//a0MZ6j\nuHRbrevP1mqu/syb/wf+S3fvqvG4/Voqz8xOBT6US9pJrI7x12SPcT3wlRR6cSAqrmlcaSm28cpP\niD0yra1cq1MnujHse59n45ej4mfOWJ+3/HtqmNg4ZsZy923u/n72XdLw6dPRHhHJqHM8OxxduN1Z\n3AAj/QyX/+dyhJkVl0aqyMwaiA5WuTrGvozSaIo/E9a6xNlMl/8pt6YJRCks4oVjPVHaKfFS9o6p\nfbm73+fuPybWGi5ZSywddSD6OXt/GXveJJzj17m/64Bn13JQigd/7qgFx8jdtxJfkEtOM7PxTBAt\nyr9/J+u9+1v2jst91kjruheZ2cPYe53nm9x9z0Q2bhJ9jb0f3/XT1A4RSdQ5ngJmttLMVo6jiuLP\nbFeMUO4rhdvFbaFH8jr23nb2h+6+vcZja1WcST7RO85Nl3ycZPFn3ZG8mBo3/Sj4L2KCT8kn3P1/\ncrffxd5fap5uZrNhK/AJleI884/LqWY20R3SLxdu/0ONHbmXUzlWfCJcXLj90QlcASH//p2U9276\n1SW/c+QSKq/pXkkxxv5LE9KoKZCWXcz/4lRLWJaITCJ1jqfGscQW0B8ysxWjls4xs2cDrykkF1ev\nKPl/7P1P7Blm9toRypbqP5VYWSHv42NpY43uZu9RobMn4RzT4Y+5v08xszOrFTaz04gJlmNiZn/L\n3iOgvwfemi+T/sn+JXu/Bj5sZvkNKw4U/8Te4UifH+25KTKz1Wb21Ep57n4zcGUu6Sjgo6PUdxwx\nOWuyfA7Ykrv9ROBjtXaQR/kCn19D+NQ0uWwyFD973pc+o0ZkZq8BnplL6iIei2lhZq8xs5rj3M3s\nKey9/GCtGxWJyCRR53jqzCeW9HnAzL5jZs9OW75WZGbHmtnFwNfZe8eu69l3hBiA9DPimwvJnzCz\nf00bi+TrbzCzlxHbKef/0X09/UQ/oVLYR35U8ywz+6yZnWNmRxa2V55No8rFrYm/ZWbPKBYysxYz\nexNwGTELf1utJzCzE4CLckmdwPMrzWhPaxy/IpfURGw7PlmdmRnJ3f9ATHYqaQMuM7OPm9mIE+jM\nrN3MnmdmXyOW5PvrKqc5H8jv8vd3Zvbl4uvXzOrSyPUVxETaSVmD2N27ifbmvxS8gbjfj650jJk1\nm9nTzOxbVN8R8xe5v9uA75vZs9LnVHFr9PHch18AX8wltQI/NbO/SeFf+bYvNLMPA58sVPPW/VxP\ne6K8DbjXzL6QHtvWSoXSZ/BfE9u/582aUW+RuUpLuU29RuC8dMHM7gTuIzpLw8Q/z+OAgysc+wDw\n3GobYLj7583s8cBLUlId8PfA+Wb2a2AzsczTqew7i/8W9h2lnkifYO+tff8mXYquJNb+nA0+T6we\ncWS6vRT4rpndS3yR6SV+hj6d+IIEMTv9NcTaplWZ2Xzil4KWXPKr3X3E3cPc/Ztm9hng1SnpSOAz\nwItqvE9zgrt/MHXW/jYl1RMd2vPN7B5iC/KdxHuynXic1o+h/j+a2dvYe8T4hcDzzewa4H6iI3kK\nsTIBxK8nb2KS4sHd/Sdm9vfAv5Gtz3w28Csz2wzcSOxY2ELEpT+MbI3uSqvilHwWeAswL91+fLpU\nMt5QjtcRG2U8LN1elM7/L2Z2LfHlYhXw6Fx7Si5190+P8/wTYT4RPvViYle8PxFftkpfjFYTmzwV\nl5/7H3cf746OIjJO6hxPjR1E57fST21HUNuSRT8DXlnj7mcvS+d8I9k/qmaqdzh/CTxzMkdc3P1r\nZnY60TmYE9y9L40U/5ysAwRwSLoUdRITsm6r8RSfIL4slfy3uxfjXSt5E/FFpDQp66/M7DJ3P6Am\n6bn7q8zsRmKyYv4LxqHUthFL1bVy3f1j6QvM+8jea/Xs/SWwZJD4MviLCnkTJrVpI9GhzK+nvZq9\nX6NjqXODmb2U6NS3jFJ8XNx9dwqB+TZ7h18tJTbWGcmnqLx76HSrI0LrRlte72tkgxoiMo0UVjEF\n3P1GYqTjCcQo0++AoRoO7SX+QTzN3Z9U67bAaXemNxNLG/2EyjszldxM/BT7+Kn4KTK163TiH9lv\niVGsWT0Bxd1vAx5B/Bw60mPdCXwBeJi7/6iWes3sBew9GfM2YuSzljb1EhvH5Lev/YSZ7c9EwFnN\n3T9FdIQ/Amys4ZDbiZ/qz3D3UX9JSctxPZ5Yb7qSYeJ9+Bh3/0JNjR4nd/86MXnzI+wdh1zJFmIy\nX9WOmbt/jejgXUiEiGxm7zV6J4y77wLOIUbib6xSdIgIVXqMu79uHNvKT6RnAu8FrmbfVXqKhon2\nn+vuf6nNP0RmBnOfq8vPzmxptOmodFlBNsKzmxj1vRm4JU2yGu+5FhH/vNcQEz86iX+Iv6m1wy21\nSWsLP54YNW4hHueNwFUpJlSmWfqCcBLxS0470YHZBdxFvOdG60xWq/tI4kvpauLL7UbgWne/f7zt\nHkebjLi/xwPLiVCPztS2m4FbfYb/IzCzdcTjupL4rNwBbCLeV9O+E95I0gomxxMhO6uJx36QmDR7\nJ3D9NMdHi0gF6hyLiIiIiCQKqxARERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5FhERERFJ\n1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnU\nORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5\nFhERERFJ1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkeJzPzdFk/3W0RERERkfFR51hEREREJFHn\nWEREREQkUedYRERERCRR51hEREREJFHneBRmVmdm55vZDWbWY2Zbzez/zOzRNRz7cDP7kpndb2Z9\nZrbNzH5sZs8e5bh6M3ujmd2YO+f3zOwxKV+TAEVEREQmgbn7dLdhxjKzBuCbwDNT0iDQCbSnv58P\nfCvlHeruG3LH/i3wabIvILuABUB9uv0l4KXuPlQ4ZyPwXeApI5zzL1Ob9jmniIiIiIyPRo6rexvR\nMR4G3goscvfFwGHAz4DPVzrIzM4g6xh/Ezg4HdcOvBtw4EXAOyoc/m6iYzwEvBFYmI5dD/wI+OwE\n3TcRERERKdDI8QjMrBXYTIz2XujuFxTym4HrgeNSUnkU18wuA54AXA2cWWF0+ANEx7gTWOPuu1P6\ngnTOVuBd7v6BwnGNwG+Bk4rnFBEREZHx08jxyP6M6Bj3AR8rZrp7H/CRYrqZLQHOTjc/WOwYJ/8C\n9AJtwFML52xNeR+vcM4B4KNjuhciIiIiUjN1jkf2iHT9B3fvGKHMlRXSHg4YETpRKZ9U33WF85SO\nLZ2zc4RzXjVii0VERERkXNQ5HtnydL2pSpmNVY7rqNLBBXigUB5gWbreXOW4au0RERERkXFQ53jy\nNE93A0RERERkbNQ5HtnWdH1QlTKV8krHtZjZ8gr5JWsL5QG2pevVVY6rliciIiIi46DO8ciuT9cn\nm9nCEcqcWSHt90S8MWQT8/ZiZouAUwrnKR1bOmfbCOd83AjpIiIiIjJO6hyP7CfAbiI84g3FTDNr\nAt5STHf3HcDl6ebbzKzSY/w2YB6xlNsPCufsSnl/V+GcDcCbxnQvRERERKRm6hyPwN27gA+nm+81\nszebWQtA2rb5O8DBIxz+HmLjkEcAl5rZ2nRcm5m9E3h7Kveh0hrH6Zx7yJaN++e0bXXpnOuIDUUO\nnZh7KCIiIiJF2gSkinFuH/0q4D+ILyBObB+9kGz76C8DL6mwQUgT8H/EmseVzpnfPvogd6+2soWI\niIiIjIFGjqtw90Hg2cDrgRuJzukQ8H1i57tvVzn2P4FTga8QS7O1AR3AT4HnuvuLKm0Q4u79wLlE\nyMZN6Xylc54FXJYrvmt891BERERE8jRyPMuY2TnAz4B73X39NDdHREREZE7RyPHs89Z0/dNpbYWI\niIjIHKTO8QxjZvVm9k0z+/O05Fsp/Xgz+ybwZGAA+Pi0NVJERERkjlJYxQyTJgEO5JJ2Aw3A/HR7\nGHiNu1881W0TERERmevUOZ5hzMyAVxMjxCcCK4BG4EHgF8BF7n79yDWIiIiIyP5S51hEREREJFHM\nsYiIiIhIos6xiIiIiEiizrGIiIiISKLOsYiIiIhI0jDdDRARmYvM7B5gIbBhmpsiIjIbrQd2u/uh\nU33iOds5fu555zlAX9NQOa3b7wNgcCDS5tW1lvOa50facGM9AH0D88p5a9pPifKDqwDo2H5LOa99\n5Zaouydub9m4MKuzNepa3NoUxw83Zg1s2B1taewvJ9mixZE2GAP6q+pbynnbdvYB8OCOrQD4wAO5\n9u0BYEnrWgAe6lhXzrtzc9zHxtYlcb967i7nDQ3cCcDVV//aEJGJtrClpWXJscceu2S6GyIiMtvc\neuut9PT0TMu552znuKdpBwADTcPltKbmZgCGu7sAqK/fWc6b1xod1wGPDqbVrSrndXTHnhy9fj8A\nB6/JHrZ1aw8G4J4NmwGYvzKLVFm7bgUAq5ZEJ3lpW9YZ7+5ZCcBdG3eX0zbtjk50b2+0s691fjlv\n1epYcs+bOwG4f8OOcl5dc1uce0UcNz93n4c2xQurc8fGON7uKec1Nm1BZLYxsw0A7r5+elsyqg3H\nHnvskuuuu2662yEiMuuccsopXH/99Rum49yKORYRERERSebsyLGIyHS7aWMH69/+/eluhkyQDR86\nd7qbICJTYO52judFOMHAUBavMtAb4Q1N6V4vWrignNfcmOJ9hyM8cHi4qZw3vylCGUrhEWefdEI5\nb/XiKN91QsQEL164LKuzKUJ5jT3pHNlAfT0Rm9zR2VxO27gjym/c3g1Az1AWL71l2/b4Y2eEUzS3\nH17O20rk7d6yDYC+3izkYv6quP+NKc66tzt7PGyoHhERERHJKKxCRGYcC68zs5vNrNfMNprZJ81s\n0Qjlm83s7Wb2RzPrNrPdZnaVmT2vSv1vMLNbivWb2YZSXLOIiBx45uzIsVmsAtHUkE1OG+yNFSga\n6mIUtd6ykdnhlFZnMUrc351N1lu2IEaDzzjmKAAOWpRNlFu+YDkArctisp0Nd5XzhgZ7o+76GCUe\n9Kx99QMxSrx6Qfb9ZGV7PB1Hr4vR5K6+bCWLG26MCYOb/hRlWuvas7bPi7Z2D8XodX5AuKU1RqGb\nBgfjujk7nw1lq2GIzDAXAa8HNgMXAwPAM4HTgSag/OYwsybgx8CZwG3Ap4D5wHOAr5nZye7+zkL9\nnwJeA2xK9fcDzwBOAxrT+URE5AA0ZzvHIjI7mdkZRMf4LuA0d9+R0t8FXA6sBu7NHfIWomP8Q+AZ\n7j6Yyl8IXAu8w8y+5+6/SumPIzrGtwOnu/uulP5O4GfAQYX6R2vvSMtRHFNrHSIiMnPM2c6xpQFj\ns2wAqKk+hm4tBlHp68tGlefXx0DUYQtjhHbt2oPKeQcvXwNAawwEs/X+h8p5S5pjubb6hohR7tqT\njTgPD0fltMyMAAAgAElEQVT9LQtilLehPlvneGA4Yn+HvLOctn1b1LthQ6w/fOiabN3rY9qjfXXH\nx324c1sWE31nR4xMbxuOvL6mLI65syvikev6Y8m4psbsKe/3bC1nkRnkZen6/aWOMYC795rZO4gO\nct7LAQfeXOoYp/IPmdn7gM8CrwB+lbJekqt/V658f6r/lxN6b0REZFaZs51jEZm1HpGur6yQ90ug\nHA9lZguAI4CN7n5bhfI/T9cPz6WV/q7UCb4GGKyQPiJ3P6VSehpRfkSlPBERmbk0IU9EZprSpLt9\ndqlJI8PbKpTdPEJdpfT2XFq1+ocgLf8iIiIHpDk7ctzfFxPjenIT5OY3xtJtjQ0RMtFcl4UVHN4e\n/zv/7JBYIm1xSzarrdcjpKGjowOAhasOy+psTZP8UlhGnedn3aUd77rTpLim7OEeShPkegazSYHN\n8+J/9u49UdfmzfeV8w5aFEvGrW+LOtcsPaKct2pH5F17Z4RObOjOTUJMExPnN8Z5WppWZPerP1vK\nTmQG6UjXK4G78xlm1gAsAx4olF1FZasL5QBK21JWqr8eWApsHHOrRURkTpiznWMRmbWuJ8IRzqTQ\neQUeC5S/ubr7HjO7CzjMzI509zsK5c/O1VnyeyK04rEV6n8UE/i5eMKaRVynjSNERGaVOds5Xrhw\nLQDNQ1ZOG+yLUd75zTHqurwliyo5KgZfsc4YVNrekS2jNtQWI7oN81cCsKOju5w3fHv8Lz70oFjS\nrT436W5oOM591z33prxsNHrNQWn0uikbva6vj6fjyCNjkvuDD9xVzuvpi5Hmgf6YYDjYm7Xv6CWx\n1NySE6LOn9yUHXfDlhhFHhiKEefOnmwZuu7c3yIzyCXEBLp3mdl3c6tVzAM+WKH854H3A/9qZs9O\noRGY2TLgPbkyJV8gJvGV6u9I5ZuAD0zC/RERkVlkznaORWR2cverzewTwPnATWb2TbJ1jneyb3zx\nR4CnpPwbzOwHxDrHzwVWAB9291/m6r/SzC4G/ha42cy+lep/OhF+sQkYRkREDkiakCciM9EbiM5x\nB/Aq4AXERh9PJLcBCMQSbMCTgHelpPOJ5druAF7o7m+rUP9rgDcDncCrgRcSaxw/CVhIFpcsIiIH\nmDk7crx0wRMA6KO3nLZtx+0A9AzG3JzWxmXlvPrOmLjX2RfrD3cN9mV5dbH73TW//A0A9+cmyp1z\n5mkAtLfFbnNLFy8u5+3YFUuo3vKnCGu84657ynlrVkeIxpmPfUyWtiomy7W1xPna2rKdcrv7oz+w\nuyvWRe7Zk62PvHQ4JtutTJMDH35ots7xht1xfzo6o/ywZcd1dmePjchM4u4OfDJditZXKN9LhETU\nFBbh7sPAx9KlzMyOBNqAW8fWYhERmSs0ciwiBxwzW2VmdYW0+cS21QDfmfpWiYjITDBnR47vvC0m\nv/m8bGm1upYYKV28LEZ5h+uysMKetFvetp2xw53nJuvVDcZxdz8Qqzs9uOXB3HExQa5tQZq015xN\nsNvZEb/MXv/H2Jvghj9mexS0NET9u3ZkK0w94XFnRB110WYfytrnRNpQU9yvHrKd/3Z1R5sX1sWk\nu7q6LG/BkniKB5ujrsHereW8JcsWInKAeiPwAjO7gohhXgWcA6wltqH+xvQ1TUREptOc7RyLiFTx\nU+Ak4M+AJcSueLcDHwcuSmEdIiJyAJqzneOdHTG625wtiUpba8TiLksbaLQ2LCnndfVFPPC8tlim\nbdHSbAS4ryWOe9jJJwBw4vCx5bwjjjwaALc4T29fNmrb0BDLuq1ddwgAg5bV2d4ao9ftS7M2PLQt\nNv5amM7XWJ8tQzc0HPU2tMbyaw0DreW8XV0x+txP1LltIBtx7huMEWqvi2urz2KpsfxGYyIHDne/\nDLhsutshIiIzj2KORUREREQSdY5FRERERJI5G1Yxv/1GAIZbsjCH5ubYNe+hjRHesH0gu/vLUsjF\nkoMjzGFZe7aM2oN7IkyhfWGETqxZsa6ct7Q9loPrStEKzU1ZG5a0twFwzmMeAcDGrdnSqd19ccD8\n+dkBC9IKbIM9sdxad/dgOa8h7ba3ZEGEUzS1tpfztvfHhMHtFsu2PdCzo5xXVxfnaW2LyXcDjdnj\nMUi205+IiIiIaORYRERERKRszo4cr1gbo67dZEulWf0CALq6Y/fZ7t1bynkNjTGqO9gQo8Nm2WR1\nG46/6+tjlLelpa2c1zcQeQO700S+tmwkeKA70hrT0mxLFs0v5/Vs7Ultyibdefq7byg2/NjTmbV9\nWduCVFds+NHSlqtrKEaFf3t/7FuwZTDb6GOgMV2n0/QxVM7r1w65IiIiInvRyLGIiIiISDJnR44b\nW9IIa0PW/+/rewiAXmJUuTE3kjvUFKO123bFyO82y0ZYqV8KwOKlBwOwszsbcb1va2wlvWxZaUm2\nbFS5cTDK9XTHSO5AFu5L+6IYCc6dhY49e+J6Z4wY22AuJtiiXaXNSpobF5SzdqV45M174n7tygaj\nGWiMM/QMxEh1b1cWx9y/J7esm4iIiIho5FhEREREpESdYxERERGRZM6GVezuShPsmrPQgZbWUhjF\n1rjdlIVVDKZd5TY/GN8Xmvf0lPMWLIu0nbtist69mx4q5+3pjFCIdesOiuMaDy/ntTdFCEPHjlha\nrbO3v5xn82OpuKH67Cno6Ii6tm/fBcDCpmxSoA9FTEZnf9yfrT1ZeMTKo08C4JHtqwH47pXfL+f1\n9nUBMDAYy73teGBnOa/RWxARERGRjEaORURERESSOTty3NS8GIDhumxZs7o0Oa+5LUZwGxt6y3mt\nwzGhbu2y2OBjfn+2YcdD2x4E4Ke/uQGAgfpsxLW+Mercsn0bAEvaGst5axfH33u2x0h1Q3N2nA9E\nG+qbmstpw+m7SncaYa7vz6brdTTFqDJpkt+KI7MR6jVHnAzAwsPi9t0bNpTzfnfzlXH/Giy1N5vI\n19SQbSQicqAzsyuAM93dRisrIiJz15ztHIuITLebNnaw/u3fH72gTJgNHzp3upsgIrOcwipERERE\nRJI5O3Lc3rYcgD2eTWrbuWsjAD4c4RQLmlvLecsXLQPgsMMPAWCJ15fzfnvz3QDcdvevAdjenU3y\na26Ocgctiwl2W445uJy3uHkFAEMNMfFvuC7bPW84LZXcTHaegeH42y3CL/qHsgl8G3rje8xxR54Y\n1yc8upzXOxx5DYMRhnHOaWeV8+qG0wS+jggNGVichWp0D2iHPJmdzOw04C3AY4FlwA7gj8Bn3f3r\nqcxLgacDDwdWAwOpzKfd/Uu5utYD9+RuZx8acKW7nzV590RERGaaOds5FpG5ycxeCXya2EPnf4E7\ngBXAI4HXAl9PRT8N3Az8AtgMLAWeCnzRzI529/ekcruAC4GXAoekv0s21NCe60bIOqbW+yQiIjPH\nnO0cNxNLq/V5NkGubiBGUbu6NwPQzrxy3sHtqyKtNdKGs/l4LF8ek/TWrosyW265tZznaRC5vjFG\njns9W2Jte1+a+Ne8EIDNW7aW82677VoAjjryyHLagrbYXa+hLp6WunnZBL66tTEi3bQ2yg8OZ09d\n786ot7M/zregOVui7pQTTgXgmpuuAeD+HfdldywrJjIrmNlxwH8Au4HHufvNhfy1uZsnuPtdhfwm\n4IfA283sM+6+0d13AReY2VnAIe5+wWTeBxERmdnmbOdYROak1xCfW+8rdowB3P2B3N93VcjvN7NP\nAU8AzgG+MN4GufspldLTiPIjxlu/iIhMrTnbOX5oU1z32ZJy2qIFaSS2L0IKD2pfUc575JGnAzAv\nheT2e1fuuBhNPnxtxDFvvntDOW94IA6Y1x/xuzse2l7Oq6uLFaFSEa7/3R/KeYO9scTcYevWlNMG\nPGKh6zw2/GhcvLSc1zo/RpW3b4sl427b013Os7oYMe5LIcSDw1nIZF19JDbXR1qzDeWOy+KdRWaJ\nR6XrH45W0MzWAW8jOsHrgOKuN2v2OUhERA54c7ZzLCJzUmlx7o3VCpnZYcC1wGLgKuAnQAcRp7we\neAnQPNLxIiJy4FLnWERmk13peg1wW5VybyYm4L3M3S/JZ5jZC4jOsYiIyD7mbOe4uy+WLvOG7C4O\n9MbsuSZi0tzRB2e7zK1eFPN49myJeIwBcqEJaRe8E44+AoDOWzeV8zq3RBhF3a4Ibei6IxvQ2rMl\nQiAe2LkTgJ0dHeW8k44/CoBlixeW04a7YxZgQ3O0uW1htoNdU1+EaGzfGvfrIc9295vXGKET7Qtj\nV0DPhUs0tETbjzk0JvKtW3tQ1vaeLHREZJa4hliV4ilU7xwfka6/VSHvzBGOGQIws3p3HxqhzJic\nsGYR12lTChGRWUWbgIjIbPJpYBB4T1q5Yi+51So2pOuzCvlPBl4xQt2lCQPrxt1KERGZtebsyHFL\n+w4AegeyNdk6u/cAcPhB8b/vkJWHlvO6u2OCXM9QTIbbM5gtyTZYH6GJaw+NwaiTTswm3W1r3gCA\nESO7nUPZgNNtd90PwM7+aMNBa5aV89avicmA81uyOUJdaam5pqY438KGtnLeA7+/M/6YF6PCLYcu\nzs5z+w0AHHVI3J+WtkXlvFWLY87RQe1xnxsas/Nt27kFkdnE3W8xs9cCnwF+b2bfJdY5XgqcSizx\ndjax3NvLgG+Y2TeBTcAJwJ8T6yA/v0L1lwHPBb5tZj8AeoB73f2Lk3uvRERkJpmznWMRmZvc/b/M\n7Cbg74mR4fOAbcCNwGdTmRvN7Gzgn4Fzic+6G4C/IOKWK3WOP0tsAvKXwD+kY64E1DkWETmAzNnO\ncfP8GAFuacpih+eljUEWz4tfXgf7srvf1RDxt52DMXq7Z2CgnFdXH9tMW3OMuratz+J2730wllXt\nTUurbdqRbfQxkHaLXr82znfYQcvLeSuXxhJzLbmNPobS9s9NKW1g645y3j3XXg/AIScfD8Dyldnm\nW7+5Jsr174nR76OOzn5tXrksRo4XzovzLViQLV/XsyfbnlpkNnH3XwPPHqXMr4j1jCuxCuWHgHem\ni4iIHKAUcywiIiIikqhzLCIiIiKSzNmwisaWiGloX5iFQCyyYwHo2x7hB7tzu8y1NcckuL60zdzw\nUParayONAAyk7xJtB68t5y3siOXa+h+MJdzWrV+QHdcU5Xt64jzz2rOl2ZoXxhJu1pzbh2CwP7Ur\nlny799fZjnqNKTxk7ZHrAairy8IxGohz/unWPwGwJxcu0bUnwkQa05J2TWnHPIClK1oRERERkYxG\njkVEREREkjk7cuxD0e+//+4sbXNnjAavTJPStm7LNtJY1BajrcNpYHWwP1vKraU1Rm3r02Dy/Lb5\n5bwTTz4JgD3bVwMw1J0tHTeYNtnY8MC9kVCXLfPmabJeV312nrt3xWS+3/36OgD67s6WjHvuuc8B\nYElarm1Prq5laXJfX09M+Ktf0FjOu/FPvwdgYDhGr4dOeWQ5b3heNOK40xERERERNHIsIiIiIlKm\nzrGIiIiISDJnwyp2bY3whrqhbBe8vu6lkTcY3wkebN5TzluxvAeAZk8T8oaziWueds0r7YI3PJiF\nYzQwnK5Dx44srKJ3T9Tf2BOhEwv6szWX+7bGRL6HevrKadf/4TYAbrgl1k4+9pCjy3lLjo71jfub\n5wEwSNb25ctjot/qdY8AoHVltkPeH6+LSX233Rt1H3bUweW8joeyc4uIiIiIRo5FRERERMrm7Mhx\no6W7Vp8bHW2OEdyu3hjt3b47W8ptT3csqdYwL46rq8uWcqsjJr+Zx3X9cDaJrr4hvl8sSsu0WW7E\nedN9MQLctyVGiTs27Szn7UgjzB1D9eW0FW0xyn3qww4B4FGPO7mcN395jAb3W5y7dyA3mbA9loWb\ntyDa0uXZ7n5LD1oJQE9aJu7OhzaW87r7svshIiIiIho5FhEREREpm7Mjx4cdFqOvTa3Zphy3Xr8B\ngB3bYxTVmrKR4/s3xUjsokNjg4+GhmxE14dihHVeivf1wWxUeTjFHFsaQR5oyh7S/tYo33ZYjAi3\nNGZLrLUsidHeNW0rymk7O6Otx86LUeKDVvWU8zo7HwSgbijq7BnuKucN9Efbh0vhzvOz83R3p01A\nFsX57tq+rZy3fccuRERERCSjkWMRERERkUSdYxGZMGa23szczC6Z7raIiIjsjzkbVrFzV4Q7rG5d\nVk5rWRBhCt4ak9K2D2ST03Z0LQageV5MYJs/mIVVtLXGZL15aWc892xJtp6+VKfFZL1sOh5YfexA\n5w1R3tqyJdZ2+SoANm9oKaf1pyiKZYsfAqCxaWs5r7kl6m/oj7CK3u5sQl7dQIRODPVFmebmtnJe\nHxFCsrUz6uro7iznDXu+tSIiIiIyZzvHIiLT7aaNHax/+/enuxn7bcOHzp3uJoiITLk52znu2Bmj\ntts355Yrs9gEZMjujduejaKuPehwAFYviyXZFuSWWGtpbQVgwGIi3p5sLhwD3TEyO1Ra5q2xqZzX\nmTb/2JM2/GhpzkaJd3VEJR1bs8l9C5ri72Xzos4FzVnUS89QlO/viuvBrqFy3uLmGPXu7o8Jhvdu\nvj9rQ19nal/U2VifHdfVlbsjIiIiIqKYYxGZHCn++FIz22ZmvWb2OzN7WoVyzWb2djP7o5l1m9lu\nM7vKzJ43Qp1uZpeY2VFm9jUze8jMhs3srFTmMDO72MzuNLMeM9uR6v6MWfqGvHedLzCzy81sV2rn\nrWb2bjNrnpQHRkREZrQ5O3LctTNGYQe6s9HalpYle6WtXL6qnHfiEScBsGJRijnObeYxbBEz3NUV\na6X19WUjzgMDESjcPRTlH9qVbeu8cXtaKq03Rm3d+st5q5dHnauXziunNQ9HHPHCthjRXblgcTmv\nuz9GpDdt2RRlLRuhbp0fy7Q1pNjoLQPZEm09HXHdWB/LxNXnlqirb8nqEJlghwDXAncDXwSWAM8H\nvmtmT3T3ywHMrAn4MXAmcBvwKWA+8Bzga2Z2sru/s0L9hwO/AW4Hvgy0ALvNbDXwW2Ah8APgW8A8\n4FDgxcAnge2lSszs88DLgAdS2V3Ao4D3AeeY2ZPcXbvliIgcQOZs51hEptVZwAXufmEpwcy+AvwI\neCtweUp+C9Ex/iHwjFJH1MwuJDrX7zCz77n7rwr1Pxb4YLHjbGbnEx3xN7r7vxfyWsnNmTWzlxId\n4+8Af+XuPbm8C4D3An8H7FVPkZldN0LWMdWOExGRmUlhFSIyGe4F/jmf4O4/Bu4DTsslvxxw4M35\nEVp3f4gYvQV4RYX6twAXVkgv6SkmuHtXvgMMvAEYBF5eSCedezvwV1XOISIic9CcHTluXxBhBDt6\ns0lnHbvvA6ChMZY+O/7Yo8p5B62OEIu6+gg7GB7Ovjd0pTXWOnoGANjT1ZfV2REhFr3DMdFtz64s\npKF3d8Q0LGxJO+WlpeAAFi6J88xrzH0/STvvDaSl2XaXYiKA7s4I16gbiDYsaluQOy7SmlOYxOLW\nleWsh7ZH/WvXrIvj67JwkS0PbkZkkvzB3YcqpN8PPBrAzBYARwAb3f22CmV/nq4fXiHvBnfvq5D+\nv8AHgE+Z2ZOJkI2rgVs8twajmc0HTgK2AW80swpV0QccWykjz91PqZSeRpQfMdrxIiIys8zZzrGI\nTKuR9iYfJPvFqrTw90jf0krp7RXyHqx0gLvfa2anARcAfw78Rcq638w+4u4fT7cXAwYsJ8InRERE\ngDncOa5riBHd9hXZ6Gv/1phQt3h53F60dEk5756NtwPQujZGkweHsodmW1eMzO7sjdGl3Z3ZJiC9\n3TESOzScll/LbRCydn6M5K5YuSbOuyibYFdfH78gW102+NWXugz9aaS5rzmbB3TvhnuijrpGALo7\nBsp5w81Rx8LGGP1e3LiinNcwEPdj+0MxSb93OGvflgeziYUi06D05lw1Qv7qQrk8r5AWGe63As83\nswZidPiJwPnAv5tZl7t/Llfn791do7siIlI2ZzvHIjKzufseM7sLOMzMjnT3OwpFzk7X1+9n/YPA\ndcB1ZvYr4BfAecDn3L3TzG4GjjezJe6+Yz/vRlUnrFnEddpIQ0RkVtGEPBGZTp8nwhv+1czK6wya\n2TLgPbkyNTGzU8xsUYWsUiB+dy7to0AT8Hkz2yd0w8wWm5lGlUVEDjBzduS4tz8m3zXNz9b1PfjQ\nCElYtiLteNezu5zXv6ANgM6hCFcYGsgm6Ny3PcIWNmyOX2K7tudCGnrj7waLEIWF87OH9MTjDou8\neRFOMZSfnuTxvcQtS+zojXWO9wzHBEAfzsIe+lMYxvbdUcZbsv0JVq2LOJGlyyJ8o3dTVmffnmjz\nzm3RJ+jNPeW9vY2ITLOPAE8BngncYGY/INY5fi6wAviwu/9yDPW9GHiVmf0SuAvYSayJ/HRigt1F\npYLu/nkzOwV4LXCXmZVW01hCrIv8eOC/gVeP6x6KiMisMmc7xyIy87l7v5k9CXgz8EIiNngQuIFY\nq/irY6zyq0AzcAZwCrE5yEbgUuDf3P2mwvn/zsx+SHSAn0hM/ttBdJL/FfjSft41gPW33norp5xS\ncTELERGp4tZbbwVYPx3nttzqRiIiMkHMrA+oJzr6IjNRaaOaSkspiky3k4Ahd28eteQE08ixiMjk\nuAlGXgdZZLqVdnfUa1Rmoiq7j046TcgTEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnUORYR\nERERSbSUm4iIiIhIopFjEREREZFEnWMRERERkUSdYxERERGRRJ1jEREREZFEnWMRERERkUSdYxER\nERGRRJ1jEREREZFEnWMRERERkUSdYxGRGpjZWjP7vJltMrM+M9tgZheZ2eLpqEekaCJeW+kYH+Hy\n4GS2X+Y2M3uOmX3CzK4ys93pNfWl/axrUj9HtUOeiMgozOxw4FfACuC7wG3AacDZwJ+Ax7j79qmq\nR6RoAl+jG4B24KIK2Z3u/pGJarMcWMzsD8BJQCfwAHAM8GV3f9EY65n0z9GG8RwsInKA+A/ig/j1\n7v6JUqKZfRR4E/B+4NVTWI9I0US+tna5+wUT3kI50L2J6BTfCZwJXL6f9Uz656hGjkVEqkijFHcC\nG4DD3X04l7cA2AwYsMLduya7HpGiiXxtpZFj3H39JDVXBDM7i+gcj2nkeKo+RxVzLCJS3dnp+if5\nD2IAd98DXA3MBx41RfWIFE30a6vZzF5kZu80szeY2dlmVj+B7RXZX1PyOarOsYhIdUen69tHyL8j\nXR81RfWIFE30a2sV8EXi5+mLgJ8Dd5jZmfvdQpGJMSWfo+oci4hUtyhdd4yQX0pvn6J6RIom8rX1\n38A5RAe5FTgR+E9gPfBDMztp/5spMm5T8jmqCXkiIiICgLtfWEi6CXi1mXUCbwEuAJ411e0SmUoa\nORYRqa40ErFohPxS+q4pqkekaCpeW59J148fRx0i4zUln6PqHIuIVPendD1SDNuR6XqkGLiJrkek\naCpeW1vTdes46hAZryn5HFXnWESkutJanH9mZnt9Zqalgx4DdAPXTFE9IkVT8doqzf6/exx1iIzX\nlHyOqnMsIlKFu98F/ISYkPR3hewLiZG0L5bW1DSzRjM7Jq3Hud/1iNRqol6jZnasme0zMmxm64FP\nppv7td2vyFhM9+eoNgERERlFhe1KbwVOJ9bcvB04o7RdaepI3APcW9xIYSz1iIzFRLxGzewCYtLd\nL4B7gT3A4cC5wDzgB8Cz3L1/Cu6SzDFmdh5wXrq5Cngy8UvEVSltm7v/fSq7nmn8HFXnWESkBmZ2\nMPBPwJ8DS4mdmL4DXOjuO3Pl1jPCh/pY6hEZq/G+RtM6xq8GHk62lNsu4A/EusdfdHUaZD+lL1/v\nrVKk/Hqc7s9RdY5FRERERBLFHIuIiIiIJOoci4iIiIgk6hzPQWZ2hZm5mb10P459aTr2iomsV0RE\nRGQ2mNPbR5vZG4n9tS9x9w3T3BwRERERmeHmdOcYeCNwCHAFsGFaWzJ7dBA70Nw33Q0RERERmWpz\nvXMsY+Tu3yGWQxERERE54CjmWEREREQkmbLOsZktM7PXmtl3zew2M9tjZl1mdouZfdTMDqpwzFlp\nAtiGKvXuM4HMzC4wMydCKgAuT2W8ymSzw83sP83sbjPrNbOdZvYLM3uFmdWPcO7yBDUzW2hmHzaz\nu8ysJ9XzT2Y2L1f+HDP7sZltS/f9F2b2uFEetzG3q3D8YjP7WO74B8zsYjNbXevjWSszqzOzF5vZ\nT81sq5n1m9kmM/uamZ0+1vpEREREptpUhlW8ndiWEmAQ2A0sAo5NlxeZ2RPd/cYJOFcnsAVYTnwB\n2Ankt7vckS9sZk8DvkFsjwkRd9sKPC5dnm9m51XZq3sxcC1wNNAF1AOHAu8BTgaeYWavJfam99S+\n+anun5nZE9z96mKlE9CupcBvie0/e4jHfQ3wSuA8MzvT3W8d4dgxMbMFwLeBJ6YkJ7YeXQ08D3iO\nmb3B3T85EecTERERmQxTGVZxH/BO4GFAi7svBZqBRwI/JjqyXzEzG++J3P0j7r4KuD8l/YW7r8pd\n/qJUNu3RfSnRAb0SOMbd24EFwKuAPqLD9+9VTlnaDvFx7t4GtBEd0EHg6Wb2HuAi4EPAUndfBKwH\nfg00AR8rVjhB7XpPKv90oC217SxiS8blwDfMrLHK8WPxhdSe64n90uen+7kEeDcwBPy7mT1mgs4n\nIiIiMuGmrHPs7h939w+6+x/dfTClDbn7dcAzgVuA44HHT1WbkncSo7F3AU919z+ltvW5+8XA61O5\nl5vZESPU0Qo8zd1/mY7td/fPEh1GiP2/v+Tu73T3XanMvcALiBHWU81s3SS0ayHwbHf/nrsPp+Ov\nBJ5CjKQfDzx/lMdnVGb2ROA8YpWLJ7j7T9y9N51vp7u/H/hH4vX2jvGeT0RERGSyzIgJee7eB/w0\n3ZyykcU0Sv3sdPNj7t5dodhngY2AAc8ZoapvuPudFdJ/lvv7g8XM1EEuHXfCJLTrqlKHvXDePwHf\nTCmuGdkAACAASURBVDdHOnYsXpKu/8vdO0Yo8+V0fXYtsdIiIiIi02FKO8dmdoyZfdLMbjSz3WY2\nXJokB7whFdtnYt4kOoyIewa4vFKBNOJ6Rbr5iBHq+eMI6Q+l616yTnDRlnS9eBLadcUI6RChGtWO\nHYsz0vW7zezBShci9hki1nrpBJxTREREZMJN2YQ8M/tLIsygFOM6TEww60u324gwgtapahMRd1uy\nsUq5ByqUz9s8QvpQut7i7j5KmXzs70S1q9qxpbyRjh2L0soX7TWWnz8B5xQRERGZcFMycmxmy4H/\nIjqAXyMm4c1z98WlSXJkk9LGPSFvP80bvci0mKntyiu9jp7l7lbDZcN0NlZERERkJFMVVvEUYmT4\nFuCF7n6duw8UyqyscNxguq7WQVxUJW80W3N/FyfE5a2tUH4yTVS7qoWolPIm4j6VQkOqtVVERERk\nxpuqznGpE3djadWEvDQB7QkVjtuVrleYWdMIdZ9a5bylc400Gn137hxnVypgZnXE8mcQy5RNhYlq\n15lVzlHKm4j79Ot0/ZQJqEtERERk2kxV57i0gsEJI6xj/Epio4qi24mYZCPW6t1LWsLs2cX0nN3p\numIsbIoD/na6+QYzqxQL+wpi4wwnNuSYdBPYrjPN7IxiopkdSbZKxUTcp0vS9ZPN7M+rFTSzxdXy\nRURERKbTVHWOf0Z04k4APm5m7QBpy+W3Ap8CthcPcvd+4Lvp5sfM7LFpi+I6M/szYvm3nirnvTld\nvyC/jXPBB4hd7Q4Cvm9mR6e2NZvZK4GPp3Kfc/e7ary/E2Ei2rUb+LaZPbX0pSRtV/1DYgOWm4Gv\nj7eh7v4jojNvwHfM7K0pzpx0zmVm9hwz+z7w0fGeT0RERGSyTEnnOK2re1G6+Tpgp5ntJLZ1/jBw\nGfCZEQ5/B9FxPhi4itiSuIvYVW8XcEGVU38uXT8X6DCz+81sg5ldmmvbXcRmHL1EmMJtqW17gIuJ\nTuRlwBtrv8fjN0Hteh+xVfX3gS4z2wP8ghil3wo8r0Ls9/76a+B/iPjwDwNbzGxnOudWYoT6qRN0\nLhEREZFJMZU75L0Z+Fvg90SoRH36+43AuWST74rH3Q2cDnyV6GTVE0uYvZ/YMGR3pePSsT8HnkWs\n6dtDhCEcAqwqlPs/4ERiRY0NxFJj3cAvU5uf7O5dY77T4zQB7doOnEZ8MdlCbFW9KdV3srvfMoFt\n7XL3ZwFPI0aRN6X2NhBrPH8deBlw/kSdU0RERGSi2cjL74qIiIiIHFhmxPbRIiIiIiIzgTrHIiIi\nIiKJOsciIiIiIok6xyIiIiIiiTrHIiIiIiKJOsciIiIiIok6xyIiIiIiiTrHIiIiIiKJOsciIiIi\nIknDdDdARGQuMrN7gIXE1u8iIjI264Hd7n7oVJ94znaO/+Etf+MAi484rpx2/8HnADBgq+P2Vz9Y\nzmvffDMAXQ0GQH3bqnJec1N7pDXGVtvt7QvLeYN9fQC0zI+0TQ9uLeftHuoFYH5zMwDrVi4p5z32\nUdGu5vquclrX7t2RNi/O3dLaWs5ra42nasM9GwDYtmN3OW/TlocAOOLQgwFYf8ix2QNh9dH2hvQj\nQV1jOauhPuo86+wzDBGZaAtbWlqWHHvssUtGLyoiInm33norPT0903LuOds5/s7/XQ5Aw7yry2mD\n8/8XgEOOfTwAS/4/e3ceZ/lV1/n/9bm36t7a197X6oSshKwIJETSDBpwIj8j6iCiQ9BxxGVAXMag\nOAkqghvigIAbgoArCMg2ZiYYIAsi6YSQpLN3pffq7tr3qnvv+f3xOfU9N8Wt3rur+/b7+Xjkcau/\nn+9ybnWl+tSnPudzQshixUI7AI2dPslt7kr/nnV29QIQrAxAS2OaYHZ2rgZg9QY//4479mWxvft8\n4vuCC9cDMDcxlMX2HRoA4MK+ldmx2bLPUcv4c4bGhrPYwOAcAOPTfs72Z9JzRg4NAtDduwKAjbli\nFguVCgCVsk+Oc6GSxXI2j4icMv2XXHJJz/3337/c4xAROetcc801bNu2rX85nq2aYxE5J5lZn5kF\nM/vIco9FRETOHJoci8gpowmoiIicbeq2rKJ5zToAGkilAzNjh/yDwccB6NzUl8V27vUa3tWrugHY\nsmVVFis2eSlDJVZhNObSzxTFon8KzVr8eYVUjjE52Q9A/zNPAjA+MpLFJhoLAKxpSyUajWWvrRkr\nlQAoz6fnzEx5iUZnh9dLv+iqq7LYPV+7G4D5spdMNOemslh7g5dmTAavm54l1THnTKXGIqfSw3tG\n6bv1C8s9DBE5BfrffdNyD0FOEWWORURERESius0cX3Hd9QBsXp8yueUJz8i2FzzLWw75LLb9AV+4\nVy57dnfFiq4stpBfbSr4QrnZudkslm/0ny/mZj0jWwnlLDa0tx+Ahw7s8OubUtb20iu8W8XM7Ex2\n7OCYfxzCQqY63auzy8fT3u4LBiu5tixWbGmIY/BxdcwdyGLnFbcDsLtyHgADDRdmMXLp/YucbGZ2\nO3Bb/OMbzOwNVeE34i3O/g14B/DFeO61QDewJYTQb2YB+EoIYWuN+38EeMPCuYtiLwJ+GbgeWAEM\nAd8G/jKE8I9HGHcO+GPgzcCngdeHEJZnybSIiJx2dTs5FpFldxfQBbwF+BbwmarYgzEGPiF+G3A3\n8GF8Mjt3vA81s58GPgiUgX8BngRWAS8Efg5YcnJsZk3AJ4DXAH8KvDmEqhYvta9Zqh3Fxcc8eBER\nWXZ1OzlevdJ7/t50w5XZsf0DXot78OAoAENjE1ms0OFt0AKeTS1X9dYrzfl1UxWvXz44mv7dnij5\nv5uz5f0AjIzszWKtTU0A5PCMbmM+1fiu7fHnDQyn7PDovLeTK8RexHlSjXJzxWuUZ0t+j+bm9FfX\nWPRYbtbrkltmd6X3FXxc3bHGecguyGLBlDmWUyeEcJeZ9eOT4wdDCLdXx81sa/zwRuBNIYQ/O9Fn\nmtmlwAeAMeC7QwiPLIpvOMy1Pfhk+jrg1hDC753oeERE5OxTt5NjETlrPHgyJsbRz+Lf13578cQY\nIISwu9ZFZrYZ+D/A+cBPhBA+cbQPDCFcs8Q97weuPtr7iIjImUGTYxFZbt84ifd6SXz90jFccxFw\nH9AKfF8I4c6TOB4RETnL1O3keK7kJRAtDalsYVO3l0AUYk+2lS2prOCugpcd5HO+aG5kcCCLFaa8\nDCN0eRu1kal03cEhL7+YmfcSiBDSp3Rzn28HPnhwDwA93Z1ZLN/oZRUTh1IZRiHucNeY87F0FUpZ\nLMQ1gENxS+ot7d3puoKPuW3Wd+Br2Z/KRSor/f03jvoivVx3KtXIdbUgcgbYfxLvtVDHvOcYrrkQ\n6MHroLedxLGIiMhZSK3cRGS5hSPElvohvqvGsYWf/tYfw/M/B/w6cCVwp5n1HsO1IiJSZ+o2c3xw\nr2dKn3giZWbbm/1ngYsv8oxuQ1WWt9j9NQAqDUUAhoYHs9jQaFysPuEbagxNpozu6L5nASibZ3SL\nc6NZbM2KmNHt8Uzz+VvWZrEV7f4cm6n69908291Q9HEWLD1nzSrPOudynu3N51NGvLnYDMD4sI/z\nie3NWax3wLPQIfh1lSvSz0MHp1N2XOQUWfhCPd7Vn8PAxsUHzSyPT2YX+zreleL7gMeO9iEhhHeZ\n2TTewu0uM/ueEMIJ/w9y2fpO7tdGASIiZxVljkXkVBrGs7+bjvP6bwCbzOzGRcffDmyucf4HgRLw\nm7FzxXMcrltFCOG9+IK+5wNfMbN1xzlmERE5i9Vt5lhEll8IYcLM/h34bjP7BPAEqf/w0fhD4JXA\nZ83sH/DNPK4DtuB9lLcuet6jZvZzwIeAB8zss3if417gu/AWby8/zHg/ZGYzwF8BXzWz/xRC2HmU\nYxURkTpQt5PjQzt9Z7jpybQjXFublxbs2f00AOXZxizW2ew9hicPeY/gA1PzWWw0eElDmPDfso6P\npNKJVQXvgXz95XERXVNKNvW0+wK7uZKXUBQb0gK4xoqvQepoSWNobvb+yRNzXr6xY8+hLNa/YwyA\nS85bDcCaTalfcTH2U94d+zA/U0675zHov80ebvESjd3PPJGFhmd8ld9ruBmRU+gn8HKFVwGvwzed\n3I3vkHdYIYQ7zexm4H8BPwpMAv8XeC2+s16ta/7CzB4GfgWfPN8MHAIeAv7yKJ75ETObBf6GNEF+\n5kjXiYhIfajbybGInBlCCE8Br14ibEscr77+X6idab4l/lfrmvuAHzrCffuXen4I4e+AvzvS2ERE\npP7U7eS4mPOM6c7+1CVqbMjf7uPbvwnA+FjKDs+Yt0abjTvjtRZS9vW6i30h3dCgZ3TbN6fscHuD\nn3/z831vgb3TPVns4d2+6L1Y9MzzwYnZ9LyD3m6tVE4L9RviTndNDf7vdVdj+uvZsb8fgGdH/F5r\nN6dYU5NfNzzliwJnmlJmuyP+079j0jPUT4/3Z7EVa9cgIiIiIokW5ImIiIiIRHWbOQ49nrX99th4\ndqx9zDOsK81boM6UDmax7h6vB+7IeVa4t6c9i82Me3Y4l/M07HkX9GWxidjW7dkZr/td1Zzu2Vj0\nT+/g+CQAU3PpZ5EQO1xVQvqtbog1w4MVv1dLMWWvt17hz+xc58fyuUoWa22Kx+Kh3nxqAbeuN7Z1\nK/r7enpP+nwUq7LjIiIiIqLMsYiIiIhIRpNjEREREZGobssqyHtbtFIhlR+Mz/rHjbE8Yl1DWpD3\nstYhADas8bKD/NpCFnt63EsTnp3yMoSGXIqdf+l5AIwN+0K5jvEvZbFL13o5xVMtvsFXU1daADcR\nSy0OHDiQHZuLe4mF4K3fRuZTaUd5xx4ALsv5wr8NF6aSiM4mX3xYzPs4c/PpfTXEHfUq494KLle1\nT1lToYiIiIiIJMoci4iIiIhEdZs5Hnim3z/YlVq5NcW3233pFgC+qzUtXLt+/5MAFCd8U48w+1QW\nW9u1CoBLVvqGIkNNnVns4AHPQn/rsR0A7AkpU33jld5SbS7nz8uvWp3F5ld59rk0l7K8zXm/Vz7v\nbddKoZzFGhs965yb8wx358D2LHZJybPku9r92UVSRvjxp739HEW/Z749ZZwblDgWEREReQ5ljkVE\nREREorrNHHe2tgLwXVddnh276ILnAzA07hnW1d9Im27t3+01uY0xndrVkTbnaGrfB8CGXs/aru59\nNoutbr8EgEPNXsxbKq3IYnMH/LwtWzw2Gqpqgec8Q73lvKrWajEDXJr2zULyIW0aspD1zs3GmuiB\nwSy2IiaYu8/3rHB5Io2h1H4+AIP7PYNenElFx825JkREREQkUeZYRERERCTS5FhEREREJKrbsop9\nz/gCuScb07FK7JU2NejlCzcMHcpinQ1eYnBwbgaA8YH0c8OqYb9J+3Dc1e7ZHVmsqeAfX7+xD4BS\nJZUtdD+zC4C24oD/+YLUAs4avDwiN9KfBhh30LOK/7XY/Fwae7kUY35OyFct1qv4x5ub/ZxyIcXm\n1ngbuZlJX+xXnk6LEK2xqq+biIiIiChzLCJnJjMLZnbXMZy/NV5z+6Ljd5lZWOIyERGR56jbzPHB\nAc/Wbv/2o+lgo2/Q8fz13lKtfWVHFnptwbO6W5r8dbSU/i3dERfIrZ337HJPQ8oAd3R4lrZrr2eJ\nKaafNypNHiv3+2K/cjm1X8uv6/IP2lP2tlLe4B/EdXg2mzYIsXkfj4WZeG56TpjzrHCYidnombTI\nr3HHHQCsa+gDoKHQk8aQ189G9SROAL8SQti63GMRERE5W9Xt5FhEzjnfAC4BDh3pxNPl4T2j9N36\nheUeRt3qf/dNyz0EEalDmhyLSF0IIUwBjy33OERE5OxWt5PjdT2+i90N116bHXvwyWcAKMx7acKe\nVX1Z7I49OwG4ZtIXrK1tTZ+alUUvpxiPbYrDXNoFr3PUz2uc8etyHSlGk5dMNPZ6+UJDeSALzT3h\nfYetN/Ukrpznu+BZi/dAtnLqi1zK+S59udGDAORnR9Nzil6HUWn2PselciqdaBr0nf5WVLzsY32u\nOYs1NmqLvNPJzG4BXg1cBawF5oFvAx8MIXx80bn9ACGEvhr3uR24DXh5COGueN+/juEbFtXXviOE\ncHvVtf8F+AXgCqAAPAX8LfCeEKoaa1eNAbgM+G3gh4EVwOPA7SGEz5hZA/BrwC3ARmAP8MchhPfX\nGHcO+O/AT+EZXgMeBT4M/FkIVdtLPve6dcDvAa8E2uM1fxRC+NtF520F/m3xez4cM3sl8BbgRfHe\nu4F/Bt4ZQhg5mnuIiEh9qdvJscgZ6IPAI8BXgX1AL/CfgY+Z2UUhhN88zvs+CLwDnzA/C3ykKnbX\nwgdm9rvA2/Cyg78FJoDvA34XeKWZ3RhCmOO5GoH/C/QAn8Un1K8DPmVmNwI/B7wY+BJeLf8jwPvM\n7GAI4R8W3etjwI8Bu4C/BALwg8AHgOuB19d4b93AvcAI/gNAF/BfgE+Y2foQwh8c8bOzBDO7Dbgd\nGAI+DxwALgd+BfjPZnZtCGHseO8vIiJnp7qdHK9o9fZrl/etz449s2s3kHbPe/GNr85i+774jwA8\n9qyfs3c2tUM7r+gJrTVxsd54VX5rfM6TbSsrHhvLp0/p3Tv84745zypfe2m6sDjm2eGp4Zns2FSn\nZ7sbm33BYGHgYBZrWOsZ5tB7AQCV6T1ZLDfpO/GFKX9OabYli42OedZ6bsyTYF1VLeDKxao+d3I6\nXBZCeLr6gJkV8InlrWb2oRDCntqXLi2E8CDwYJzs9dfKmprZtfjEeBfwohDC/nj8bcCnge/HJ4W/\nu+jSdcA2YOtCZtnMPoZP8P8JeDq+r5EYew9e2nArkE2Ozex1+MT4AeBlIYSJePztwFeAHzOzLyzO\nBuOT1X8CfnQhs2xm7wbuB95pZp8KITxzbJ8xMLOX4xPj+4D/XJ0lrsrEvwN461Hc6/4lQhcf67hE\nRGT5qV2ByGmyeGIcj80Bf4r/oPqKU/j4n4yvv7MwMY7PLwG/DFSA/7bEtb9YXXIRQvgasAPP6v5a\n9cQyTlTvAS4zs+pG2gvPv3VhYhzPn8TLMlji+eX4jErVNTuA/41ntX9iyXd8eG+Orz+9uHwihPAR\nPBtfK5MtIiJ1rm4zx8z5b4efuu++7FB5t9fdrrnsagB6VvZmsQOr1gIwP+Zt0EarqjafiT9DjOEt\n07oKVvUc/3c+N+vH7m1pykJfLHYDsHKf33PF3GQWu6jRP/Xli1+UxnfRDwBQemIbAPlH0r/Z+b1P\n+Pg6z/NzhlNs5pDXMs9O+LH58ZSNZtIzxW0FH9fFF6f5yu5Cakknp56ZbcIngq8ANgHNi05Z/x0X\nnTxXx9cvLw6EEJ4ws93AFjPrDCFUFbQzUmtSD+wFtuAZ3MX24N9b1sSPF55foarMo8pX8EnwVTVi\nO+NkeLG78DKSWtccjWvxmu8fMbMfqREvACvNrDeEMHi4G4UQrql1PGaUr64VExGRM1f9To5FziBm\ndh7eaqwb+BpwBzCKTwr7gDcAp3KFZGd83bdEfB8+Ye+K41owWvt0SgCLJtLPieGZ3ernD9WoaSaE\nUDKzQ8CqGvcaqHEMYCH73blE/Eh68e9/tx3hvDbgsJNjERGpL5oci5wev4RPyN4Yf22fifW4b1h0\nfgXPXtbSdRzPX5jErsHrhBdbu+i8k20U6DGzxhDCfHUgdrxYAdRa/LZ6ifutqbrv8Y4nF0LoOeKZ\nIiJyTqnbyXG3bQbg4qbnZccuu/w6AJo3+9xidjqVH8zFNmjj7R5ra0zlB5PluIgtvsxUlVx0Nnu5\nwsCs32uguzuLXXPliwGYnvB/vx948ltZbOOwl1Ba6cnsmPV8BYDGi670x33Pz6TxPeq7+zX03wlA\nYTSNvXnYB1Yqx4GV05yqEktBSrHso3k6zSXCeFrwJ6fcwhfip2rEbqhxbBi4vNZkEnjhEs+oAPkl\nYg/gv+LfyqLJsZk9D9gA7DiF7csewMtJXgbcuSj2Mnzc22pct8nM+kII/YuOb6267/H4OnCTmT0/\nhPDIcd7jiC5b38n92qhCROSsogV5IqdHf3zdWn0w9tmttRDtG/gPr29cdP4twEuXeMYg3mu4lg/H\n17eb2cqq++WBP8S/F/zVUoM/CRae/y4zy9qpxI/fHf9Y6/l54Pdij+SFa7bgC+pKwMdrXHM0/ji+\n/kXso/wcZtZqZi85znuLiMhZrG4zx5dd9z0AXHTeC7Jj1uJvt1SIC+9b0s8Gg1NeCrk/tnBbX0jt\n0FrznrirNHgspG5o5GKZaCHv91q5vi+LbbzC1wo9/rgvpts5kMonB0Z9Id+KXc9mx/Kf/Ki/vuAh\nH+8NN6exv+ynfAwPfB6AhvtS8s1KnjGuzPsYJqs2DxmPm5Psm/L3PFzMGhVQft4wctp8AJ/o/pOZ\nfRJf0HYZ8CrgH4HXLjr/ffH8D5rZK/AWbFfiC8k+j7deW+xO4EfN7HN4FnYe+GoI4ashhHvN7PeB\n/wk8HMcwifc5vgy4GzjunsFHEkL4WzP7AbxH8SNm9hm8z/HN+MK+fwghfKLGpQ/hfZTvN7M7SH2O\nu4D/ucRiwaMZz51mdivwLuBJM/si3oGjDdiMZ/Pvxv9+RETkHFK3k2ORM0kI4aHYW/d3gJvw//e+\nBbwG3+DitYvOf9TMvgfvO/xqPEv6NXxy/BpqT47fgk84X4FvLpLDe/V+Nd7z18zsAXyHvP+KL5h7\nGng7vuPcdyyWO8leh3em+ElgoWZoO/BH+AYptQzjE/jfx39Y6MB3yPvDGj2Rj0kI4ffM7B48C309\n8AN4LfIe4M/xjVJEROQcU7eT40uv3wDAXKmq5VncLnlsylO/A9vTfgsPPPYYAOWcZ19nVqWuWs1D\nnvFdl/PrVhWqFuHn/FPYEjPHvavSb2hbY6u0NSt8Ef6/t2S/zeYp822dW6sW9M9PTgPQ9OBXASjs\nfiyL2fXfB0B4iW9csv+hvVls4vF/B2Cw5GMYrpSy2FjJs8gDcz72StXmJhvzNXfrlVMkhHAv8J+W\nCNviAyGEu/F63MUewjewWHz+AXyjjcON4e+Bvz/SWOO5fYeJbT1M7BZ8O+nFxyt4Bv0DR/n86s/J\njx/F+XdR+/O49TDX3I1niEVERADVHIuIiIiIZDQ5FhERERGJ6rasorHo5QNNhbQJWXvcJa69yX/z\nOj+ZdrO7/MpLARgc84Vyzx5Kbc5Gdnv5RbHki9o2dLRlsQt6vU3q8zr8ORcW0z4Oo5O+I15TjOXb\nUnvaHcEX/J1XSbvmLRQ8lILfv2V8Kos1bLsHgKk+X+S3tz21fx2b9jKK4VjaMWapXOJQ3jt7HWpv\n9ff5oq1ZrHf9+YiIiIhIosyxiIiIiEhUt5njthbP6Fo+ZXKLBc8Yr4hrdvp60gYh33Whb7i178Ah\nAHbuTW3XHnvSN+p4dPt2AB7YtTuLPbLfd+O9oNezwj/8gquz2LqKZ22bGuKGH5aaAeyKbdcOVdKe\nDSHmjgtxEd1sKbWTa5yPbeiGhwA40JzeV3+7v9eDDf6+RvPpr3UiLhgsrPBddsPalHEOttR+ESIi\nIiLnJmWORUREREQiTY5FRERERKK6LauYjAvrcqRShkqT9x3Ox5IEy09nsbZGL2U4f107ABtXtWax\nqy/ZDMDAtdcA8NhTO7LYtoceBuDhRx8B4M5vfzuL/cxLvKXtyCHvSXywapFfOZZO7Kqkv4J88J9V\n8pVKfJ3JYrZvEICVU/6+xjb3ZbF9L/Rdbudif+Py3GwWa5zxBX+bYznFQu9lgPGJMUREREQkUeZY\nRERERCSq28zx3JxnTC2Xsq8NldZ4zH8maGxMPxvkLWaVYxu08nzKOPc0+6ept20FABtWplZuL7jI\ns8oPPnoRAP39abHewH7PMI9OeUu2Z3ftymKlec/yfmZiPB2b80y2Bf+zWdVfT9HvsfJu3z1vtiWN\n4eCAZ6YHB30x4f64SBAgV/ZFfoPPu8DfXy61ttuw5TxEREREJFHmWEREREQkqtvMcVvc6KPYnGps\nC41+zPAMbaUcslhj/Ezk4qG8WRaz4Flky/lrpapWearB63sv3eI1vd1tjVnsvvvuAuDQuGevDxwc\nymJj4147/OjYaHasNOtZ60ps6dbQkO41V/Hzbc8X/bWSNvoohfQxgFWNvafTa6hp8DdYbEkbnzQ1\nNSIiIiIiiTLHIiIiIiKRJsciIiIiIlHdllUw76UMFUulE6WyL4ILcSe5iqVyhNngrdUseElDdVlF\nPu4kV4mlDLOz5SzWEPye7XHhX9PKtKtdS9E/vfNP7wRgbW9aDNdofn6e1DJudsrHUCn7a0ND+tml\nueIlEOVYQlFsSuUixYKXSrS3dQCwavWqLLZ54wYALujzhYNrqmKFYiqxEFlgZncBN4QQ7EjnnuBz\n+oAdwEdDCLecymeJiIgcLWWORURERESius0cHxjyTTMslzKsTU2euW1u8rfdkKtakGe+2K6Y92RZ\nvurnhpzFzTkWFrVVLZRrbOsEoK3JM8aWT8m2lSs949wVF8Vt2TCcxcanPeM8OjqRHRuZ9I/nZjwz\nXSqlDDVxqM0t/pz2jo4s1NPZDUBnl7+2t6dYZ2cXAL3dPs6e7p40vtVrEanhvwItRzxLRESkDtXt\n5FhEjk8IYedyj0FERGS51O3kuLvD62nnyymTm4uZ4vkZb8U2OZ+2Wc7HmuNCrOltqKpVzsfPUiH2\neytUb86R849zjXETkVyq4y3EDPOqlb5ddXNLytqOT3qmemYmbTYyF2uajZiZDuk5uUbPQne0++Yf\nHVWZ445Ozwq3t/tra1uKLdQhF5q8tnm+Khk9NJo2IJH6Zma3AK8GrgLWAvPAt4EPhhA+vujcu1hU\nc2xmW4F/A94BfBG4DbgW6Aa2hBD6zaw/nn4F8E7gB4Fe4BngQ8D7Qgjpf6ylx3oh8JPA9wCbgQ5g\nP/CvwG+FEHYvOr96bJ+Jz34pUAD+A3hbCOHeGs9pAP47nim/FP9++DjwV8AHQljUI1FERM4JzfnW\nbgAAIABJREFUqjkWOTd8EJ9ofhV4L/D38c8fM7PfPob7XAt8DWgCPgx8FJiriheA/we8Mj7jL4Au\n4E+A9x/lM14DvAnYBfwd8D7gUeC/Af9hZuuXuO6FwL1xbH8JfB64HrjTzC6qPtHMGmP8T+P4/hb4\nc/x74vvi+xIRkXNQ3WaOReQ5LgshPF19wMwKwJeAW83sQyGEPUdxnxuBN4UQ/myJ+Fo8U3xZCGE2\nPuc2PIP7c2b2DyGErx7hGR8D/njh+qrx3hjH+3bgZ2tcdxPwxhDCR6qu+Rk8a/0W4Oeqzv0NfAL/\nfuAXQ/A2NWaWxyfJP2lmnwwhfPYIY8XM7l8idPGRrhURkTNP3U6OL7nkSgCKVsqOLfyWdGGhW7k0\nn8VKs74YLszNxVhKhpXLC8fidVUJ90ps85Zr8LIKayimWFwM2FzwMoliSz6LrV4VF/cVUyu3XIOX\nZDQU/B4NVTvYFZq8nKIQz89XLQokjmEuvp2RybSD3+59UwAMjR4AYHB0JouNzfhvuC97fh9S3xZP\njOOxOTP7U+A/Aa8A/uYobvXgYSbGC95WPbENIQzF7PRfA2/Es9eHG2vNSXoI4Q4zewSf1NZyT/XE\nOPowPgF+0cIBM8sB/wMv1XjrwsQ4PqNsZr8cx/l64IiTYxERqS91OzkWkcTMNgG/hk+CNwHNi05Z\nqlRhsW8cIV7CSxsWuyu+XnWkB5jvf/564Ba8frkbyFedMlfjMoBvLj4QQpg3s4F4jwUXAj3Ak8Db\nq7dbrzINXHKkscZnXFPreMwoX3009xARkTNH3U6OP/XAJAAr29O/qT3tnpntbI2bZjSn7GtLTOAW\n48YbDVX/FOdjojiH/yP6nH9Lc7l4LJ5kKatsOb+J5T2DnMunrDJ5H0PZqjLN8eNynAfMVS0mnJ33\nLO/gtKeHx4ZTdnhsyrPDoxOeJZ+YThnxyVk/by4em5pPc6Kx+cXzI6lHZnYePqntxuuF7wBGgTLQ\nB7wBKC51/SL7jxA/VJ2JrXFd51E84z3ALwL78EV4e/DJKviEefMS140scbzEcyfXvfH1Anxh4VLa\njmKsIiJSZ+p2ciwimV/CJ4RvXFx2YGavwyfHR+tI3SZWmFm+xgR5TXwdPdzFZrYKeDPwMHBdCGF8\nUfx1xzDWpSyM4dMhhNechPuJiEgdUbcKkfr3vPj6qRqxG07ysxqA62oc3xpfHzjC9efh35fuqDEx\n3hDjJ+oxPMv8kti1QkREJFO3meMHprcA0FJKrUqbxz3p1drkPxN0NKXftLbFj1tjqUVrVclFW7OX\nRbS1eBlCoZB+A11o9Hs1xp3xGhtSKUQ+3j4ruaj6WWQ25tWm51Mibnre4zPxWHk+jX3h2NScnzM3\nm0oipuPHE7GJ8fh02nVvIi7AGxvx6yerqjVLTWkxoNS1/vi6FfjcwkEzeyXeHu1ke5eZvaKqW0UP\n3mECfFHe4fTH1+urM9Bm1oa3hTvh71khhJKZvQ/4TeB/m9kvhRCmq88xs7VAdwjh0RN9noiInF3q\ndnIsIpkP4N0X/snMPgnsBS4DXgX8I/Dak/isfXj98sNm9i9AI/DDeIu3DxypjVsIYb+Z/T3wo8CD\nZnYHXqf8vcAM8CBw5UkY52/ji/3eBLzazL6M1zavwmuRX4q3ezuRyXHf9u3bueaamuv1RETkMLZv\n3w6+Lua0q9vJ8T++5ZqaS9DPDT1VH29atlHImSGE8JCZvRz4HbwXcAPwLXyzjRFO7uR4Dt/Z7nfx\nCe4KvO/xu/HNNY7GT8VrXgv8PHAQ+Bfgf1G7NOSYxS4WNwM/ji/y+358Ad5BYAeeVf7ECT6mbXp6\nurxt27ZvneB9RE6VhV7cjy3rKERqu4JlWhhtR7Gbq4jIES1sHx1C6FvekZwZFjYHWarVm8hy09eo\nnMmW8+tTC/JERERERCJNjkVEREREIk2ORURERESiul2QJyKnl2qNRUSkHihzLCIiIiISqVuFiIiI\niEikzLGIiIiISKTJsYiIiIhIpMmxiIiIiEikybGIiIiISKTJsYiIiIhIpMmxiIiIiEikybGIiIiI\nSKTJsYiIiIhIpMmxiMhRMLMNZvZhM9trZrNm1m9m7zWz7uW4j8hiJ+NrK14Tlvhv/6kcv9Q3M/th\nM3ufmX3NzMbi19THj/Nep/T7qHbIExE5AjM7H7gXWAV8FngMeBHwcuBx4KUhhMHTdR+RxU7i12g/\n0AW8t0Z4IoTwhydrzHJuMbMHgSuACWA3cDHwiRDCjx/jfU7599GGE7lYROQc8QH8G/GbQwjvWzho\nZu8B3gq8E3jTabyPyGIn82trJIRw+0kfoZzr3opPip8CbgD+7Tjvc8q/jypzLCJyGDFL8RTQD5wf\nQqhUxdqBfYABq0IIk6f6PiKLncyvrZg5JoTQd4qGK4KZbcUnx8eUOT5d30dVcywicngvj693VH8j\nBgghjAP3AC3AS07TfUQWO9lfW0Uz+3Ez+3Uze4uZvdzM8idxvCLH67R8H9XkWETk8C6Kr08sEX8y\nvl54mu4jstjJ/tpaA3wM//X0e4EvA0+a2Q3HPUKRk+O0fB/V5FhE5PA64+voEvGF412n6T4ii53M\nr62/Bl6BT5BbgRcAfwb0AV8ysyuOf5giJ+y0fB/VgjwREREBIITwjkWHHgbeZGYTwC8DtwM/eLrH\nJXI6KXMsInJ4C5mIziXiC8dHTtN9RBY7HV9bH4qvLzuBe4icqNPyfVSTYxGRw3s8vi5Vw3ZBfF2q\nBu5k30dksdPxtXUwvraewD1ETtRp+T6qybGIyOEt9OK80cye8z0ztg56KTAFfP003UdksdPxtbWw\n+v+ZE7iHyIk6Ld9HNTkWETmMEMLTwB34gqSfXxR+B55J+9hCT00zazSzi2M/zuO+j8jROllfo2Z2\niZl9R2bYzPqA98c/Htd2vyLHYrm/j2oTEBGRI6ixXel24MV4z80ngOsWtiuNE4kdwLOLN1I4lvuI\nHIuT8TVqZrfji+6+CjwLjAPnAzcBTcAXgR8MIcydhrckdcbMbgZujn9cA7wS/03E1+KxQyGEX4nn\n9rGM30c1ORYROQpmthH4LeBVQC++E9OngXeEEIarzutjiW/qx3IfkWN1ol+jsY/xm4CrSK3cRoAH\n8b7HHwuaNMhxij983XaYU7Kvx+X+PqrJsYiIiIhIpJpjEREREZFIk2MRERERkUiTYxERERGRSJPj\nY2BmIf7Xt9xjEREREZGTT5NjEREREZFIk2MRERERkUiTYxERERGRSJNjEREREZFIk+MqZpYzs/9h\nZt8ys2kzO2hmnzOza4/i2pVm9i4z+7aZTZjZpJk9bGbvNLOeI1x7mZl92Mx2mNmMmY2Y2T1m9iYz\na6xxft/C4sD455eY2SfNbJ+Zlc3svcf/WRARERE5dzUs9wDOFGbWAHwS+IF4qIR/fr4feJWZvfYw\n116P7++9MAmeAyrA8+N/P2Fm3xtCeLzGtb8A/AnpB5UJoA24Lv73WjO7KYQwtcSzXwt8PI51FCgf\n7XsWERERkedS5jj5NXxiXAF+FegMIXQD5wH/D/hwrYvMbDPwOXxi/EHgAqAZ35P+BcAdwEbgn80s\nv+jam4H3AZPA/wRWhhDagRZ8v/Anga3AHx9m3H+JT8y3hBC64rXKHIuIiIgcBwshLPcYlp2ZtQL7\ngHbgHSGE2xfFi8A24NJ4aEsIoT/GPg68Hnh3COFtNe5dAP4DuBz4kRDCJ+PxPPA0sBl4VQjhX2tc\nez7wEFAANoUQ9sXjfcCOeNo9wMtCCJXje/ciIiIiskCZY3cjPjGepUaWNoQwC/zh4uNm1gL8CJ5t\nfk+tG4cQ5vByDYDvrQptxSfGD9eaGMdrnwa+jpdMbF1i7H+kibGIiIjIyaGaY3d1fH0whDC6xDlf\nqXHsGjyrG4Bvm9lS92+Orxurjl0XXy8ws/2HGVtnjWur3XeYa0VERETkGGhy7FbG172HOWdPjWNr\n46sBq4/iOS01ri0ex7XVDh7FtSIiIiJyFDQ5PjELZSmjcTHc8Vz72RDCzcc7gBCCulOIiIiInCSq\nOXYL2dd1hzmnVmwgvnaYWWeN+OEsXLvpGK8TERERkVNEk2O3Lb5eaWYdS5xzQ41j38T7IRveeu1Y\nLNQKX25m64/xWhERERE5BTQ5dncAY3j971sWB2M7tl9efDyEMA58Kv7xt8ysfakHmFmDmbVVHboT\n2AXkgT843ODMrPtIb0BERERETpwmx0AIYRL4/fjH28zsl8ysGbKewp9m6W4RtwJDwIXAvWb2qoUt\nn81dbGa/CjwOvLDqmfPAL+CdLl5nZp8xsysX4mZWiNtC/xGpp7GIiIiInELaBCRaYvvoCaArfvxa\nUpY42wQkXvtdwGdIdcnzeCa6HW/1tmBrCOE5LeHM7I3Ah6rOm47/deJZZQBCCFZ1TR9xwlx9XERE\nREROjDLHUQihBPwQ8GZ8V7oSUAa+ANwQQvjnw1z7H8DF+BbU95Im1VN4XfL/jvf4jl7JIYS/Bi7C\nt3x+JD6zAxgE7gJui3EREREROcWUORYRERERiZQ5FhERERGJNDkWEREREYk0ORYRERERiTQ5FhER\nERGJNDkWEREREYk0ORYRERERiTQ5FhERERGJNDkWEREREYk0ORYRERERiRqWewAiIvXIzHbgW8H3\nL/NQRETORn3AWAhhy+l+cN1Ojm+59RUBYM8Tw9mxRx/ZBUDJ5gHI56rfvn88Peax1s5iFmnp8I+n\nRj02emAsxZoL/kHJt+Eukbbjbij4x/OT/ufOte1ZrGd9CwAbNq/IjlWC33/n4z7mSqmSxSbGp/3Y\njAFgDZbF5mb8urnpOX9eJZ/FLOexVZt6AJiZn0rXTZcB2L/9YLqZiJwsHc3NzT2XXHJJz3IPRETk\nbLN9+3amp6eX5dl1OzkWkbOTmb0ZeBOwBWgC3hpCeO/yjuq49F9yySU9999//3KPQ0TkrHPNNdew\nbdu2/uV4dt1OjnfEjHFne3N27PKrLwHgW994CoCJsYkslm+NGd+Yaa2UU/Z1ZspjUxOzALR2FLLY\n/Kxnd/PleCw/n8VKFb/OKp6YnT0wk8UGY7Z3dCCNYXbOz+tdHe+VEsdcdMXz/F6hBMAzT+7PYpPD\nng3ON/hf54q1vVlsy8VrAGhq9Zvd/eXHslhDUMJYzixm9qPAnwAPAO8FZoGvL+ugRETknFK3k2MR\nOSt9/8JrCGHvso7kJHh4zyh9t35huYchIueQ/nfftNxDOOupW4WInEnWAdTDxFhERM5OdZs57i10\nAtCQqiNo7vYyhwsuXQ3AQ/elQu/5KV+cZvH86cm5LFYeD/FeXoYQGhuzWD5WY7R1+4WFYksWWyir\nKKz1MonKXCq5mJrx502OT2bHciW/x7CvG6RrfVrANz7tiwBX9LYB8LwXrMpis+f7or7GZv/rPH/z\n6jT26bjQMBwEYP261iz25LYBRM4EZnY7cFvVn7OVrSEEi3/+CvCjwO8A3wesAX4qhPCReM1a4O3A\nTfgkexT4GvDOEMJ3FP6aWSfwDuCHgRV4V4k/Bz4DPA18NIRwy0l9oyIicsar28mxiJxV7oqvtwCb\n8UnrYj14/fEE8M94Vf4AgJltAe7GJ8VfBv4O2Aj8CHCTmf1QCOHzCzcys6Z43tV4ffMngE7gN4Dv\nPqnvTEREzip1Ozk+OLUHgNx4yuQW2j0Z1bbKM7lXXHteFjt00DOrB/f74raJkZRVzjf6pynEdXKz\nkykD3LvGM7kdKzxYHk+L3BpiS7WZGc9Cl6yUxcqxnZyRstDFgt9jbtazymMHR7LY3ITHhnb7QsO1\nW1ZmsRWrfNHh2LDff3w8tZp79Js7/T3E9965pjOLda5JWWuR5RRCuAu4y8y2AptDCLfXOO0FwMeA\nnwwhlBbFPoRPjN8eQnjnwkEz+wDwVeCjZrY5hLCwAvZX8Ynx3wM/FkII8fx3AtuOZexmtlQ7iouP\n5T4iInJmUM2xiJwt5oBfWTwxNrMNwI3ATuD3q2MhhHvxLHIP8Jqq0BvwzPPbFibG8fxdeJcMERE5\nR9Vt5njfAc+Krlmb2q5Nl/xngXysBV6xPrV569ngdbrtO8cBeOL+g+m6idiCbeH65nIWG9zviajW\nbq/7bepIRc7zo/5xyHsbtda2lMWeqHh2d346ZaEbOj3r3LzGx9Xend7P/KiP+cBOz2jvmk31wpVZ\nr00e3O1Z78fvT/XSc/NxE5DzvL1bQ1v6fFx41TpEziL9IYQDNY5fFV+/FkKYrxH/MvDj8by/MbMO\n4HxgVwihv8b5dx/LoEII19Q6HjPKVx/LvUREZPkpcywiZ4v9SxxfqBXat0R84XhXfO2Ir0utSNVK\nVRGRc5gmxyJytghLHB+Nr2uWiK9ddN5CUf7qGuce7riIiJwD6rasYnSv72aXK6dFbeT87Za6vZ3Z\nqgtT3cJMJe6Mh5dCVNI6OQpt/odCg792rkg70DUWvGSiJcRkVNoED2JpZGjwUohyJZU7tHQW/VhT\n+ivIN3m5RiX+ZniwqtNr72ovnejZGBfWdaYSjVwcV8cG/1ln/4Mp8VVs8mPNzX7Oob1DWay7pwmR\nOvBAfL3ezBpqLNZ7eXzdBhBCGDOzZ4A+M+urUVpx/cka2GXrO7lfDflFRM4qyhyLyFkthLAb+L9A\nH/CL1TEzezHwY8Aw8Omq0N/g3//eZWZWdf7GxfcQEZFzS91mjhvw7HBbLmWHi22euW1r97c9OFy1\n6C4mdUdGPcu7YkO6rrExH18901q1PwH5+OPFzOx0/HOKWbNnlXNzfmz4QGqxFpPYFJuK2bGmWf/4\n4AH/7W/v6tR27cKLNgMwOuKZ8O3f3pnFDuz2RYTdaz2b3NKSFgX2rvcyyz3P+nutzKSfh+YnqtPc\nIme1NwH3AH9gZjcC3yT1Oa4AbwwhjFed//vAzfimIheZ2R147fJ/wVu/3RyvExGRc4wyxyJy1gsh\nPAO8EO93fBHwK/guev8HeGkI4bOLzp/Gyy3eh9cqvzX++XeBd8XTxhARkXNO/WaOC/6b0mJzKj9s\n7vZE0OiUJ5BmB1KGdXTcz5uPWzznqtb+zDX4vRqKnl4O5XTPUvB7Vkr+WmhMn9IQt4he+K1tW0/a\nDrqlybO8YSp1njrvu3w90brRHgA6O1NWeXSnt4ybKnmGuqM7xeZmF96Hj6upI8VmS/F9Tfv7mZtK\n2eJiSzpP5EwQQti6xHGrdXzROXuAnz2GZ40Ab47/Zczsp+OH24/2XiIiUj+UORaRc5KZfUejbzPb\nBPwm/pPm5077oEREZNnVbeZYROQIPmVmjcD9wAi+oO/7gRZ857y9h7lWRETqVN1Ojjs3+m9hD4xP\nZseK815iMDvjJRBda1I7tE3rfPHb0G4vOxgdSeWGhVZvg2bluENeS9pZry3nO86NHfBSjflSKsfo\n6PRFgeV5L8e48LKVaSyNHlvRldrC7TqwB4CJSR9zoZDu9fRjvv9B0xp/XmtVWUX7rP81juzxhXyV\nqh38Gmc8tlD2UZpKa4z2PVXV5k7k3PMx4CeAH8IX400A/w68P4Twz8s5MBERWT51OzkWETmcEMIH\ngA8s9zhEROTMUreT48kxz9Y2FlOGdXrMM8CtrZ59nRlJGdbOFv9UFLt8cVtLSFllcp5tnZv2xXOl\nSsq+Ws7v2bPCz29b0ZbF5spTAKzc7G3hSmkPEPLTnh2ebilkxzZs8o1EJof8ul1PHkrvJ2afDz3q\nsUJTWp80NekZ8WKT36tjddrcoyHv53UW/FhzY/p8WLabroiIiIiAFuSJiIiIiGTqNnNcjLXAhap3\n2Bs3yWgqePZ0dDDVI8+Pe83w2KBvOz0zm7LKlVnPzDY0+81ypdR+rbgyfQwQmlJ6uFL2DPOuZ4cB\nsHLKOOdnfAwDQ2lfgqGDXu+8bqPXI1fm0uCLsc65rcczwWND6Tm9q/z8Ned53XRIHeqYn/fxDezz\nMVRyqQ1dQyG9RxERERFR5lhEREREJKPJsYiIiIhIVLdlFS3tPu+v5FPpwEzFSxhyOd9tjqpWaRNT\n/vHUoJc2TE2ncomWTi/RKM34sdmqXe3yBX9O+0pf8FaaTbGmBv/0FuKiwKnJqSxWmfUSi4GB1DIu\n1+DnDR7089ZsSYv7Oju8dGJyyt9PQ0f6uWYklodMjfl1kwNpF7zhuLhvocSjPJlKO4rd6f2LiIiI\niDLHIiIiIiKZus0cT4/EBWtNKTuaK/rPAk9/07O1XT2p5Vk5tmcL8fTmjtTyLOT8uqmD035gNi1q\nm8h7rDTjC+XmZlImuNjqscYWv9eqtVVt3mb8QR293dmx2bJngOdLnh0+OJQyzbuf9bZuIwM+hu61\nqQ3b8JBv5pEv+Uq82arxdcSFfBY/D7PFlDmumBbkiYiIiFRT5lhEREREJKrbzHFDk9fozo7PZsd6\n2nsAsHWeMR4aStsnz1c8s5oreH1xY9pjg9ly3FAk1h4P757OYpVh//nCZj1rOz6ZntfU4RuEzE7H\n80spU7tmSzsAYSJlcstj/tcxedDPGytNZLHVG1cAMMGAj31HGntrq78fi4nwlp60vXU++BvpWuPH\n9u8azGITh9KzRURERESZYxE5w5hZv5n1L/c4RETk3KTJsYiIiIhIVLdlFS293lKtId+aHZud9hZn\nXav9Z4JCV0cWK5d8EdvIkP+5WEzbzHW2+nnDse1aQz592rq6fGFcc4vfc3IytVFj1ks1ela0x4ek\nxYHPfMsfVKkqtWhf7Tv4dfT5Ar62lrTobnLCyzW2XLnab1W1e97ITh/X/t2+aK96h7xCu5dVTIz7\n+xs9lMbXsym9fxE5+R7eM0rfrV84rc/sf/dNp/V5IiL1RpljEREREZGobjPHxaJnYZs3p7dYqHhK\ndT5miVua57LYgR3lGPNj+ea0Im92PLZUm/IFbMWGdM/mXn/t3eDPyxXSGBqaPAO8+vyVAJTm0s8i\nDQvr6YrpXtYUNyCZ8TH0P7o7i4WYdR5p9uc8/0Wbs9i65/lCw0un+gC4718fy2JrtnhsfZ8PdHBo\nKIvlC6ldncjpZGYG/Dzws8D5wCDwaeA3DnPN64D/DlwFNAE7gE8AfxBCmK1x/sXArcArgNXAMHAn\n8I4QwuOLzv0I8IY4lpuAnwYuAP49hLD1+N+piIicbep2ciwiZ7T3Am8G9gF/DswDPwC8GCgAc9Un\nm9mHgTcCu4FPASPAS4DfBl5hZt8bQihVnf8q4J+BRuBzwFPABuA1wE1m9vIQwrYa4/oT4LuBLwBf\nBI7YDNzM7l8idPGRrhURkTNP3U6On/zmMABNrSkDvGGTZ3BbO/xYx+qU5h0b9MTT1JTHJofH082m\n/dNUim3XznvB+iyUa/TYgUF/XmhNn9LO9X6vgUMHAejtbMlivXEsL15zUXZsrsHro7+26x6/d3lT\nFnvs8WcBWH2+Z3tHR4ez2NChuGV1o8euuHFtFpsc9Wx0/5N7AFi5NtUZz5TTVtcip4uZXYdPjJ8G\nXhRCGIrHfwP4N2At8GzV+bfgE+NPA68PIUxXxW4HbsOz0H8Sj3UDfwdMAS8LITxadf5lwNeBvwSu\nrjG8q4GrQgg7Ts67FRGRs41qjkXkdHtjfH3nwsQYIIQwA7ytxvlvAUrAT1ZPjKPfxksyXl917L8C\nXcBt1RPj+IyHgb8ArjKzS2s86/ePdWIcQrim1n/AY0e8WEREzjh1mzkWkTPWQsb2KzVid1NVymBm\nLcAVwCHgF71U+TvMApdU/fna+HpFzCwvdmF8vQR4dFHsG4cbuIiI1L+6nRyv3ewlBiGfdoGbD97y\nbC54ecP43tTz7IorfYHb1KwnpoYOplLDqYNe/rjlUm+tNl/1afvKpx8A4JWX9AFw+RV9WWzftD9v\n+/A+v+fusSz2bJeXO7zsgsuyY3fe9+8ADOY8mda9MZVh3Ph8/7d/1zO+km/nkwPpzcYOcSs3dwLQ\n0JR2yBvZ4eUXOx/zZx8aSuUYs1PaIU+WRWd8HVgcCCGUzOxQ1aFuwICVePnE0YjLZPnpI5zXVuPY\n/qN8hoiI1CmVVYjI6TYaX1cvDphZA7CixrkPhBDscP/VuOaKI1zz0RpjCzWOiYjIOaRuM8eFuCnH\n9GTKDpdycaOOGc8OF4qNWeyb9/r6n9a2JgBWruzJYhde3w3AyJi3dHv4q3uy2A9d90IAfuCFnnmu\nWLrnjWs8lr/WP837Bw9ksX0D3qZtLDRlxyZz/m/6yP5JAEb3pUWBA72eDe7o8PNXdKXrBnb5++l/\n0Bf+DTyTMtTnX+5JtNE1PneYGk0lm7m85gGyLLbhpRU3AM8sil0PZP/ThhAmzOwR4Plm1lNdo3wY\nXwd+CO868dDJGfLxuWx9J/drUw4RkbOKMscicrp9JL7+hpllP4WaWRPwrhrnvwdv7/ZhM+taHDSz\nbjOr7jzx13irt9vM7EU1zs+Z2dbjH76IiNSzus0ci8iZKYRwj5m9D/gfwMNm9klSn+NhvPdx9fkf\nNrNrgJ8DnjazfwV2Aj3AFuBl+IT4TfH8QTP7Ybz129fN7E7gEbxkYiO+YK8X30hERETkOep2crz3\nKS+BaO9Ku8A1NXtf49K0lxOMjUxmsbmylx20zPsitfnZiSz28MNerlCe90T70NBUFhvf4sd2DvhC\nt/MvuyaLhbUb/fxhL3OYzqdyh0KXx/bPplKL6flBACYnfLHe/HhaMDc64M/cE5P9G9enso+ceY/m\nuZm4y99cGvv2//DFhL3r/PPQ3JEW+ZVmtCBPls1bgCfw/sQ/Q9oh79eBby0+OYTw82b2JXwC/D14\nq7YhfJL8B8DHF51/p5ldDvwK8Eq8xGIO2At8Gd9IRERE5DvU7eRYRM5cIYQAvD/+t1jfEtd8Hvj8\nMTyjH/iFozz3FuCWo723iIjUr7qdHE/EneE6e9MueJWKZ1Znpz1jOnowZU57utsBaCh4VnlmfiaL\nDR/yXWkP7PHFbJOjKTM7OuIfh/N94f2ekXTdfItnpmfGfGHd3Fy2uy17Zz1T/Q//eGdx6JI+AAAg\nAElEQVQa80zMGMfbT8+kHXSbmnyNkgV/D4VKygBffa0vGDw46NnlA3tTu7b5Ob9HU97fX4hZZoDG\nnrR4UERERES0IE9EREREJFO3mePztnhrtd17U01vY9M8AL1rPGO6ekWqR7aKZ1RnYz1yaT5lVUcH\n/Lp8xT9dL7k8bcb13S+4AIC7n/b9DMaeTHXF3/1Czw6P7N8JwPqetOfAvY/6sf17U2eqhgb/WaVS\n8gxzqZQyzWs3rAVg8yavNS4W072Ghr1WOd/s179w60VZbPyAp6G33ecds0qzKVve1qb1SCIiIiLV\nlDkWEREREYk0ORYRERERieq2rGI6tilra23PjnV1dgBw3roNAMzNpx3oxia9NGF4yH9emEvr6uju\nbAWgFDeU27S2O4t94du+sx7mJRSdVaUK//R//h8AuVjJsPWyLVnsnnvvBaAxn/4KyuX4WvEx5POp\n7GNXv49vbNgXBW7clMYwOe7lF4VWv8E3njyUxTas8zKMzg5/zrNPpffckq/bv34RERGR46LMsYiI\niIhIVLepw8lpXxjX0pJanpVjBnd8yjOskxMhizW3+IK3llZvfTYzndq1NbR4NrhQ8EzuyJRlsfGJ\nmMld1QlAd0/KHD/xtI/heX2bANg/klqstbYuZKPTc5oa/f4NRX8dG02L9RayyiMxczw5ltq8NeS8\nzVtj48L7S2nvkT3eTq4SM9sV0oK8gcG0eFBERERElDkWEREREcnUbeY4BM+Ujo2lGttCo7dryzV4\npnVmKm2I0VH2zG+p4pnVPXv3ZLFis2eDN673LZ9bOzqz2OqcbzIyEfy6NSFllc+/8GI/v+DHxmen\ns9iKlb5pSKUx1RXnQ/xZxfy1u2tFFpuNG4SEmAEuVVKbt8kpv28uxI1CGlL2eja2g7N4XWtbRxYL\nKXEuIiIiIihzLCIiIiKS0eRYRERERCSq27KKnPlbK1fVDszOehnFZCy1aG1rzWIh+Iq3hV3pNqzf\nnMVKMbZ69XoAKvn0nKYWL6toLPsuejv2H8xi1zz/0nhsxJ9RSJ/uyy9fB8D+4ZE0hvk4hrlYCpFL\nJRqxKoJK2WNzM6lEYzS+n7kYa8ynn3laW3wnvVxcrdfYkMaQbyggIiIiIokyxyJyxjCzPjMLZvaR\nozz/lnj+LSdxDFvjPW8/WfcUEZGzR91mji3O+1tb29KxuBAvxJ5uU1Mp+1qoeNa2seCL2ZqLzVls\ndMxbnj2zox+Ant6uLNbc5Ofl4oYdnT3rs9iuAd+4I2eetS1UtVFb0e33mJwuZ8faVvtCv5lpX3zX\n2pY2MGlrWRhXzFRbutdczDjPxl1KGvMp41xs9wV4w6OeXS4yn8WamlPmXERERETqeHIsIueETwNf\nB/Yt90BERKQ+1O3kuL2rF4BAyszm4nbJlYUdNUj1yIW48Ubs5MbgUNqAo7vbs6+rV3n7tbHx1B6u\nXPLNOBZarTX1rsxi+YJnfluLnrFuLqQtn8dnvD64uytloedjfXSh4Oc/8eTj6Q3FVnG9PX7+xnVr\n0ntt94xza7NnsYeHUt3zgZFdAJTm/XnDB1OLutVr1iFyNgshjAKjyz2OpTy8Z5S+W79wTNf0v/um\nUzQaERE5Gqo5FpEzkpldbGafMbMhM5s0s7vN7MZF59SsOTaz/vhfh5m9J348X11HbGarzeyvzGzA\nzKbN7EEze8PpeXciInKmqtvMsYic1bYA9wHfBv4MWAu8FviSmf1YCOEfjuIeBeDLQA9wBzAG7AAw\nsxXAvcB5wN3xv7XAh+K5IiJyjqrbyXFLbGE2PZ1KIBZapDU1+eK2qem0IA/zUotiLK9oakkL+Wbn\nPLZz124A5mbnslhzLGUoNvnitgMHDmSx+VjKsKLXd7rr6ko76xWbWvyxVYv0nt3pJQ/NRf9raW1J\nC+am41if7t8JwOTkZBbbsGEDAA2x9VtnjVKNYlyk17optajr6OxB5Az1MuAPQwi/unDAzN6PT5g/\nZGZfCiGMHeEea4FHgRtCCJOLYr+LT4zfG0J4a41nHDUzu3+J0MXHch8RETkzqKxCRM5Eo8BvVR8I\nIXwT+ATQBfzgUd7nlxdPjM2sEXg9MA7cvsQzRETkHFW3meOhQV+UNluVHS4UPWNcyHtrtfa2jixm\nceOMtlbPBDc1pzZqHR3+cWWhBdz4RBabn/eFeJ0rfXHb9ESKTY77Bh/jcQFfY7Exiy3ki7uqsrzd\n3T3xfE+I9W3aWPUcz0JvDp75bS6kDTxG4/nEDm4NDWkusPBeG4r+HqbiRigAh4aPlHgTWTbbQgjj\nNY7fBbwBuAr46BHuMQM8VOP4xUAL8LW4oG+pZxyVEMI1tY7HjPLVR3sfERE5MyhzLCJnooElju+P\nr51LxKsdCKFqi8xk4dojPUNERM5BdZs57l3pdb7lUvq3sVx+7hbRs3NpQ4wVKzxru36tt2srNrVV\nXRe3c47t1DqrNufA4sYbjV6r3LVxQwrNe5Z2aMTbwk3HLDOANfi9nn7isezY6lWrAGha6f92T02m\nraWnxob9fa3wdnBb1q7IYo2bvW3djl17AehoTX+tMzNTADz2yBNxnCnjvHbdakTOUEt9cS70MDya\n9m21JsbV1x7pGSIicg5S5lhEzkRXm1l7jeNb4+sDJ3Dvx4Ap4Eozq5WB3lrjmIiInCPqNnMsIme1\nTuB/AdXdKl6IL6QbxXfGOy4hhHkz+wTw0/iCvOpuFQvPOCkuW9/J/drUQ0TkrFK3k2MzT4qvXJkW\nvK1ZsxaAZ/r7ARg6mHaSGx32Eoa2Fl+QN7MvlSP2rvCyhcZG/3Tl8ynh3tHqLdnWr/HfxIZKavPW\nlPdnX/H8PgAqls9iQ4O+2+2u9lTmMDziYzgw5Iv6CsXmLLZhnY+hvd2PPfpQ6jb1vAsuBGBVh/8W\ned/eZ7JYruDlHi++6gIApqdTaUd3R1qQKHKG+Srw38zsxcA9pD7HOeBnjqKN25H8OvAK4BfjhHih\nz/FrgS8C/98J3l9ERM5SdTs5FpGz2g7gTcC742sR2Ab8VgjhX0/05iGEQ2b2Urzf8auBFwKPAz8L\n9HNyJsd927dv55prajazEBGRw9i+fTtA33I822ov5hYRkRNhZrNAHvjWco9FZAkLG9U8dtizRJbH\nFUA5hFA83Q9W5lhE5NR4GJbugyyy3BZ2d9TXqJyJDrP76CmnbhUiIiIiIpEmxyIiIiIikSbHIiIi\nIiKRJsciIiIiIpEmxyIiIiIikVq5iYiIiIhEyhyLiIiIiESaHIuIiIiIRJoci8j/3969R9d5lXce\n/z7S0d26Wr7FdqzEudghwU1CCSU0l1JuTUsDLdOWMkNgDavpMIWm0LVa6AwOHVpWy2JCYVi005LQ\nlGE605KyGKCli5ACoWGmDklIcO6R7xdZtu7Skc45e/549nn3qZAUyZIl+ej3Wcvrld79vvvdxz7r\n+NGjZ+8tIiIikYJjEREREZFIwbGIiIiISKTgWEREREQkUnAsIiIiIhIpOBYRERERiRQci4jMg5lt\nM7PPmtlRM8ubWa+Z3WVmnSvRj8h0S/HeiveEWf4cP5fjl+pmZr9oZp80s2+b2VB8T/3VWfZ1Tj9H\ntUOeiMiLMLOdwHeBjcCXgCeBlwM3A08B14cQ+perH5HplvA92gt0AHfN0DwSQvjYUo1Z1hYzewTY\nA4wAh4FdwOdDCG9bYD/n/HM0t5ibRUTWiE/jH8TvCSF8snzSzD4O3AF8BLh9GfsRmW4p31sDIYS9\nSz5CWevuwIPiZ4EbgW+eZT/n/HNUmWMRkTnELMWzQC+wM4RQqmhrBY4BBmwMIYye635EplvK91bM\nHBNC6DlHwxXBzG7Cg+MFZY6X63NUNcciInO7OR6/XvlBDBBCGAYeBJqBVyxTPyLTLfV7q8HM3mZm\nHzCz95rZzWZWu4TjFTlby/I5quBYRGRul8fj07O0PxOPly1TPyLTLfV7azNwL/7r6buA+4FnzOzG\nsx6hyNJYls9RBcciInNrj8fBWdrL5zuWqR+R6ZbyvXU38Go8QG4BrgL+FOgBvmZme85+mCKLtiyf\no5qQJyIiIgCEEO6cdupx4HYzGwHeB+wF3rTc4xJZTsoci4jMrZyJaJ+lvXx+YJn6EZluOd5bn4nH\nGxbRh8hiLcvnqIJjEZG5PRWPs9WwXRqPs9XALXU/ItMtx3urLx5bFtGHyGIty+eogmMRkbmV1+J8\nrZn9q8/MuHTQ9cAY8NAy9SMy3XK8t8qz/59fRB8ii7Usn6MKjkVE5hBCeA74Oj4h6d3Tmu/EM2n3\nltfUNLM6M9sV1+M8635E5mup3qNmttvMfiQzbGY9wKfit2e13a/IQqz056g2AREReREzbFe6H7gO\nX3PzaeCV5e1KYyDxAnBg+kYKC+lHZCGW4j1qZnvxSXffAg4Aw8BO4BagEfgq8KYQwuQyvCSpMmZ2\nK3Br/HYz8Dr8NxHfjudOhRDeH6/tYQU/RxUci4jMg5ltBz4MvB5Yj+/EdB9wZwjhTMV1Pczyob6Q\nfkQWarHv0biO8e3A1aSl3AaAR/B1j+8NChrkLMUfvj40xyXZ+3GlP0cVHIuIiIiIRKo5FhERERGJ\nFByLiIiIiEQKjkVEREREIm0fvUqZ2W34UiV/F0J4ZGVHIyIiIrI2KDhevW4DbgR68ZnCIiIiInKO\nqaxCRERERCRScCwiIiIiEik4Pgtxi83PmNnTZjZmZgNm9gMz+xMzu7biugYze4uZ/aWZPWpmp8xs\nwswOmNnnK6+tuOc2Mwt4SQXA3WYWKv70LtPLFBEREVlztAnIApnZbwD/FaiNp0aBKaAjfv9PIYSb\n4rU/C3w5ng/4TkNN+DacAAXgnSGEeyv6/yXgE0AXUAcMAeMVQzgUQvjxpX1VIiIiIgLKHC+Imb0F\n+BM8MP4b4IoQwroQQie+feHbgH0Vt4zE628A1oUQukIITcAO4C58QuSfmdmF5RtCCH8dQtiM7xsO\n8N4QwuaKPwqMRURERM4RZY7nyczq8H2+twJfCCG8dQn6/AvgncDeEMKd09oewEsr3hFCuGexzxIR\nERGRF6fM8fy9Gg+Mi8BvL1Gf5ZKL65eoPxERERFZBK1zPH+viMdHQwhH5nuTmXUB7wbeAFwOtJPq\nlcsuWJIRioiIiMiiKDiev03xeHC+N5jZFcD9FfcCDOMT7AJQD3QCLUs0RhERERFZBJVVnFt344Hx\nw8DrgdYQQlsIYVOcdPeWeJ2t1ABFREREJFHmeP5OxOOO+VwcV6B4OV6j/MZZSjE2zXBORERERFaI\nMsfz91A8vtTMts7j+m3x2DdHjfJPz3F/KR6VVRYRERFZJgqO5+8bwBF8Mt0fz+P6wXjcZGYbpzea\n2VXAXMvBDcVjxxzXiIiIiMgSUnA8TyGEKeB98dtfMbP/ZWa7yu1m1mVm7zKzP4mn9gOH8czvX5vZ\nJfG6OjN7M/CP+CYhs3kiHt9sZu1L+VpEREREZGbaBGSBzOy38Mxx+QeLEXwb6Jm2j34TvpNe+dph\noAFfpeIg8EHgXuBACKFn2nN2AY/GawvASXyb6sMhhFedg5cmIiIisuYpc7xAIYSPA1fjK1H0AnX4\nsmyPAZ8A7qi49j7gp/As8XC89gDwsdjH4Tme8yTwGuDv8RKNzfhkwG2z3SMiIiIii6PMsYiIiIhI\npMyxiIiIiEik4FhEREREJFJwLCIiIiISKTgWEREREYkUHIuIiIiIRAqORUREREQiBcciIiIiIpGC\nYxERERGRSMGxiIiIiEik4FhEREREJMqt9ABERKqRmb0AtAG9KzwUEZHzUQ8wFEK4aLkfXLXB8et+\n7lUBoKm5ITvX1dUBQHt7GwCFwmTWdnrgDADDE8MAFHOFrK1UX/QvcgbA1ORU1pYfm/C+RvxcIV9x\nX7EUn+PHENL46usbAVjX1padq62t9fsmvK/t3Zuztm3dmwBorW/2oYSU9C9M+RiKkz5OK1nWZgUf\nz8TYkPdzQXfW9hMv/3EArrr119INIrJU2pqamrp2797dtdIDERE53+zfv5/x8fEVeXbVBsflQLGU\nK2XnSlMxyC15wNjUWJe1tazzrydrPegcL6X7pqY8iC5M+Ln8WAqOx4c8MJ0c82uKUxXBceyiEIPk\nulx91paLgfDwYPqHHxsdAaC11q/b2HNF1tZRvw6AUydPAVBTW1ERk/Ooe3jI758anUh/D+N5AKzg\nx3XN6TU/+eRTAFyFyOphZu8BbgcuAhqBO0IId63sqM5K7+7du7v27du30uMQETnvXHvttTz88MO9\nK/Hsqg2OReT8Y2a/DHwC+D5wF5AHHlrRQYmIyJqi4FhEVpOfLR9DCEdXdCRL4PEjg/T8zldWehgi\nVaX3o7es9BCkylVtcJwPXubQ3dmZnWvp8vrjYs7LDnL16eW3x7a6Sa8BHhpNZbijE17CUGte7jCU\nH8vaxke9LKIYy5etNpVO1NV56UQIXmpRKqY+iwX/uqmxKTvX3eb1xE2xHqMx3g/Q1OZjrZn0tlJd\nKvuob/BnluJ9o+Opljof65HX5bzG+fIrX5a17brsYkRWmQsAqiEwFhGR85OWchORFWdme80sADfH\n70P5T8X3D5jZZjP7czM7YmZFM7utoo8tZvbfzKzXzCbNrM/Mvmhm187yzHYzu8vMDpvZhJk9aWa/\nZWYXx+fdswwvXUREVpmqzRxfsedyANZf0JGdq633iWsTI75yQ0tFlrel3ieqhZJPfMu3rc/a8nH+\nXQh+zYma/qxt5JRnjsenfLWLQqmYBlHw502Vs73FlO2tq/UJcju2XZCd27rFV5IY6e8DYNJGsrbc\nOs+Ab2jwie/9Q4NZ2/Cwv57xMV9pY2Is3Tc67FnurTv9OZe+ZE/WduH2tBqGyAp7IB5vA3YAd85w\nTRdefzwCfBEoAScAzOwi4Dt45vl+4AvAduAtwC1m9gshhP9T7sjMGuN11+D1zZ8H2oEPAj+5kIGb\n2Wwz7nYtpB8REVkdqjY4FpHzRwjhAeABM7sJ2BFC2DvDZVcB9wLvDOVapeQzeGD8eyGEj5RPmtmn\ngW8BnzOzHSGE8k+Ov40Hxv8TeGsIoZyh/gjw8FK9LhEROf9UbXB8wXbPGJfqU7Z2bML/XyyWPBVc\n39CStbU0e8Y4xLLg4nBaYs1qc7HNq1A6NqT/ly+p6QGg4YjXLPf1DaRBFP2+uhofw/joaNZUF5dy\na2lNdcUtXZ51rm/yDHVjSxp7bYu3rTN/zkDF0n/DQ54xromDX9/WmrVt79gIwE/dcL2PM5fqnh97\n9F8AuO7iH0PkPDAJvH96YGxm24DXAgeBP6psCyF818y+ALwNeDPwl7Hp7Xjm+XfLgXG8/pCZ3QX8\nl/kOKoQwW9nGPjwAFxGR84hqjkXkfNEbQjg5w/mr4/HbIYSpGdrvr7zOzNqAncCREELvDNd/Z7ED\nFRGR85eCYxE5Xxyf5Xx7PB6bpb18vjwBobwt5YlZrp/tvIiIrAFVW1aRL3h5w2TFds6Tcdvnuviy\nB4dTCcTopJc8WFwWbaRil7nJyYpJdkBtqoSgPlYwtLX7kmzrO9NEvrpaX5ptbMT76u/rS22xvKGj\nI20f3dDs17e1eYnHuvq09XX/gI/vzKBPsBscTCUaU3EJt61bvIRic2saQ1eLxw0XXewT8koh3ZcP\naUk6kfNAmOV8eXbqbDNMt0y7bigeN81y/WznRURkDaja4FhE1ozvx+OrzCw3w2S9m+PxYYAQwpCZ\nPQ/0mFnPDKUVr1qqgV25tZ192rBAROS8UrXB8VT873FyIk1qo+hVJAWLjbUVbTV+rhA8y1u/LmVt\nLe+p4pGYrS1UVjXGuTwXbvesbXfXxqypNOXZ4eF439bN69Ljavy+1vbmivH5eMobg3S2dmVNgwWf\ndHdixCcVTo6lJFpzo/dR3+AT+To2po1PLty+HYCJRn99J0fTb55Hc2mzEJHzVQjhsJn9I/Aa4DeB\nj5XbzOw64K3AGeC+itv+EtgL/KGZVa5WsT32ISIia1TVBscisqbcDjwI/LGZvRb4F9I6xyXgHSGE\n4Yrr/wi4Ffhl4HIz+zpeu/xv8KXfbo33iYjIGqMJeSJy3gshPA+8DF/v+HLg/cAbgL8Hrg8hfGna\n9eN4ucUn8VrlO+L3fwD8YbxsCBERWXOqNnNcLoGor3iJzXVeflDX4D8TtKxPbQ1tfm44roU8MZJq\nJ0olL2FoiD9LNNen9ZHbWv3r1g4vw8jVpj7HRr1soSHn19RuTOsP15hP8iuSFiyeKvqueRbnHY2N\np4mAFifwNbXEtslUVtnY5GUY1ujPHq9NYz886itfTQ57EmxgOO2ed/SYTxB84xsRWRVCCDfNct5m\nOj/tmiPAry/gWQPAe+KfjJm9K365f759iYhI9VDmWETWJDO7YIZzFwL/CSgAX172QYmIyIqr2szx\n1IRnjmtDeomh6Mu01TX48mYN9XVZW2OTZ2RHJ7wsMT9WkdGNK551NPkyqZdddFnW1r7OJ9kNj54B\noP/MmaxtNC67NjEZl4WrTZPomlrjs2tSlneq6Nng8XHP8p4eyae2Kb93YNQzv1OWssoN9f4zzrh5\nX8+dPJBe8ym/r7PbJwpO5lMZ5cnT/YisYX9rZnXAPmAA6AF+FmjGd847uoJjExGRFVK1wbGIyIu4\nF/i3wC/gk/FGgO8BnwohfHElByYiIiunaoPjugavwy1OpUzpwIRncPM5f9mFoVRV0lzrX0/Epd9G\nx1NGN4fXEzfFLLFV7AJSKHoGt6Xes9GF5tTn+IS3DcU65uHxNFk+V/IxBEullGOjnikeHfGMc30u\nLSdXa3798ID3VVNxn9V6FroYd85t60hLxrV1+iYjwzHjfOJ4yhafPqP5RrJ2hRA+DXx6pcchIiKr\ni2qORUREREQiBcciIiIiIlHVllUMjMTd7App4lpN8CXVxi3+TDBasexayUsSzgx56UMxzYWjscZL\nGIZin8++8FzWlovlDc1Ncbm2XCq5yDX51221XtqQyzdmbcPj/oD+ijKHxlqfMNhW5zvcHTtxPGsr\nxZ346hq81KKyrKJcClJf588LxTTRcCq+jr6TXkJx4vhA1namv+JFioiIiIgyxyIiIiIiZVWbOS4U\nPNMaLMX/UyVfKm1q3CenrbeurK21xZdpO3bCM6uDZ9JmGXXBl3UbqvVzHfVpolxbnWdp+/KHAJis\n2HG2sc2zyZMxeT0xXtGW8w1Brth0aXZu20YfT4ibeXzznx/M2kYmfQylmJguWFoWbiI+IAz6piP9\nAynjXIjLw42O+/2T+bR5SLGYJh2KiIiIiDLHIiIiIiKZqs0cW8FTrGYpW5szz5pajWddJ0bSRh99\nec+6lsbj1tJ17VlbPi7XNlnyY+34ZNb20i7fZGv9Vq8r/n7v81lb77GYfW7xtu6Gjqzt0i0XAlCX\nH8vOhbjc2uERX3Lu5HDaUKS85Fu51rhhXVPW1r1lPQDHj/p20BMV206D/z0MnYmvtZTG3thQh4iI\niIgkyhyLiIiIiEQKjkVEREREoqotqxjNx9KCiolr5a9ravxngqEjp7OmUsFLLkLRyzCKFZUJFpdP\ny+X8vtH6tFzbRM6XX+s7eQqAjvrmdN+o93+g9xkA6mrqs7bh548A0LOpOzvXvsXLLp4+8IKPr6Ls\no6XF++1c75P8Nm3bkLWdPuXP6e/ziXjtLZ1ZW33czS80xmXkQppMODmRSixEVgMz6wFeAD4XQrht\nHtffBtwNvCOEcM8SjeEm4JvAnSGEvUvRp4iInD+UORYRERERiao2czw+UZ7oljbLKBVjFrm8gUZl\nUjmesjjpLhRSY23w7GsxZownLKWVQ6NnYjuaPDN72eYtWVvLJp+s17T/WQAuvPCirG1ju2d3n3vi\nB9m5o0f6YqeeYW5qSJMCQ/Bl16zWM9z1jWlJtvZ2zypf89IrAGi0NNGuFDcIGRzyLHTfQFqirr6m\nBZHz3H3AQ8CxlR6IiIhUh6oNjkWk+oUQBoHBlR7HbB4/MkjP73xlpYexbHo/estKD0FEZNFUViEi\nq5KZ7TKzvzOz02Y2ambfMbPXTrvmNjMLsfa48nxv/NNmZh+PX0+Z2d6KazaZ2V+Y2QkzGzezR8zs\n7cvz6kREZLWq2szx5s518asU/w8NjwKQjzvKFdMSyFj8mwg13lZfk+5rznljLu62t61zfda2Y4uX\nUWxq9nWH+weGsrYnfrgfgP37fUJed/fmrG3LlS8FYGxoIDv31PNefjE6Vp6Ilyb+lYo+ea6mxgfd\n13c4a2vM+c56Hc1eJtGQKi4YjusojwzE55RSyUVtfZogKLLKXAT8M/AD4E+BLcAvAV8zs7eGEP56\nHn3UA/cDXcDXgSF8sh9m1g18F7gY+E78swX4TLxWRETWqKoNjkXkvHYD8LEQwm+XT5jZp/CA+TNm\n9rUQwtCsd7stwA+BG0MIo9Pa/gAPjO8KIdwxwzPmzcz2zdK0ayH9iIjI6lC1wfFPv+LHATh0/Hh2\nbt8TP/QvYsLUatPLL8X5dzWTnpntXNeateXG/Vx98Fl7AwdTtvfux+8DYM+unQBceeUVWdvW7RcD\ncMEO/z+yJqRM8OFnPfO7a8812bmBek/5Pva9B+OgUva6nOWtrfEMdQ1TWZuZZ5VDwc+Nj6XJhIXy\na233vhoLKXM8EnfiE1mFBoEPV54IIfyLmX0eeDvwJuBz8+jnfdMDYzOrA34VGAb2zvEMERFZg1Rz\nLCKr0cMhhOEZzj8Qj1fPo48J4LEZzu8CmoFH4oS+2Z4xLyGEa2f6Azy5kH5ERGR1qNrM8QuHfZON\nZ17ozc4NjXj97VQ5Y1ybMqwNwX9O6M55rXLjVMryHj/iG3wMD/n9p4bScmhD4xPxeScAODaQklQv\n2bMHgOteeT0Azz31dNa273v/D4DewbQC1XOnD/r44mYllktjqG/05drG854d3tDRlLXVxeuOHT0K\nQLEiT9Z+gddH59p9qbn+E6lxslBRdC2yupyY5Xz5V0Hts7RXOhlCCDOcL9/7YjxsFZkAABDVSURB\nVM8QEZE1SJljEVmNNs1yvjyrdT7Lt80UGFfe+2LPEBGRNUjBsYisRteYWesM52+Kx+8vou8ngTHg\nx8xspgz0TTOcExGRNaJqyyr+95e/AUBTcyo/aGn2kolSnH2Xz1eUNBb954R8nZcaTIX0c8Ox475z\n3cCYl1CUcmkJtFDnE9xGSz5Z76neI1nboT6/b99jPpn9qpfuztq27doIwMGTRyuGEPuPE+ymCmkn\nPpvw/jes93hhy4a2rO3IoUMANDf4a21uSTFF3br4+oP/Uw8NTqY+K5Z8E1ll2oH/DFSuVvEyfCLd\nIL4z3lkJIUzFSXfvwifkVa5WUX7Gkrhyazv7tDGGiMh5pWqDYxE5r30L+Pdmdh3wIGmd4xrg1+ax\njNuL+QDwauA3Y0BcXuf4l4CvAm9cZP8iInKeqtrg2MwnsBXGLTs3kc8DUMp79rQwkbKog3EJtyHz\nbHJNxSYgebytrc2zsIWpdN/EuG/YMZX3v8pDh8eytt2XXgjAxk6fDFeXSxP52jd6dnfzpTuzc1eV\ntgHwg/0+ce+xxw5mbWPD/syJfo8JBhrzWdvVl3lGenPrVgA6W1Mp5f4jzwHw3ccf9hPFVIZZa6qq\nkVXrBeB24KPx2AA8DHw4hPAPi+08hHDKzK7H1zv+OeBlwFPArwO9KDgWEVmzqjY4FpHzTwihF7CK\nUz//ItffA9wzw/meeTzrOPDOWZptlvMiIlLlqjY4bm/1l7alqTs7d+akb95xOi6/1rKuI2s7OnQS\ngELBM7QtDWmzjM42/3r7hXGb5g1dWdtAzOgeP3IGgMH+M2kQ5v+/nurrB6CuKS2d1tTmy681tafs\nbVd3AwA9O31cxcmUhW6p8dcxNeWFwqExbQLS1uXLtfX1+2sYG0+11BNDvgzdlvhaS+tasrbT/fOZ\n8C8iIiKyduj36iIiIiIikYJjEREREZGoassqrrnqJQAUjo1n54ZLPiGuNueT9YYnKnaIq/OfE2pq\nveRiy7a0/OnPvP46ANZv9qXgcm0VpQkjXt7wrX94CICpYppEf+BYLwDHT8WfQRouydquvOYKf2xt\nKm08dcyXgVvf5uUVXbt7srZNTT5ZL1ZVcKRiZ70DvT5xb+NGL73INaad9dqCj7lzyxYADh49lbXV\n1HYiIiIiIokyxyIiIiIiUdVmjuuK/tKCpQ07urp9V9ix03Hy3Mm+rK1Y9CzyZXsuAuA1r7k2a2tq\n9L6eOngcgFxj+mvr2uxLsr36Z672PvvS0mwj/T4xrjTly6ftvKQnPS/n546cTpncgSmfMLgujnlj\n8/rUV34UgMm4/Fx9Q3qtTXE4h1444K+5JmWOd1x6sT+vxpd+a+hImfTurcoci4iIiFRS5lhERERE\nJFJwLCIiIiISVW1ZRZjwNYJzFTvdUeulE1PBSxOmplKJQWuXT9Lb1OOlDKcmD2dtz53wiXxTYz4R\nrzOXJuQNDB0FYEfPBu/HCllbTc6f3VDjpRdnRvuzticefByAbZfuyM61bPL1k0+NeanFsaPPZ23N\nhfjMuHby+gvTWssdW/zrobxPDhwYSzv4HTjmE/c613upxSWXtGZtoZTWShYRERERZY5FRERERDJV\nmzneeoEvfXbqaNoFLn/CJ+JNjnvGuDCVMqyTRT82+Mpn/MSNV2VtA6c9c9z7tB+HhlKf4yVfum1w\n0DOz3e0bsrbLd/rkvJEhXx7umWcPZm09l/tEuY2bNqUxlHxcxVyb950bzdo2tPsybUePeab6YO+h\nrG3zpo0AtHR4VngyLkcHMDjkkwKnJj1LfMGGC7O27u70bBERERFR5lhEREREJFO1meOWVk8Bb7p6\nS3aufaMvXdb09FMArN/SnLW1bvCa3sKIp5D/6UuPZG215lnX2pwvsVZj6a9t9Iw/5+iIL83WellT\n1mZ13n+o9wx1a3db1par9ef1x2XlAEYnPDNdqvO64tamdH1bm2eFu7uujH2m5dos59c/86zXKDc1\npOXr2tsuAKCz3e/PWXrNk2PpaxERERFR5lhEREREJKPgWERWFTPrNbPelR6HiIisTVVbVjE+6SUK\n9anKAWvyCW4bd3g5wfpLUtlCvuDlFIcO+i5zwwNpmbP2dV7CcMXu3QD09Y1kbaUxXx5u3Q5fTm2w\nkM/anj/hO+rVxbKHpnVpMDVxAiDFVB5Bzss+JuOPLEODw1lTf8nHfvWeHwPgiWefytoGR3xS4Ppu\nLyEJAwMVr8uXlrNa/6cuFNM/+cEDR/yLVyIiIiIiVHFwLCKy0h4/MkjP73xlpYexpHo/estKD0FE\n5Jyq2uB4OPgmIKV8yuRag2dRO+PEuEefeDZr+8HjvsxabZ1//7o3bMvaXnPTdd5nv2d5x0Z7s7au\n7jgRr9MzzTUVE+VOnzkJpOXUtnRvztq6m9sBqGtJ2eRTJ32TkCOn+rzvlrRhxwWXbQdgqugp58PH\njmdtU3HfkaZWzzxTm/5Zx8c84/yDJ54EoDHXmLW1NqavRUREREQ1xyKyAsz9RzN7wswmzOyImX3K\nzNrnuOdXzOybZjYQ79lvZr9nZg2zXL/LzO4xs0NmNmlmJ8zsf5jZ5TNce4+ZBTO72Mx+w8weM7Nx\nM3tgCV+2iIicB6o2c9wck7SlQqrbDXEjjFzBs7ubNqTM6dE2zwCPDXrGua3QkbWNHvJMblONL8n2\n9luuTX0GXzbte4/6dtD9gylTXVPyts5mz0JPjqYl1k4Merp3Ip82FHn6iGeyS3U+vub6uqzt8Mm4\n+cdhz3C3tKascvcGX67Nary2uURIYx/11zM87Md8bWprLO94IrL87gLeAxwD/gyYAn4euA6oByYr\nLzazzwLvAA4DfwsMAK8Afh94tZm9JoRQqLj+9cAXgTrgy8CzwDbgzcAtZnZzCOHhGcb1CeAnga8A\nXwWKM1wjIiJVrGqDYxFZnczslXhg/Bzw8hDC6Xj+g8A3gS3AgYrrb8MD4/uAXw0hjFe07QU+BLwb\nD2wxs07gC8AYcEMI4YcV118JPAT8OXDNDMO7Brg6hPDCAl7Pvlmads23DxERWT1UViEiy+0d8fiR\ncmAMEEKYAH53huvfCxSAd1YGxtHvA/3Ar1ac+3dAB/ChysA4PuNx4L8DV5vZFTM8648WEhiLiEj1\nqdrM8Ya4o9zAQCk7V2r0XekmJv23r6E+/Wyw/SW+DFpTi5daDFhayu3R504AsK17PQD5Rw5lbTt2\nXApAY/smvz+XyiQKcTLcyVNHY99pR7rNW/x5+VOFdG7jRv8iTqirrU9lH4f6fXm2/pMeS7Q1p5KI\n2vgzTn589F99D9AU4m57dV6W2dSa7hufHENkBZQztv80Q9t3qChlMLNmYA9wCvhNM5upvzywu+L7\nn4jHPTGzPN1l8bgb+OG0tv8718BnEkK4dqbzMaM8U3ZaRERWsaoNjkVk1SpPujsxvSGEUDCzUxWn\nOgEDNuDlE/OxPh7f9SLXzVR0f3yGcyIisoZUb3A87BtjtNSkLGoBn+iWL8aJebVpglxjk2dpOzb4\nMm87d1yYtbXUe8b3YK9ngJ/qTf+nP9bvfVnRJ+Jt2tCZta3r8Ox1iMunBUtZ4pExHx81KbPd0OAT\n8ArBJ80NDWe/caZtnffVvdHH0t7SkrXVN5biNT72xro0ef/kKY8zOjv9vpaOFA80NqZMtsgyKv96\nZRPwfGWDmeWAbnziXeW13w8hzDcLW75nTwjhsQWOLbz4JSIiUs2qNzgWkdXqYbzc4EamBcfAq4Bs\nsfAQwoiZPQG8xMy6KmuU5/AQ8Av4qhMLDY6X1JVb29mnTTNERM4rmpAnIsvtnnj8oJl1lU+aWSPw\nhzNc/3F8ebfPmlnH9EYz6zSzyqzy3fhSbx8ys5fPcH2Nmd109sMXEZFqVrWZ462dPrlteCpNOivW\n+2SeiRr/rWvd+FDWtn2Dlyk01HipxdRAmlg31ey/aS3Fregmp9LSp+OTxXhuwp9xJiW21k367ncd\nsZRhbCw9Lz/m11NKP5/kJ2KJRc7H2d7elrV1r/cyzeZYepGrmJfUd9TLJDd0dgNw0Y7tWVt9o7+e\nkQn/exgcTOM7fOQwIssthPCgmX0S+A3gcTP7G9I6x2fwtY8rr/+smV0L/AfgOTP7B+Ag0AVcBNyA\nB8S3x+v7zewX8aXfHjKzbwBP4CUT2/EJe+sBbREpIiI/omqDYxFZ1d4LPI2vT/xr+HJs9wEfAB6d\nfnEI4d1m9jU8AP5pfKm203iQ/MfAX027/htm9lLg/cDr8BKLSeAocD++kci51rN//36uvXbGxSxE\nRGQO+/fvB+hZiWdbCJp/IiKy1Mwsj9dP/0iwL7JKlDeqeXJFRyEysz1AMYTQ8KJXLjFljkVEzo3H\nYfZ1kEVWWnl3R71HZTWaY/fRc04T8kREREREIgXHIiIiIiKRgmMRERERkUjBsYiIiIhIpOBYRERE\nRCTSUm4iIiIiIpEyxyIiIiIikYJjEREREZFIwbGIiIiISKTgWEREREQkUnAsIiIiIhIpOBYRERER\niRQci4iIiIhECo5FRObBzLaZ2WfN7KiZ5c2s18zuMrPOlehHZLqleG/Fe8Isf46fy/FLdTOzXzSz\nT5rZt81sKL6n/uos+zqnn6PaBERE5EWY2U7gu8BG4EvAk8DLgZuBp4DrQwj9y9WPyHRL+B7tBTqA\nu2ZoHgkhfGypxixri5k9AuwBRoDDwC7g8yGEty2wn3P+OZpbzM0iImvEp/EP4veEED5ZPmlmHwfu\nAD4C3L6M/YhMt5TvrYEQwt4lH6GsdXfgQfGzwI3AN8+yn3P+OarMsYjIHGKW4lmgF9gZQihVtLUC\nxwADNoYQRs91PyLTLeV7K2aOCSH0nKPhimBmN+HB8YIyx8v1OaqaYxGRud0cj1+v/CAGCCEMAw8C\nzcArlqkfkemW+r3VYGZvM7MPmNl7zexmM6tdwvGKnK1l+RxVcCwiMrfL4/HpWdqficfLlqkfkemW\n+r21GbgX//X0XcD9wDNmduNZj1BkaSzL56iCYxGRubXH4+As7eXzHcvUj8h0S/neuht4NR4gtwBX\nAX8K9ABfM7M9Zz9MkUVbls9RTcgTERERAEIId0479Thwu5mNAO8D9gJvWu5xiSwnZY5FROZWzkS0\nz9JePj+wTP2ITLcc763PxOMNi+hDZLGW5XNUwbGIyNyeisfZatgujcfZauCWuh+R6ZbjvdUXjy2L\n6ENksZblc1TBsYjI3Mprcb7WzP7VZ2ZcOuh6YAx4aJn6EZluOd5b5dn/zy+iD5HFWpbPUQXHIiJz\nCCE8B3wdn5D07mnNd+KZtHvLa2qaWZ2Z7YrrcZ51PyLztVTvUTPbbWY/khk2sx7gU/Hbs9ruV2Qh\nVvpzVJuAiIi8iBm2K90PXIevufk08MrydqUxkHgBODB9I4WF9COyEEvxHjWzvfiku28BB4BhYCdw\nC9AIfBV4UwhhchleklQZM7sVuDV+uxl4Hf6biG/Hc6dCCO+P1/awgp+jCo5FRObBzLYDHwZeD6zH\nd2K6D7gzhHCm4roeZvlQX0g/Igu12PdoXMf4duBq0lJuA8Aj+LrH9wYFDXKW4g9fH5rjkuz9uNKf\nowqORUREREQi1RyLiIiIiEQKjkVEREREIgXHIiIiIiKRgmMRERERkUjBsYiIiIhIpOBYRERERCRS\ncCwiIiIiEik4FhERERGJFByLiIiIiEQKjkVEREREIgXHIiIiIiKRgmMRERERkUjBsYiIiIhIpOBY\nRERERCRScCwiIiIiEik4FhERERGJFByLiIiIiET/Hz34YzzyuTRAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ddb76d8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
